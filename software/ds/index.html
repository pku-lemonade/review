
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Youwei Zhuo">
      
      
        <link rel="canonical" href="https://review.youwei.xyz/software/ds/">
      
      
        <link rel="prev" href="../os/">
      
      
        <link rel="next" href="../hpc/">
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Distributed Systems - New Lemonade Review</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-JXHKVQ8LKS"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-JXHKVQ8LKS",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-JXHKVQ8LKS",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#distributed-systems" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="New Lemonade Review" class="md-header__button md-logo" aria-label="New Lemonade Review" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M368 80c-3.2 0-6.2.4-8.9 1.3C340 86.8 313 92 284.8 84.6c-57.4-14.9-124.6 7.4-174.7 57.5S37.7 259.4 52.6 316.8c7.3 28.2 2.2 55.2-3.3 74.3-.8 2.8-1.3 5.8-1.3 8.9 0 17.7 14.3 32 32 32 3.2 0 6.2-.4 8.9-1.3 19.1-5.5 46.1-10.7 74.3-3.3 57.4 14.9 124.6-7.4 174.7-57.5s72.4-117.3 57.5-174.7c-7.3-28.2-2.2-55.2 3.3-74.3.8-2.8 1.3-5.8 1.3-8.9 0-17.7-14.3-32-32-32m0-48c44.2 0 80 35.8 80 80 0 7.7-1.1 15.2-3.1 22.3-4.6 15.8-7.1 32.9-3 48.9 20.1 77.6-10.9 161.5-70 220.7s-143.1 90.2-220.7 70c-16-4.1-33-1.6-48.9 3-7.1 2-14.6 3.1-22.3 3.1-44.2 0-80-35.8-80-80 0-7.7 1.1-15.2 3.1-22.3 4.6-15.8 7.1-32.9 3-48.9-20.1-77.6 10.9-161.5 70-220.7s143.2-90.1 220.7-70c16 4.1 33 1.6 48.9-3 7.1-2 14.6-3.1 22.3-3.1M246.7 167c-52 15.2-96.5 59.7-111.7 111.7-3.7 12.7-17.1 20-29.8 16.3s-20-17-16.2-29.7c19.8-67.7 76.6-124.5 144.3-144.3 12.7-3.7 26.1 3.6 29.8 16.3s-3.6 26.1-16.3 29.8z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            New Lemonade Review
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Distributed Systems
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/pku-lemonade/review" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pku-lemonade/review
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="New Lemonade Review" class="md-nav__button md-logo" aria-label="New Lemonade Review" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M368 80c-3.2 0-6.2.4-8.9 1.3C340 86.8 313 92 284.8 84.6c-57.4-14.9-124.6 7.4-174.7 57.5S37.7 259.4 52.6 316.8c7.3 28.2 2.2 55.2-3.3 74.3-.8 2.8-1.3 5.8-1.3 8.9 0 17.7 14.3 32 32 32 3.2 0 6.2-.4 8.9-1.3 19.1-5.5 46.1-10.7 74.3-3.3 57.4 14.9 124.6-7.4 174.7-57.5s72.4-117.3 57.5-174.7c-7.3-28.2-2.2-55.2 3.3-74.3.8-2.8 1.3-5.8 1.3-8.9 0-17.7-14.3-32-32-32m0-48c44.2 0 80 35.8 80 80 0 7.7-1.1 15.2-3.1 22.3-4.6 15.8-7.1 32.9-3 48.9 20.1 77.6-10.9 161.5-70 220.7s-143.1 90.2-220.7 70c-16-4.1-33-1.6-48.9 3-7.1 2-14.6 3.1-22.3 3.1-44.2 0-80-35.8-80-80 0-7.7 1.1-15.2 3.1-22.3 4.6-15.8 7.1-32.9 3-48.9-20.1-77.6 10.9-161.5 70-220.7s143.2-90.1 220.7-70c16 4.1 33 1.6 48.9-3 7.1-2 14.6-3.1 22.3-3.1M246.7 167c-52 15.2-96.5 59.7-111.7 111.7-3.7 12.7-17.1 20-29.8 16.3s-20-17-16.2-29.7c19.8-67.7 76.6-124.5 144.3-144.3 12.7-3.7 26.1 3.6 29.8 16.3s-3.6 26.1-16.3 29.8z"/></svg>

    </a>
    New Lemonade Review
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/pku-lemonade/review" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pku-lemonade/review
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    README
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../hardware/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    Hardware
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Hardware
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hardware/processor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Processor Architecture
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hardware/parallel/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Parallel and Multi-Processor Architecture
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hardware/memory/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Memory Architecture
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hardware/accelerators/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Domain-Specific Accelerators
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hardware/network/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Interconnection Networks
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hardware/eda/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Electronic Design Automation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hardware/fail/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Security and Reliability
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hardware/emerging/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Emerging Technologies
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hardware/perf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Performance Analysis (to be deleted)
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    Software
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Software
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../algorithm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Algorithms
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Programming Languages
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../os/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Operating Systems
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Distributed Systems
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Distributed Systems
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#distributed-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Distributed algorithms
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Distributed algorithms">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#computing-framework" class="md-nav__link">
    <span class="md-ellipsis">
      
        Computing Framework
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Computing Framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#domain-specific-computing-framework" class="md-nav__link">
    <span class="md-ellipsis">
      
        Domain Specific Computing Framework
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#graph-mining-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Graph Mining Frameworks
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parallel-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parallel Strategies
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Parallel Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      
        Data Parallelism
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      
        Model Parallelism
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-specific-parallel-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM-specific Parallel Strategies
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hardware-componment-co-design" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hardware componment co-design
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cloud-computing-platforms-and-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cloud computing platforms and architectures
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Cloud computing platforms and architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cloud-platform-llm-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cloud Platform LLM Scheduling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resource-management-in-cloud-platform" class="md-nav__link">
    <span class="md-ellipsis">
      
        Resource Management in Cloud Platform
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#microservices" class="md-nav__link">
    <span class="md-ellipsis">
      
        Microservices
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Microservices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#root-cause-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      
        Root Cause analysis
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#memory-management" class="md-nav__link">
    <span class="md-ellipsis">
      
        Memory Management
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Memory Management">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#remote-memory" class="md-nav__link">
    <span class="md-ellipsis">
      
        Remote Memory
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scratchpad-memory" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scratchpad Memory
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-optimization-for-graph-processing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Memory Optimization for Graph Processing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-memory-management" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Memory Management
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLM Memory Management">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#memory-management-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Memory Management Algorithms
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#general-llm-memory-management" class="md-nav__link">
    <span class="md-ellipsis">
      
        General LLM Memory Management
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="General LLM Memory Management">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#application-specific-memory-management" class="md-nav__link">
    <span class="md-ellipsis">
      
        Application specific memory management
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kv-cache-reuse-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        KV Cache Reuse Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="KV Cache Reuse Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#prefix-sharing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Prefix Sharing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kv-cache-offloading" class="md-nav__link">
    <span class="md-ellipsis">
      
        KV cache offloading
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#position-independent-methid" class="md-nav__link">
    <span class="md-ellipsis">
      
        Position Independent Methid
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      
        Other Techniques
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kv-cache-storage-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        KV Cache Storage Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kv-cache-evict-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        KV Cache Evict Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#systems-with-other-caches" class="md-nav__link">
    <span class="md-ellipsis">
      
        Systems with Other Caches
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Systems with Other Caches">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llm-prefetching" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Prefetching
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#communication-centric-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Communication-Centric Optimization
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Communication-Centric Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#io-characterization-and-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        I/O Characterization and Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-gpu-communication" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPU-GPU Communication
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#many-core-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Many-Core Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Many-Core Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#workload-characterization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Workload Characterization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fault-propagation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Fault Propagation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fault-injection-technique" class="md-nav__link">
    <span class="md-ellipsis">
      
        Fault Injection Technique
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#communication" class="md-nav__link">
    <span class="md-ellipsis">
      
        Communication
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#heterogeneous-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Heterogeneous Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Heterogeneous Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-applications" class="md-nav__link">
    <span class="md-ellipsis">
      
        General Applications
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decentralized-serving" class="md-nav__link">
    <span class="md-ellipsis">
      
        Decentralized Serving
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ml-training-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        ML Training Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-inference-heterogeneous-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Inference Heterogeneous Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLM Inference Heterogeneous Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mobile-edge-network-serving" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mobile &amp; Edge-Network Serving
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-gpu-heterogeneous-system" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPU-GPU Heterogeneous System
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#xpu-gpu-heterogeneous-system" class="md-nav__link">
    <span class="md-ellipsis">
      
        XPU-GPU Heterogeneous System
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneous-device-task-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Heterogeneous Device Task Scheduling
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Heterogeneous Device Task Scheduling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#task-scheduling-for-specific-tasks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Task Scheduling for specific tasks
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-training-heterogeneous-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Training Heterogeneous Systems
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#schedule-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Schedule Optimization
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Schedule Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-task-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        General Task Scheduling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#speculative-execution-non-llm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Speculative Execution (Non-LLM)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-related-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM-Related Scheduling
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLM-Related Scheduling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llm-request-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Request Scheduling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batching-for-better-throughput" class="md-nav__link">
    <span class="md-ellipsis">
      
        Batching for Better Throughput
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Batching for Better Throughput">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#info-predict-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Info Predict Scheduling
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-application-level-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Application-Level Scheduling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-speculative-inference" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Speculative Inference
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLM Speculative Inference">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#spec-others" class="md-nav__link">
    <span class="md-ellipsis">
      
        Spec + Others
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-serving-outages-and-incidents" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Serving Outages and Incidents
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#energy-optimized-llm-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Energy-Optimized LLM Scheduling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-llm-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multi-LLM Scheduling
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dnn-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        DNN Scheduling
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DNN Scheduling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#task-offloading" class="md-nav__link">
    <span class="md-ellipsis">
      
        Task Offloading
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#thermal-aware-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Thermal-Aware Scheduling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dark-silicon-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dark Silicon Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rl-based-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        RL-based Scheduling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rl-based-mapping" class="md-nav__link">
    <span class="md-ellipsis">
      
        RL-based Mapping
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#general-optimizations-for-deep-learning-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        General optimizations for Deep Learning Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="General optimizations for Deep Learning Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llm-training-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Training Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLM Training Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        General Optimizations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizations-on-special-scene" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optimizations on Special Scene
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#experiments" class="md-nav__link">
    <span class="md-ellipsis">
      
        Experiments
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-modal-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multi-Modal Optimizations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kernel-level-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Kernel-Level Optimizations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sft-and-fine-tuning-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        SFT and Fine-tuning Optimizations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fault-tolerance-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Fault tolerance Optimizations
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Fault tolerance Optimizations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ml-sparse-related-optimizaions" class="md-nav__link">
    <span class="md-ellipsis">
      
        ML Sparse related optimizaions
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-inference-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Inference Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLM Inference Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slo-aware-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        SLO-Aware Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#surveys" class="md-nav__link">
    <span class="md-ellipsis">
      
        Surveys
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Surveys">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#system-optimization-surveys" class="md-nav__link">
    <span class="md-ellipsis">
      
        System Optimization Surveys
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#application-surveys" class="md-nav__link">
    <span class="md-ellipsis">
      
        Application Surveys
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimodal-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multimodal Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixture-of-experts-llm-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mixture-of-Experts LLM Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mixture-of-Experts LLM Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#expert-offloading-and-placement" class="md-nav__link">
    <span class="md-ellipsis">
      
        Expert Offloading and Placement
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batching-and-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Batching and Scheduling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-and-communication-efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      
        Memory and Communication Efficiency
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#architectural-innovations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Architectural Innovations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute-kernel-level-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Compute-Kernel-Level Optimizations
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#long-sequence-llm-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Long Sequence LLM Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Long Sequence LLM Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sparse Attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ring-computation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Ring Computation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#p-d-disaggregated-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        P-D Disaggregated Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="P-D Disaggregated Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#p-d-disaggregated-system-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        P-D Disaggregated System Optimizations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-f-disaggregated-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        A-F Disaggregated Systems
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#throughput-optimized-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Throughput-Optimized Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mtp-llm-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        MTP LLM Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MTP LLM Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#speculative-decodeing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Speculative Decodeing
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#diffusion-llm-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Diffusion LLM Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kv-cache-in-dllm-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        KV Cache in dLLM Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parallel-decoding-in-dllm-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parallel Decoding in dLLM Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#visual-auto-regressive-models-serving-system" class="md-nav__link">
    <span class="md-ellipsis">
      
        Visual Auto-Regressive Models Serving System
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fair-serving-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Fair Serving Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#agent-colocating-serving" class="md-nav__link">
    <span class="md-ellipsis">
      
        Agent Colocating Serving
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rl-system" class="md-nav__link">
    <span class="md-ellipsis">
      
        RL System
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="RL System">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rlhf-system" class="md-nav__link">
    <span class="md-ellipsis">
      
        RLHF System
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#agentic-rl-system" class="md-nav__link">
    <span class="md-ellipsis">
      
        Agentic RL System
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#communication-computation-overlap" class="md-nav__link">
    <span class="md-ellipsis">
      
        Communication-Computation Overlap
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Configuration Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#diffusion-model-serving-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Diffusion Model Serving Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-agent-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multi Agent Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Multi Agent Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dsl-for-agent-representation" class="md-nav__link">
    <span class="md-ellipsis">
      
        DSL for agent representation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cpu-based-optimizations-for-large-scale-multi-agent-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        CPU based Optimizations for Large Scale Multi-Agent Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-based-optimizations-for-multi-agent-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPU based Optimizations for Multi-Agent Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reuse-optimizations-for-multi-agent-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reuse Optimizations for Multi-Agent Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#environment-optimizations-for-multi-agent-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Environment Optimizations for Multi-Agent Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dsl-expression-for-agents" class="md-nav__link">
    <span class="md-ellipsis">
      
        DSL Expression for Agents
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-kernel-optimizations-for-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPU Kernel Optimizations for Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GPU Kernel Optimizations for Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tiling-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Tiling Optimizations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-level-kernel-svcheduling-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPU level kernel svcheduling optimizations
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hpc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    High-Performance Computing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../perf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Performance Analysis
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#distributed-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Distributed algorithms
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Distributed algorithms">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#computing-framework" class="md-nav__link">
    <span class="md-ellipsis">
      
        Computing Framework
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Computing Framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#domain-specific-computing-framework" class="md-nav__link">
    <span class="md-ellipsis">
      
        Domain Specific Computing Framework
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#graph-mining-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Graph Mining Frameworks
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parallel-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parallel Strategies
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Parallel Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      
        Data Parallelism
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      
        Model Parallelism
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-specific-parallel-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM-specific Parallel Strategies
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hardware-componment-co-design" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hardware componment co-design
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cloud-computing-platforms-and-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cloud computing platforms and architectures
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Cloud computing platforms and architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cloud-platform-llm-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cloud Platform LLM Scheduling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resource-management-in-cloud-platform" class="md-nav__link">
    <span class="md-ellipsis">
      
        Resource Management in Cloud Platform
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#microservices" class="md-nav__link">
    <span class="md-ellipsis">
      
        Microservices
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Microservices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#root-cause-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      
        Root Cause analysis
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#memory-management" class="md-nav__link">
    <span class="md-ellipsis">
      
        Memory Management
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Memory Management">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#remote-memory" class="md-nav__link">
    <span class="md-ellipsis">
      
        Remote Memory
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scratchpad-memory" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scratchpad Memory
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-optimization-for-graph-processing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Memory Optimization for Graph Processing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-memory-management" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Memory Management
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLM Memory Management">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#memory-management-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Memory Management Algorithms
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#general-llm-memory-management" class="md-nav__link">
    <span class="md-ellipsis">
      
        General LLM Memory Management
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="General LLM Memory Management">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#application-specific-memory-management" class="md-nav__link">
    <span class="md-ellipsis">
      
        Application specific memory management
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kv-cache-reuse-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        KV Cache Reuse Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="KV Cache Reuse Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#prefix-sharing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Prefix Sharing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kv-cache-offloading" class="md-nav__link">
    <span class="md-ellipsis">
      
        KV cache offloading
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#position-independent-methid" class="md-nav__link">
    <span class="md-ellipsis">
      
        Position Independent Methid
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      
        Other Techniques
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kv-cache-storage-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        KV Cache Storage Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kv-cache-evict-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        KV Cache Evict Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#systems-with-other-caches" class="md-nav__link">
    <span class="md-ellipsis">
      
        Systems with Other Caches
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Systems with Other Caches">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llm-prefetching" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Prefetching
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#communication-centric-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Communication-Centric Optimization
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Communication-Centric Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#io-characterization-and-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        I/O Characterization and Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-gpu-communication" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPU-GPU Communication
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#many-core-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Many-Core Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Many-Core Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#workload-characterization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Workload Characterization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fault-propagation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Fault Propagation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fault-injection-technique" class="md-nav__link">
    <span class="md-ellipsis">
      
        Fault Injection Technique
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#communication" class="md-nav__link">
    <span class="md-ellipsis">
      
        Communication
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#heterogeneous-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Heterogeneous Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Heterogeneous Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-applications" class="md-nav__link">
    <span class="md-ellipsis">
      
        General Applications
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decentralized-serving" class="md-nav__link">
    <span class="md-ellipsis">
      
        Decentralized Serving
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ml-training-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        ML Training Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-inference-heterogeneous-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Inference Heterogeneous Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLM Inference Heterogeneous Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mobile-edge-network-serving" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mobile &amp; Edge-Network Serving
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-gpu-heterogeneous-system" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPU-GPU Heterogeneous System
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#xpu-gpu-heterogeneous-system" class="md-nav__link">
    <span class="md-ellipsis">
      
        XPU-GPU Heterogeneous System
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneous-device-task-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Heterogeneous Device Task Scheduling
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Heterogeneous Device Task Scheduling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#task-scheduling-for-specific-tasks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Task Scheduling for specific tasks
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-training-heterogeneous-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Training Heterogeneous Systems
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#schedule-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Schedule Optimization
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Schedule Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-task-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        General Task Scheduling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#speculative-execution-non-llm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Speculative Execution (Non-LLM)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-related-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM-Related Scheduling
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLM-Related Scheduling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llm-request-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Request Scheduling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batching-for-better-throughput" class="md-nav__link">
    <span class="md-ellipsis">
      
        Batching for Better Throughput
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Batching for Better Throughput">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#info-predict-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Info Predict Scheduling
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-application-level-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Application-Level Scheduling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-speculative-inference" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Speculative Inference
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLM Speculative Inference">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#spec-others" class="md-nav__link">
    <span class="md-ellipsis">
      
        Spec + Others
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-serving-outages-and-incidents" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Serving Outages and Incidents
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#energy-optimized-llm-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Energy-Optimized LLM Scheduling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-llm-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multi-LLM Scheduling
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dnn-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        DNN Scheduling
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DNN Scheduling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#task-offloading" class="md-nav__link">
    <span class="md-ellipsis">
      
        Task Offloading
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#thermal-aware-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Thermal-Aware Scheduling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dark-silicon-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dark Silicon Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rl-based-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        RL-based Scheduling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rl-based-mapping" class="md-nav__link">
    <span class="md-ellipsis">
      
        RL-based Mapping
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#general-optimizations-for-deep-learning-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        General optimizations for Deep Learning Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="General optimizations for Deep Learning Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llm-training-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Training Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLM Training Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        General Optimizations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizations-on-special-scene" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optimizations on Special Scene
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#experiments" class="md-nav__link">
    <span class="md-ellipsis">
      
        Experiments
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-modal-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multi-Modal Optimizations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kernel-level-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Kernel-Level Optimizations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sft-and-fine-tuning-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        SFT and Fine-tuning Optimizations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fault-tolerance-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Fault tolerance Optimizations
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Fault tolerance Optimizations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ml-sparse-related-optimizaions" class="md-nav__link">
    <span class="md-ellipsis">
      
        ML Sparse related optimizaions
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-inference-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Inference Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLM Inference Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slo-aware-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        SLO-Aware Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#surveys" class="md-nav__link">
    <span class="md-ellipsis">
      
        Surveys
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Surveys">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#system-optimization-surveys" class="md-nav__link">
    <span class="md-ellipsis">
      
        System Optimization Surveys
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#application-surveys" class="md-nav__link">
    <span class="md-ellipsis">
      
        Application Surveys
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimodal-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multimodal Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixture-of-experts-llm-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mixture-of-Experts LLM Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mixture-of-Experts LLM Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#expert-offloading-and-placement" class="md-nav__link">
    <span class="md-ellipsis">
      
        Expert Offloading and Placement
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batching-and-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Batching and Scheduling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-and-communication-efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      
        Memory and Communication Efficiency
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#architectural-innovations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Architectural Innovations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute-kernel-level-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Compute-Kernel-Level Optimizations
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#long-sequence-llm-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Long Sequence LLM Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Long Sequence LLM Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sparse Attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ring-computation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Ring Computation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#p-d-disaggregated-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        P-D Disaggregated Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="P-D Disaggregated Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#p-d-disaggregated-system-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        P-D Disaggregated System Optimizations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-f-disaggregated-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        A-F Disaggregated Systems
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#throughput-optimized-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Throughput-Optimized Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mtp-llm-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        MTP LLM Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MTP LLM Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#speculative-decodeing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Speculative Decodeing
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#diffusion-llm-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Diffusion LLM Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kv-cache-in-dllm-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        KV Cache in dLLM Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parallel-decoding-in-dllm-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parallel Decoding in dLLM Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#visual-auto-regressive-models-serving-system" class="md-nav__link">
    <span class="md-ellipsis">
      
        Visual Auto-Regressive Models Serving System
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fair-serving-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Fair Serving Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#agent-colocating-serving" class="md-nav__link">
    <span class="md-ellipsis">
      
        Agent Colocating Serving
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rl-system" class="md-nav__link">
    <span class="md-ellipsis">
      
        RL System
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="RL System">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rlhf-system" class="md-nav__link">
    <span class="md-ellipsis">
      
        RLHF System
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#agentic-rl-system" class="md-nav__link">
    <span class="md-ellipsis">
      
        Agentic RL System
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#communication-computation-overlap" class="md-nav__link">
    <span class="md-ellipsis">
      
        Communication-Computation Overlap
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Configuration Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#diffusion-model-serving-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Diffusion Model Serving Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-agent-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multi Agent Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Multi Agent Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dsl-for-agent-representation" class="md-nav__link">
    <span class="md-ellipsis">
      
        DSL for agent representation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cpu-based-optimizations-for-large-scale-multi-agent-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        CPU based Optimizations for Large Scale Multi-Agent Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-based-optimizations-for-multi-agent-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPU based Optimizations for Multi-Agent Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reuse-optimizations-for-multi-agent-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reuse Optimizations for Multi-Agent Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#environment-optimizations-for-multi-agent-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Environment Optimizations for Multi-Agent Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dsl-expression-for-agents" class="md-nav__link">
    <span class="md-ellipsis">
      
        DSL Expression for Agents
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-kernel-optimizations-for-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPU Kernel Optimizations for Systems
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GPU Kernel Optimizations for Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tiling-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Tiling Optimizations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-level-kernel-svcheduling-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPU level kernel svcheduling optimizations
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/pku-lemonade/review/edit/main/docs/software/ds.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/pku-lemonade/review/raw/main/docs/software/ds.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<h1 id="distributed-systems">Distributed Systems<a class="headerlink" href="#distributed-systems" title="Permanent link">&para;</a></h1>
<h2 id="distributed-algorithms">Distributed algorithms<a class="headerlink" href="#distributed-algorithms" title="Permanent link">&para;</a></h2>
<p>Focusing on the distributed algorithms, such as consensus and replication, like RAFT.</p>
<p>Challenge: concurrency, synchronous and communication complexities across independent nodes</p>
<p>Solution: problems that require coordination, computation, and data management across multiple independent computer systems.</p>
<h3 id="computing-framework">Computing Framework<a class="headerlink" href="#computing-framework" title="Permanent link">&para;</a></h3>
<p>Solution: Developing distributed algorithms requires a clear understanding of the computing framework, which scales small computing units to achieve a more clear data processing. The common computing frameworks are MapReduce, Spark, etc.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2004</td>
<td>OSDI</td>
<td>Google</td>
<td>MapReduce: simplified data processing on large clusters</td>
<td>divife the data processing into map and reduce stages; use master-worker architecture</td>
<td>4</td>
<td>5</td>
<td>5</td>
</tr>
</tbody>
</table>
<h4 id="domain-specific-computing-framework">Domain Specific Computing Framework<a class="headerlink" href="#domain-specific-computing-framework" title="Permanent link">&para;</a></h4>
<p>Challenge: specific bounds of different situations</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2012</td>
<td>OSDI</td>
<td>CMU</td>
<td>PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs</td>
<td>GAS (Gather-Apply-Scatter) decomposition; vertex-cut partitioning; delta caching; parallel locking for high-degree vertices</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2018</td>
<td>PLDI</td>
<td>UT Austin &amp; UIUC</td>
<td>Gluon: A Communication-Optimizing Substrate for Distributed Heterogeneous Graph Analytics</td>
<td>communication-optimizing substrate; heterogeneous processor support; structural/temporal invariant-based communication optimization; memoization of address translation</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2024</td>
<td>PPoPP</td>
<td>NUDT</td>
<td>GraphCube: Interconnection Hierarchy-aware Graph Processing</td>
<td>interconnection hierarchy-aware; topology-aware graph partitioning; extreme-scale graph processing</td>
<td>4</td>
<td>5</td>
<td>5</td>
</tr>
</tbody>
</table>
<h4 id="graph-mining-frameworks">Graph Mining Frameworks<a class="headerlink" href="#graph-mining-frameworks" title="Permanent link">&para;</a></h4>
<p>Challenge: inefficient algorithms and communication patterns</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2015</td>
<td>SOSP</td>
<td>QCRI</td>
<td>Arabesque: A System for Distributed Graph Mining</td>
<td>distributed graph mining; think like an embedding paradigm; filter-process model; ODAG embedding storage</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2023</td>
<td>ASPLOS</td>
<td>Purdue</td>
<td>Khuzdul: Efficient and Scalable Distributed Graph Pattern Mining Engine</td>
<td>distributed graph mining engine; extendable embedding abstraction; fine-grained task scheduling; BFS-DFS hybrid exploration</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
</tbody>
</table>
<h3 id="parallel-strategies">Parallel Strategies<a class="headerlink" href="#parallel-strategies" title="Permanent link">&para;</a></h3>
<p>Soultion: using the computation and memory resources of multiple processors to solve a problem.</p>
<p>Challenge: communication overhead and load balancing</p>
<h4 id="data-parallelism">Data Parallelism <a name="Data-Parallelism"></a><a class="headerlink" href="#data-parallelism" title="Permanent link">&para;</a></h4>
<p>Solution: Data parallelism addresses scenarios where a single GPU can accommodate the model, but the dataset's size necessitates distribution across multiple GPUs for efficient processing and accelerated training.</p>
<p>Modern DNN acceleration systems commonly use the combination of <a href="./#Data-Parallelism">data parallelism</a> and <a href="./#Model-Parallelism">model parallelism</a>.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2012</td>
<td>Nips</td>
<td>Google</td>
<td>Large Scale Distributed Deep Networks</td>
<td>data parallel; use many model to optimize the same data; distributed model training</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2014</td>
<td>OSDI</td>
<td>CMU</td>
<td>Scaling Distributed Machine Learning with the Parameter Server</td>
<td>the foundation of tensor parallel; parameter server; pull-based data transfer</td>
<td>4</td>
<td>5</td>
<td>3</td>
</tr>
<tr>
<td>2020</td>
<td>SC</td>
<td>Miscrosoft</td>
<td>ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</td>
<td>fix the problem that dp cannot reduce the memory usage on single GPU</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="model-parallelism">Model Parallelism <a name="Model-Parallelism"></a><a class="headerlink" href="#model-parallelism" title="Permanent link">&para;</a></h4>
<p>Solution: Model parallelism addresses scenarios where the model's size exceeds the processing and memory capacity of a single GPU. There are two types of model parallelism:</p>
<ol>
<li>
<p>Pipeline parallelism: divide the model as pipeline stages, each gpu processes one or more stages.</p>
</li>
<li>
<p>Tensor parallelism: divide the tensor into different GPUs.</p>
</li>
</ol>
<p>Usually, pipeline parallelism and tensor parallelism are used together.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2019</td>
<td>SOSP</td>
<td>Stanford</td>
<td>PipeDream: Fast and Efficient Pipeline Parallel DNN Training</td>
<td>raise the idea of pipeline parallelism; divide model into different GPUs</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2019</td>
<td>arXiv</td>
<td>NVIDIA</td>
<td>Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</td>
<td>transformer based model parallel; pipeline parallel; divide model into different GPUs</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2021</td>
<td>SC</td>
<td>NVIDIA</td>
<td>Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</td>
<td>Megatron2; dive deep into tensor parallelism; how to train a LLM on 1000 GPUs</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2022</td>
<td>arXiv</td>
<td>NVIDIA</td>
<td>Reducing Activation Recomputation in Large Transformer Models</td>
<td>Megatron3; sequence parallel; selective activation recomputation; reduce the amount of recomputed activation</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="llm-specific-parallel-strategies">LLM-specific Parallel Strategies<a class="headerlink" href="#llm-specific-parallel-strategies" title="Permanent link">&para;</a></h4>
<p>Focusing on the parallel strategies for LLM-specific deep learning systems.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022</td>
<td>ACL</td>
<td>NUS</td>
<td>Sequence Parallelism: Long Sequence Training from System Perspective</td>
<td>splits input sequences into chunks; Ring Self-Attention; sparse attention</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="hardware-componment-co-design">Hardware componment co-design<a class="headerlink" href="#hardware-componment-co-design" title="Permanent link">&para;</a></h4>
<p>Challenge: rencent accelerator like GPU has many components, such as tensor cores, SRAM, etc. How to co-design the software and hardware to improve the performance.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>ATC</td>
<td>UC</td>
<td>TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs</td>
<td>use cuda core to transform sparse graph martix into dense for tensor core; codesign of tensor core and cuda core for GNN application</td>
<td>3</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2022</td>
<td>HPCA</td>
<td>SJTU</td>
<td>Tacker: Tensor-CUDA Core Kernel Fusion for Improving the GPU Utilization while Ensuring QoS</td>
<td>cuda core and tensor core operator fuse; a static kernel fusion and scheduling approach to improve GPU utilization; best-effort schedule</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>BJPT</td>
<td>Libra: Synergizing CUDA and Tensor Cores for High-Performance Sparse Matrix Multiplication</td>
<td>mapping computation to cuda core and tensor core; different mapping stragety for different sparse matrix</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Sabanci</td>
<td>BLEST: Blazingly Efficient BFS using Tensor Cores</td>
<td>BVSS workload normalization; MMA layout transformation; warp-level cooperative scheduling</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h2 id="cloud-computing-platforms-and-architectures">Cloud computing platforms and architectures<a class="headerlink" href="#cloud-computing-platforms-and-architectures" title="Permanent link">&para;</a></h2>
<p>Challenge: when providing services to users, facing scalability, resource management, fault tolerance, and cost-effectiveness for building and deploying large-scale distributed applications and services.</p>
<h3 id="cloud-platform-llm-scheduling">Cloud Platform LLM Scheduling<a class="headerlink" href="#cloud-platform-llm-scheduling" title="Permanent link">&para;</a></h3>
<p>Challenge: meet the SLO when providing LLM service on cloud platform.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Azure</td>
<td>TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud Platforms</td>
<td>thermal/power property characterization; dynamically adjust in response to power or cooling failures; thermal- and poweraware manner</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="resource-management-in-cloud-platform">Resource Management in Cloud Platform<a class="headerlink" href="#resource-management-in-cloud-platform" title="Permanent link">&para;</a></h3>
<p>Challenge: reclaim resources that have been allocated to users but are temporarily idle without affecting user experience</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021</td>
<td>EuroSys</td>
<td>Stanford</td>
<td>SmartHarvest: harvesting idle CPUs safely and efficiently in the cloud</td>
<td>ElasticVM for dynamically harvesting; cost-sensitive multi-class classification; two-level safeguard mechanism</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2022</td>
<td>ASPLOS</td>
<td>Stanford</td>
<td>SOL: Safe On-Node Learning in Cloud Platforms</td>
<td>model and actuator decoupled interface; watchdog-style safeguards to avoiding bad models</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h2 id="microservices">Microservices<a class="headerlink" href="#microservices" title="Permanent link">&para;</a></h2>
<p>Focusing on the microservices.</p>
<h3 id="root-cause-analysis">Root Cause analysis<a class="headerlink" href="#root-cause-analysis" title="Permanent link">&para;</a></h3>
<p>Challenge: accurately and efficiently identify true root causes from massive, noisy, and interdependent monitoring data in complex online service systems.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2020</td>
<td>ISSRE</td>
<td>THU&amp;NKU</td>
<td>Unsupervised Detection of Microservice Trace Anomalies through Service-Level Deep Bayesian Networks</td>
<td>service trace vector(STV); deep bayesian networks with posterior flows; STV based RCA algorithm</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2022</td>
<td>SIGKDD</td>
<td>THU&amp;BizSeer</td>
<td>Causal Inference-Based Root Cause Analysis for Online Service Systems with Intervention Recognition</td>
<td>intervention recognition criterion; causal bayesian network; regression-based hypothesis testing; descendant adjustment</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2023</td>
<td>DATE</td>
<td>IIE-CAS</td>
<td>ImpactTracer: Root Cause Localization in Microservices Based on Fault Propagation Modeling</td>
<td>isolation forest for outlier detection; backward tracing algorithm; fault propagation modeling</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2023</td>
<td>SIGKDD</td>
<td>Microsoft</td>
<td>Root Cause Analysis for Microservice Systems via Hierarchical Reinforcement Learning from Human Feedback</td>
<td>site reliability engineer experimence for RCA's first stage; human-in-the-loop training; temporal dimension extension</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<h2 id="memory-management">Memory Management<a class="headerlink" href="#memory-management" title="Permanent link">&para;</a></h2>
<p>Challenge: coordinating memory access and maintaining data consistency across multiple independent nodes with their own local memories, especially when dealing with shared data.</p>
<h3 id="remote-memory">Remote Memory<a class="headerlink" href="#remote-memory" title="Permanent link">&para;</a></h3>
<p>Challenge: efficiently providing access to memory on a remote node while minimizing latency and overhead, and ensuring consistency and reliability despite network communication complexities and potential failures.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2020</td>
<td>TC</td>
<td>Georgia Tech</td>
<td>Hierarchical Orchestration of Disaggregated Memory</td>
<td>XMemPod architecture for hierarchical memory orchestration; compressed swap page table (CSPT) for metadata management; hybrid swap-out algorithm for memory utilization; proactive swap-in optimization for performance; RDMA-based remote memory sharing for low-latency access</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>ATC</td>
<td>HUST</td>
<td>Fast Distributed Transactions for RDMA-based Disaggregated Memory</td>
<td>fast commit protocol by coalescing validation and commit phases; RDMA-enabled offloading for data synchronization; priority-based locking for mission-critical transactions</td>
<td>2</td>
<td>3</td>
<td>4</td>
</tr>
</tbody>
</table>
<h3 id="scratchpad-memory">Scratchpad Memory<a class="headerlink" href="#scratchpad-memory" title="Permanent link">&para;</a></h3>
<p>Challenge: efficiently allocating and coordinating limited fast memory across distributed nodes to minimize access latency and contention, while ensuring data consistency and scalability.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>ASPLOS</td>
<td>Cornell</td>
<td>Beyond Static Parallel Loops: Supporting Dynamic Task Parallelism on Manycore Architectures with Software-Managed Scratchpad Memories</td>
<td>work-stealing based dynamic task parallelism; stack/task queue in SPM; read-only data duplication</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="memory-optimization-for-graph-processing">Memory Optimization for Graph Processing<a class="headerlink" href="#memory-optimization-for-graph-processing" title="Permanent link">&para;</a></h3>
<p>Challenge: efficiently optimize huge memory requirement from graph processing.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>PPoPP</td>
<td>KAIST</td>
<td>INFINEL: An efficient GPU-based processing method for unpredictable large output graph queries</td>
<td>unpredictable large output queries; one-phase GPU graph processing; kernel stop/restart</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>MICRO</td>
<td>University of Maryland</td>
<td>Bootes: Boosting the Efficiency of Sparse Accelerators Using Spectral Clustering</td>
<td>spectral clustering for sparse matrix row reordering; optimized preprocessing via sparse similarity matrix; decision tree for reordering cost-benefit analysis; ML-based optimal cluster (k) selection</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="llm-memory-management">LLM Memory Management<a class="headerlink" href="#llm-memory-management" title="Permanent link">&para;</a></h3>
<p>Solution: efficient memory management can reduce memory usage, thus enable larger batch size and higher throughput.</p>
<h4 id="memory-management-algorithms">Memory Management Algorithms<a class="headerlink" href="#memory-management-algorithms" title="Permanent link">&para;</a></h4>
<p>Solution: efficient memory management algorithms, like virtual memory, page table, etc.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>SOSP</td>
<td>UCB</td>
<td>Efficient Memory Management for Large Language Model Serving with PagedAttention</td>
<td>Paged KV-Cache management; Better memory management for larger batch size; Preemptive memory scheduling</td>
<td>4</td>
<td>5</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>ASPLOS</td>
<td>Miscrosoft</td>
<td>vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention</td>
<td>use cuda hardware page table instead of vllm's; hack cuda's driver to support page table modify</td>
<td>2</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>SJTU</td>
<td>eLLM: Elastic Memory Management Framework for Efficient LLM Serving</td>
<td>activation weight paged; all scene virtual memory; cpu memory swap</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>SJTU</td>
<td>Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training</td>
<td>offline planning and online patch; MoE-aware reuse pockets; time-aware reuse</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="general-llm-memory-management">General LLM Memory Management<a class="headerlink" href="#general-llm-memory-management" title="Permanent link">&para;</a></h4>
<p>Challenge: LLM memory management faces challenges like limited HBM memory, efficient KV Cache management, memory sharing between multiple GPUs, multi-level memory management.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022</td>
<td>SC</td>
<td>Miscrosoft</td>
<td>DeepSpeed-Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale</td>
<td>kernel fusion; GPU-CPU-NVMe heterogeneous memory; PCIe-based memory prefetch</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>SOSP</td>
<td>THU</td>
<td>Jenga: Effective Memory Management for Serving LLM with Heterogeneity</td>
<td>two-level LCM allocator; customizable prefix caching; request-aware allocation</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>FAST</td>
<td>THU</td>
<td>Mooncake: Trading More Storage for Less Computation  A KVCache-centric Architecture for Serving LLM Chatbot</td>
<td>PD-disaggregate system; kv-cache centered; global kv-cache pool; dynamic SLO scheduler; paged KV-Cache storage</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>SOSP</td>
<td>SJTU</td>
<td>KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider</td>
<td>real-world KVCache trace anatomy; workload-aware eviction; analysis of KVCache hit in the real-world</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<h5 id="application-specific-memory-management">Application specific memory management<a class="headerlink" href="#application-specific-memory-management" title="Permanent link">&para;</a></h5>
<p>Solution: Memory management is actually the core for request scheduling. Application specific memory management use the application information to manage the memory.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UCSD</td>
<td>KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows</td>
<td>agent grapg; prefetch KV Cache from CPU for next agent; agent-aware prefix cache management</td>
<td>2</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>ICML</td>
<td>PKU</td>
<td>MuxServe: Flexible Spatial-Temporal Multiplexing for Multiple LLM Serving</td>
<td>multi-LLM serving; colocating multi LLM in one GPU-instance; CUDA MPS based optimization; PD based scheduler</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>ATC</td>
<td>THU</td>
<td>Weaver: Efficient Multi-LLM Serving with Attention Offloading</td>
<td>GPU memory harvest across GPU; harvest GPU memory and SM for hot LLM attention computation; GPU control flow optimization</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="kv-cache-reuse-systems">KV Cache Reuse Systems<a class="headerlink" href="#kv-cache-reuse-systems" title="Permanent link">&para;</a></h4>
<p>Solution: reduce redundant computation and high memory consumption during inference by allowing the reuse of previously computed key-value pairs for shared or repeated parts of input sequences.</p>
<h5 id="prefix-sharing">Prefix Sharing<a class="headerlink" href="#prefix-sharing" title="Permanent link">&para;</a></h5>
<p>Solution: reuse KV Cache when the input sequence has shared or repeated parts, use prefix tree to manage KV Cache.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>Nips</td>
<td>Stanford</td>
<td>SGLang: Efficient Execution of Structured Language Model Programs</td>
<td>KV-Cache share; python-like DSL; compute graph; LRU cache management stragety</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>ACL</td>
<td>Microsoft</td>
<td>ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition</td>
<td>prefix aware attention compute; manage kv-cache chunks as prefix tree; reduce kv-cache redundancy</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Microsoft</td>
<td>BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix Sharing and Throughput-oriented Token Batching</td>
<td>global prefix tree ahead-of-time; request reorder; horizontal fusioned prefix-shared attention kernel</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Berkeley</td>
<td>BlendServe: Optimizing Offline Inference for Auto-regressive Large Models with Resource-aware Batching</td>
<td>offline batch inference; resource-aware prefix tree; compute-intensive / memory-intensive requests</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>UChicago</td>
<td>DroidSpeak: Enhancing Cross-LLM Communication</td>
<td>selectively layer reuse; communication protocol for inter-agent exchanges; LLMs that share a common foundational model</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>PKU</td>
<td>TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained Elastic Long-Context LLM Serving</td>
<td>bipartite matching dispatching; segment level cache pooling</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h5 id="kv-cache-offloading">KV cache offloading<a class="headerlink" href="#kv-cache-offloading" title="Permanent link">&para;</a></h5>
<p>Solution: offload the KV cache to the memory or other storage device, supporting multi-level storage.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>ATC</td>
<td>Huawei</td>
<td>Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention</td>
<td>store KV cache in the memory; multi level KV cache management; position mask modified</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>SIGCOMM</td>
<td>UChicago</td>
<td>CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving</td>
<td>efficient KV Cache streaming; KV Cache compression; knowledge delivery network; The transfer part of LMCache</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>EuroSys</td>
<td>THU</td>
<td>Fast State Restoration in LLM Serving with HCache</td>
<td>offload activation instead of KVC to reduce IO; use little recomputation to tradeoff offloading latency</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
</tbody>
</table>
<h5 id="position-independent-methid">Position Independent Methid<a class="headerlink" href="#position-independent-methid" title="Permanent link">&para;</a></h5>
<p>Solution: Use recover method to reduce the KV cache size.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>EuroSys</td>
<td>UChicago</td>
<td>CacheBlend: Fast Large Language Model Serving for RAG with  Cached Knowledge Fusion</td>
<td>multiple precomputed text chunks; selective KV recompute; sparsity of attention matrices; The system intro of LMCache</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>ICML</td>
<td>PKU</td>
<td>EPIC: Efficient Position-Independent Caching for Serving Large Language Models</td>
<td>only conpute the first tokens to avoid dynamic selecstion; top-k selection based on attn-sink</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2026</td>
<td>arXiv</td>
<td>Huawei</td>
<td>MEPIC: Memory Efficient Position Independent Caching for LLM Serving</td>
<td>delay position embedding to attn kernel; chunk-based KVC design</td>
<td>2</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2026</td>
<td>arXiv</td>
<td>Huawei</td>
<td>Joint Encoding of KV-Cache Blocks for Scalable LLM Serving</td>
<td>encoding KVC blocks beyond prefix cache; like the blend in CacheBlend</td>
<td>2</td>
<td>2</td>
<td>1</td>
</tr>
</tbody>
</table>
<h5 id="other-techniques">Other Techniques<a class="headerlink" href="#other-techniques" title="Permanent link">&para;</a></h5>
<p>Solution: KV cache reuse techniques beyond prefix sharing. Prefix is a high requirement and is not always possible.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Berkeley</td>
<td>Optimizing LLM Queries in Relational Workloads</td>
<td>prefix sharing maximization; KV cache hit rate; deduplication and cost estimation techniques</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>ICML</td>
<td>UCSD</td>
<td>InferCept: Efficient Intercept Support for Augmented Large Language Model Inference</td>
<td>offload KVC when receive function call; reactive upload</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="kv-cache-storage-systems">KV Cache Storage Systems<a class="headerlink" href="#kv-cache-storage-systems" title="Permanent link">&para;</a></h4>
<p>Solution: efficiently storing and retrieving the key-value cache, thus reuse when needed.</p>
<p>Challenge: the prefetch and eviction of the KV cache, the balance between saving GPU memory and refetching time from the storage device.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>NVIDIA</td>
<td>FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving</td>
<td>block-sparse format; customizable attention template; dynamic load-balanced scheduling framework</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>PKU</td>
<td>FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference</td>
<td>imbalanced KV cache compression mitigation; fair-copying for load balancing; best-effort assignment</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="kv-cache-evict-systems">KV Cache Evict Systems<a class="headerlink" href="#kv-cache-evict-systems" title="Permanent link">&para;</a></h4>
<p>Challenge: selectively discard the least important key-value pairs to free up memory for longer contexts or larger batch sizes without significantly degrading the model's generation quality or increasing computational overhead for the eviction process itself.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>NIPS</td>
<td>UT-Austin</td>
<td>H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models</td>
<td>sparsity for small cache size; heavy-hitters; greedy algorithm for low-cost policy</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Fujitsu</td>
<td>CO2: Precise Attention Score Observation for improving KV Cache Replacement in Large Language Models</td>
<td>long measurement step; decay of the accumulated attention score; adjusting FIFO cache size</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="systems-with-other-caches">Systems with Other Caches<a class="headerlink" href="#systems-with-other-caches" title="Permanent link">&para;</a></h4>
<p>Solution: use other caches (not just KV cache) to improve the performance of LLM inference.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>KAIST</td>
<td>Efficient LLM Inference with Activation Checkpointing and Hybrid Caching</td>
<td>activation checkpointing; KV-activation hybrid caching; balanced approach to determine the best ratio</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h5 id="llm-prefetching">LLM Prefetching<a class="headerlink" href="#llm-prefetching" title="Permanent link">&para;</a></h5>
<p>Solution: prefetch to avoid memory transfer between different devices, reduce the latency of memory access.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Huawei Zurich</td>
<td>PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM Serving</td>
<td>computational graph-based prefetching; prefetch KV cache to L2 cache</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="communication-centric-optimization">Communication-Centric Optimization<a class="headerlink" href="#communication-centric-optimization" title="Permanent link">&para;</a></h2>
<p>Challenge: communication is a bottleneck of some distributed systems, trying to reduce the communication.</p>
<h3 id="io-characterization-and-optimization">I/O Characterization and Optimization<a class="headerlink" href="#io-characterization-and-optimization" title="Permanent link">&para;</a></h3>
<p>Challenge: minimize data movement and maximize resource utilization across heterogeneous distributed environments.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2020</td>
<td>ASPLOS</td>
<td>CMU</td>
<td>Livia: Data-Centric Computing Throughout the Memory Hierarchy</td>
<td>Memory service programming model; task graphs linked to data location; dynamic task/data scheduling for minimal movement</td>
<td>2</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UOregon</td>
<td>Parallel I/O Characterization and Optimization on Large-Scale HPC Systems: A 360-Degree Survey</td>
<td>different HPC I/O stack layers; profiling and tracing tools; tuning echniques</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>ETH</td>
<td>TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents</td>
<td>IO optimization for large scale agent simulation; distributed agent simulation engine; communication optimization for agent scenarios</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="gpu-gpu-communication">GPU-GPU Communication<a class="headerlink" href="#gpu-gpu-communication" title="Permanent link">&para;</a></h3>
<p>Challenge: limited interconnect bandwidth between GPUs using nvLink, PCIe, synchronization delays in parallel workloads, load imbalance across GPUs. The GPU-GPU can also across different nodes.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Apple</td>
<td>SPD: Sync-Point Drop for efficient tensor parallelism of Large Language Models</td>
<td>sync-point drop; block-wise sensitivity analysis; attention output synchronization reduction</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Miscrosoft</td>
<td>Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters</td>
<td>heterogeneous pipeline stages with flexible GPU counts and types; CPU offloading of both parameters and activations</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>SIGCOMM</td>
<td>PKU</td>
<td>InfiniteHBD: Building Datacenter-Scale High-Bandwidth Domain for LLM with Optical Circuit Switching Transceivers</td>
<td>optical circuit switching; node-level fault isolation; optimize TP in DCN network</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h2 id="many-core-systems">Many-Core Systems<a class="headerlink" href="#many-core-systems" title="Permanent link">&para;</a></h2>
<p>Challenge: the heterogeneity of cores, the load imbalance, and the communication overhead.</p>
<h3 id="workload-characterization">Workload Characterization<a class="headerlink" href="#workload-characterization" title="Permanent link">&para;</a></h3>
<p>Challenge: dynamic workloads across numerous cores, resource contention for shared hardware.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2015</td>
<td>VLDB</td>
<td>Intel</td>
<td>GraphMat: High performance graph analytics made productive</td>
<td>vertex program to sparse matrix mapping; generalized SPMV for graph analytics; single-node multicore framework</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2018</td>
<td>SC</td>
<td>Intel</td>
<td>Many-Core Graph Workload Analysis</td>
<td>multicore simulator sniper; selective caching and prefetching; heterogeneous high-performance low-power cores</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2018</td>
<td>DATE</td>
<td>UGA</td>
<td>Parallel Code Generation of Synchronous Programs for a Many-core Architecture</td>
<td>banked memory mapping; worst-case response time analysis</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>IPDPS</td>
<td>UChicago</td>
<td>Optimizing Fine-Grained Parallelism Through Dynamic Load Balancing on Multi-Socket Many-Core Systems</td>
<td>lock-less and concurrent task queue xqueue; distributed tree barrier; NUMA-aware redirect push/work stealing</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="fault-propagation">Fault Propagation<a class="headerlink" href="#fault-propagation" title="Permanent link">&para;</a></h3>
<p>Challenge: one core or component can easily spread to others due to shared resources, leading to system-wide reliability issues. Core counts grow make it hard to predict, detect, and contain errors effectively.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2008</td>
<td>ASPLOS</td>
<td>UIUC</td>
<td>Understanding the Propagation of Hard Errors to Software and Implications for Resilient System Design</td>
<td>stuck-at fault; bridging fault; software failure detection</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2010</td>
<td>PRDC</td>
<td>UBC</td>
<td>Modeling the Propagation of Intermittent Hardware Faults in Programs</td>
<td>instruction based intermittent fault; dynamic dependency graph(DDG) based propagation modeling</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2015</td>
<td>SC</td>
<td>IBM</td>
<td>Understanding the Propagation of Transient Errors in HPC Applications</td>
<td>fault propagation in MPI application; fault classification:V,ONA,WO,PEX,C; fault propagation speed factors</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>ISCA</td>
<td>UChicago</td>
<td>Understanding and Mitigating Hardware Failures in Deep Learning Training Accelerator Systems</td>
<td>NVDLA based fault injection framework; re-execution based light-weight recovery technique; failure effects:SlowDegrade,SharpSlowDegrade,SharpDegrade,LowTestAccuracy</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="fault-injection-technique">Fault Injection Technique<a class="headerlink" href="#fault-injection-technique" title="Permanent link">&para;</a></h3>
<p>Challenge: It is difficult to target specific components, reproduce realistic fault scenarios, and observe system behavior without disturbing normal operation, especially as system scale and complexity increase.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2008</td>
<td>VLSI</td>
<td>DISCA</td>
<td>Enhancement of Fault Injection Techniques Based on the Modification of VHDL Code</td>
<td>saboteurs and mutants technique based fault injection; VHDL level fault-tolerance mechanism</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2014</td>
<td>DSN</td>
<td>UBC</td>
<td>Quantifying the Accuracy of High-Level Fault Injection Techniques for Hardware Faults</td>
<td>fault injection quantification; assembly level fault injection; LLVM compiler based fault injector</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="communication">Communication<a class="headerlink" href="#communication" title="Permanent link">&para;</a></h3>
<p>Challenge: efficiently managing data exchange between a large number of cores, due to limited bandwidth, high latency, and contention in shared resources like interconnects and memory.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UCLM</td>
<td>Understanding intra-node communication in HPC systems and Datacenters</td>
<td>intra- and inter-node simulation model; intra-node network interface bottleneck; impacts of communication pattern</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>MICRO</td>
<td>HKUST(GZ)</td>
<td>Optimizing All-to-All Collective Communication with Fault Tolerance on Torus Networks</td>
<td>HalfRing algorithm for bidirectional bandwidth;DimRotation multi-dimensional scheduling;FoldedRing fault-tolerant algorithm;MATE multi-dimensional acceleration scheduling</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h2 id="heterogeneous-systems">Heterogeneous Systems<a class="headerlink" href="#heterogeneous-systems" title="Permanent link">&para;</a></h2>
<p>Heterogeneous systems are systems that have different types of processors, such as CPUs and GPUs.</p>
<p>Solution: ultilize the heterogeneous resources to improve the performance.</p>
<h3 id="general-applications">General Applications<a class="headerlink" href="#general-applications" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2013</td>
<td>SOSP</td>
<td>MSR Silicon Valley</td>
<td>Dandelion: a Compiler and Runtime for Heterogeneous  Systems</td>
<td>unified programming model; single machine abstraction; a rich object-oriented programming language for data-parallel computing</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>EuroSys</td>
<td>SJTU</td>
<td>Improving GPU Sharing Performance through Adaptive Bubbleless Spatial-Temporal Sharing</td>
<td>Bubble-less spatial-temporal sharing; kernel squad scheduling; fine-grained concurrent kernel management</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>ISPASS</td>
<td>CMU</td>
<td>Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Coupled Architectures</td>
<td>Effective regions for balanced utilization of PUs; Proximity-based kernel fusion recommendation; operator-kernel dependency graphs from PyTorch Profiler traces</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>OSDI</td>
<td>SJTU</td>
<td>XSched: Preemptive Scheduling for Diverse XPUs</td>
<td>a unified XPU usage interface layer; queue abstraction turns every XPU into a preemptible command queue; enable microsecond preemption on closed-source CUDA kernels</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="decentralized-serving">Decentralized Serving<a class="headerlink" href="#decentralized-serving" title="Permanent link">&para;</a></h3>
<p>Challenge: managing diverse hardware and software environments, balancing workloads across uneven resources, minimizing communication overhead, ensuring consistency without centralized control.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2019</td>
<td>ASPLOS</td>
<td>USC</td>
<td>Hop: Heterogeneity-aware Decentralized Training</td>
<td>iteration gap; queue-based synchronization; backup workers and bounded staleness</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2020</td>
<td>ASPLOS</td>
<td>USC</td>
<td>Prague: High-Performance Heterogeneity-Aware Asynchronous Decentralized Training</td>
<td>Partial All-Reduce to reduce synchronization cost; group scheduling to avoid conflicts</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Berkeley</td>
<td>DeServe: Towards Affordable Offline LLM Inference via Decentralization</td>
<td>decentralized LLM inference; high-latency optimization; idle GPU utilization; modular on-chain integration</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>HKUST</td>
<td>DreamDDP: Accelerating Data Parallel Distributed LLM Training with Layer-wise Scheduled Partial Synchronization</td>
<td>partial synchronization based local SGD; DFS algorithm with pruned search space; enables the opportunity of overlapping communication and computation</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="ml-training-systems">ML Training Systems<a class="headerlink" href="#ml-training-systems" title="Permanent link">&para;</a></h3>
<p>Solution: balance between faster training and high precision.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>SOSP</td>
<td>CMU</td>
<td>Sia: Heterogeneity-aware, goodput-optimized ML-cluster scheduling</td>
<td>heterogeneity-aware and adaptivity-aware; ILP formulation for scheduling; bootstrapped from observing just a few mini-batches</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>SOSP</td>
<td>Rice&amp;AWS</td>
<td>Gemini: Fast Failure Recovery in Distributed Training with In-Memory Checkpoints</td>
<td>holistic trace analysis; cross-layer failure detection; checkpoint partition algorithm</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="llm-inference-heterogeneous-systems">LLM Inference Heterogeneous Systems <a name="LLM-Inference-Heterogeneous-Systems"></a><a class="headerlink" href="#llm-inference-heterogeneous-systems" title="Permanent link">&para;</a></h3>
<p>Solution: managing diverse hardware and software environments, balancing workloads across uneven resources, meeting the SLO.</p>
<h4 id="mobile-edge-network-serving">Mobile &amp; Edge-Network Serving<a class="headerlink" href="#mobile-edge-network-serving" title="Permanent link">&para;</a></h4>
<p>Challenge: limited computation, memory, power coupled with intermittent and unreliable network connectivity, making it difficult to perform computationally intensive training tasks, manage large datasets, and ensure efficient communication and synchronization across distributed edge nodes.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>UIC</td>
<td>Priority-Aware Model-Distributed Inference at Edge Networks</td>
<td>priority-aware model distributed inference algorithm; prioritization of ML inference tasks; model-distributed inferencing mechanism</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Yonsei</td>
<td>Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large Language Models</td>
<td>hybrid language model; selectively skip uplink transmissions; uncertainty-aware</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>UMD</td>
<td>Distributed Mixture-of-Agents for Edge Inference with Large Language Models</td>
<td>Mixture-of-Agents; semantics of the data being gossiped and its timeliness; queuing stability</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>PKU</td>
<td>SplitLLM: Hierarchical Split Learning for Large Language Model over Wireless Network</td>
<td>hierarchical split learning; edge-cloud collaboration; LoRA adapter update</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>SJTU</td>
<td>HeteroLLM: Accelerating Large Language Model Inference on Mobile SoCs platform with Heterogeneous AI Accelerators</td>
<td>both layer-level and tensor-level GPU-NPU parallelism; different tensor partition strategies; fast synchronization mechanism based on predictable kernel waiting times; tensor partition solver</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="gpu-gpu-heterogeneous-system">GPU-GPU Heterogeneous System<a class="headerlink" href="#gpu-gpu-heterogeneous-system" title="Permanent link">&para;</a></h4>
<p>Solution: the system is composed of heterogeneous GPUs and not inferencing on CPU/ The system need to manage the heterogeneous GPUs' communication and memory.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>CMU</td>
<td>Helix: Distributed Serving of Large Language Models via Max-Flow on Heterogeneous GPUs</td>
<td>LLM model placement as a max-flow problem; per-request pipeline; mixed integer linear programming</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>ICLR</td>
<td>HKUST</td>
<td>HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous Environment</td>
<td>a combination of graph partitioning and max-flow algorithm; TP and PP with disaggregation; bottleneck and underutilized edges; swap edges</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UM</td>
<td>Hetis: Serving LLMs in Heterogeneous GPU Clusters with Fine-grained and Dynamic Parallelism</td>
<td>dynamic parallelism design; LLM module level optimization; selective parallelism for dense modules</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="xpu-gpu-heterogeneous-system">XPU-GPU Heterogeneous System<a class="headerlink" href="#xpu-gpu-heterogeneous-system" title="Permanent link">&para;</a></h4>
<p>Challenge: effectively managing and coordinating diverse hardware (CPUs, TPUs, etc.), interconnects, and memory hierarchies</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>ICML</td>
<td>Stanford</td>
<td>FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU</td>
<td>dynamic offload tensor; quantize the weights to 4-bits; linear aggregation of the store and load operations</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>CMU</td>
<td>Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Coupled Architectures</td>
<td>SKIP profiling tool; TKLQT metric for CPU/GPU boundedness; proximity score kernel fusion</td>
<td>2</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>SPAA</td>
<td>Huawei</td>
<td>WindVE: Collaborative CPU-NPU Vector Embedding</td>
<td>seamless CPU-NPU collaboration for vector embedding; linear regression based estimator; high-throughput offloading vector embedding</td>
<td>2</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Huawei</td>
<td>High-Throughput LLM inference on Heterogeneous Clusters</td>
<td>lightweight profiling while avoiding resource-intensive throughput benchmarks; a scheduler that accounts for both instance computational capacity and memory usage; exhaustive search method</td>
<td>2</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>KAIST</td>
<td>EOD: Enabling Low Latency GNN Inference via Near-Memory Concatenate Aggregation</td>
<td>concatenated ZVC compression; precomputation for neighborhood explosion problem</td>
<td>2</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>UIUC</td>
<td>LIA: A Single-GPU LLM Inference Acceleration with Cooperative AMX-Enabled CPU-GPU Computation and CXL Offloading</td>
<td>heterogeneous LLM offloading via AMX; optimal compute-offloading policy for LLM inference; CXL-DDR hybrid memory offloading for throughput</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="heterogeneous-device-task-scheduling">Heterogeneous Device Task Scheduling<a class="headerlink" href="#heterogeneous-device-task-scheduling" title="Permanent link">&para;</a></h4>
<p>Solution: assigning different parts of the LLM serving workload to the most suitable heterogeneous devices to maximize throughput and minimize latency.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>PACT</td>
<td>Yonsei</td>
<td>Virtual PIM: Resource-aware Dynamic DPU Allocation and Workload Scheduling Framework for Multi-DPU PIM Architecture</td>
<td>dynamic DPU allocation for multitasking; fine-grained scheduling</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>NUS</td>
<td>Data-aware Dynamic Execution of Irregular Workloads on Heterogeneous Systems</td>
<td>lightweight and input-aware framework; multiobjective and multi-constraint design space; dynamically creating optimal schedules</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>Samsung</td>
<td>PAISE: PIM-Accelerated Inference Scheduling Engine for Transformer-based LLM</td>
<td>task scheduling algorithm across host and PIM; interleave-batched GEMM; data layout adjustment</td>
<td>2</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h5 id="task-scheduling-for-specific-tasks">Task Scheduling for specific tasks<a class="headerlink" href="#task-scheduling-for-specific-tasks" title="Permanent link">&para;</a></h5>
<p>Solution: In specific scene, the schedule goal is different. Assigning tasks to differnet devices can fix the gap between the characteristics of devices' and tasks'.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>HPCA</td>
<td>Princeton</td>
<td>Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications</td>
<td>distributed data-local tiled architecture; task-based programming for pointer indirection; traffic-aware task scheduling with headerless NoC</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>AAMAS</td>
<td>MIT</td>
<td>flame: A Framework for Learning in Agent-based ModEls</td>
<td>differentiable agent-based modeling framework via autograd primitives; hybrid DNN-ABM (Agent-Based Models) optimization pipeline</td>
<td>3</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Georgia Tech</td>
<td>HARP: A Taxonomy for Heterogeneous and Hierarchical Processors for Mixed-reuse Workloads</td>
<td>a taxonomy to classify the heterogeneous and hierarchical accelerators; characterize hardware organization of different accelerators; classify based on relative location of sub-accelerators</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>PKU</td>
<td>Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC</td>
<td>agent application-specific scheduling on heterogeneous SoC; heterogeneous execution graph with eastic kernels; bandwidth-aware dispatch for NPU-iGPU contention mitigation</td>
<td>3</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Stanford</td>
<td>Efficient and Scalable Agentic AI with Heterogeneous Systems</td>
<td>mapping agent worloads to heterogeneous accelerators; MLIR based excution graph; cost model based device choose</td>
<td>2</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<h3 id="llm-training-heterogeneous-systems">LLM Training Heterogeneous Systems<a class="headerlink" href="#llm-training-heterogeneous-systems" title="Permanent link">&para;</a></h3>
<p>Solution: compared to <a href="./#LLM-Inference-Heterogeneous-Systems">LLM Inference Heterogeneous Systems</a>, need to solve the backward compatibility and heterogeneity issues.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>PKU</td>
<td>Demystifying Workload Imbalances in Large Transformer Model Training over Variable-length Sequences</td>
<td>data sampling imbalance; data packing imbalance; subgraph abstraction</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Ant Group</td>
<td>EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large Language Models</td>
<td>Local Stochastic Gradient Descent (Local SGD); consistent stragglers within heterogeneous devices; hierarchical distribution strategy on a two-dimensional device mesh; layer by layer forward syncing; pseudo-gradient penalty method</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>ZJU</td>
<td>Frenzy: A Memory-Aware Serverless LLM Training System for Heterogeneous GPU Clusters</td>
<td>efficient and low-overhead task-to-cluster scheduling; bin-packing algorithms; seamless and user-friendly</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>OSU</td>
<td>Scaling Large Language Model Training on Frontier with Low-Bandwidth Partitioning</td>
<td>low-bandwidth interconnects; three-level hierarchical partitioning strategy; improved hierarchical partitioning on top of ZeRO++</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>PKU</td>
<td>Split Fine-Tuning for Large Language Models in Wireless Networks</td>
<td>split fine-tuning; device and server partition; novel compression scheme and resource management algorithm</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Neuchatel</td>
<td>SkipPipe: Partial and Reordered Pipelining Framework for Training LLMs in Heterogeneous Networks</td>
<td>partial pipeline parallelism; stage skipping; path scheduling algorithm</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="schedule-optimization">Schedule Optimization<a class="headerlink" href="#schedule-optimization" title="Permanent link">&para;</a></h2>
<p>Solution: develop task schedule algorithms, to achieve efficient overall system performance despite incomplete and evolving system state information.performance.</p>
<h3 id="general-task-scheduling">General Task Scheduling<a class="headerlink" href="#general-task-scheduling" title="Permanent link">&para;</a></h3>
<p>Solution: optimizing the allocation and execution of diverse and dynamic workloads.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2019</td>
<td>NSDI</td>
<td>MIT</td>
<td>Shinjuku: Preemptive Scheduling for second-scale Tail Latency</td>
<td>preemptive scheduling; single-address space OS; hardware-supported virtualization</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2021</td>
<td>SOSP</td>
<td>UPenn</td>
<td>When Idling is Ideal: Optimizing Tail-Latency for Heavy-Tailed Datacenter Workloads with Persphone</td>
<td>reserve cores; non-conserving; request dispatching algorithm</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2017</td>
<td>HPCA</td>
<td>UGent</td>
<td>Reliability-Aware Scheduling on Heterogeneous Multicore Processors</td>
<td>core reliability characteristics difference; system soft error rate; sampling-based reliability-aware scheduling algorithm</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2020</td>
<td>TCAD</td>
<td>ASU</td>
<td>Runtime Task Scheduling Using Imitation Learning for Heterogeneous Many-Core Systems</td>
<td>offline Oracle optimizaion strategy; hierarchical imitation learning based scheduling; two-level scheduling</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="speculative-execution-non-llm">Speculative Execution (Non-LLM) <a name="Speculative-Execution-Non-LLM"></a><a class="headerlink" href="#speculative-execution-non-llm" title="Permanent link">&para;</a></h3>
<p>Solution: balancing the potential performance gains from speculative executions, including accurately predicting outcomes, handling incorrect speculations and their side effects across multiple nodes.</p>
<p>Refer to <a href="./#Speculative-Execution-LLM">Speculative Execution</a> for the speculative execution algorithms for LLM.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>MSR</td>
<td>Forerunner: Constraint-based Speculative Transaction Execution for Ethereum</td>
<td>constraint-based speculative transaction execution; many-future nature; specialized fast-path program</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Politecnico di Milano</td>
<td>Minimizing speculation overhead in a parallel recognizer for regular texts</td>
<td>speculation overhead; chunk automaton; reduced-interface DFA</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="llm-related-scheduling">LLM-Related Scheduling <a name="LLM-Related-Scheduling"></a><a class="headerlink" href="#llm-related-scheduling" title="Permanent link">&para;</a></h3>
<p>Challenge: efficiently managing the immense computational and memory demands of training and inference across numerous interconnected devices, requiring sophisticated strategies to partition massive models.</p>
<h4 id="llm-request-scheduling">LLM Request Scheduling<a class="headerlink" href="#llm-request-scheduling" title="Permanent link">&para;</a></h4>
<p>Solution: develop intelligent strategies to route requests, prioritize urgent or critical tasks, handle varying input lengths and complexities, manage resource contention to meet the SLO requirements.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Yale</td>
<td>TimelyLLM: Segmented LLM Serving System for Time-sensitive Robotic Applications</td>
<td>segmented generation; time-sensitive scheduling; latency-guided batch size selection</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>MSRI</td>
<td>Niyama : Breaking the Silos of LLM Inference Serving</td>
<td>QoS-driven LLM inference serving system; co-scheduling requests with diverse QoS targets on a shared rather than siloed infrastructure; allows graceful service degradation during overload conditions; deadline slack; a hybrid prioritization and an eager relegation policy</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>MIT</td>
<td>Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints</td>
<td>fluid dynamics approximation; Waiting for Accumulated Inference Threshold; a hierarchical framework comprising multiple segments</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>PKU</td>
<td>SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large Language Model Inference</td>
<td>service-aware and latency-optimized scheduling algorithm; doubling budget (DB) scheduling algorithm; search-based placement algorithm</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>SOSP</td>
<td>UChicago</td>
<td>PrefillOnly: An Inference Engine for Prefill-only Workloads in Large Language Model Applications</td>
<td>prefill-only workload; hybrid prefilling; continuous JCT calibration; JCT-aware scheduling</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="batching-for-better-throughput">Batching for Better Throughput<a class="headerlink" href="#batching-for-better-throughput" title="Permanent link">&para;</a></h4>
<p>Challenge: the batching is a trade-off between the throughput and the latency. The requests in the same batch also call for different resources. Split different compute pattern request for different batches is a good idea.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>OSDI</td>
<td>UA</td>
<td>NanoFlow: Towards Optimal Large Language Model Serving Throughput</td>
<td>LLM inference is indeed compute-bound; model-based batch size analysis; micro batch split for overlapping between all resources</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>BUAA</td>
<td>OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving</td>
<td>colocating online request and offline LLM request; SLO based schedule method</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h5 id="info-predict-scheduling">Info Predict Scheduling<a class="headerlink" href="#info-predict-scheduling" title="Permanent link">&para;</a></h5>
<p>Challenge: The general schedule if for better batching and meeting the SLO requirements. By predicting the information of the requests, we can make the schedule more efficient.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>Nips</td>
<td>Harvard</td>
<td>S3: Increasing GPU Utilization during Generative Inference for Higher Throughput</td>
<td>predict the length of LLM request to fixed types; Orca based dynamic batching</td>
<td>3</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>ASPLOS</td>
<td>UIUC</td>
<td>Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction</td>
<td>length prediction; left time prediction; bert-based proxy model</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="llm-application-level-scheduling">LLM Application-Level Scheduling<a class="headerlink" href="#llm-application-level-scheduling" title="Permanent link">&para;</a></h4>
<p>Solution: to optimize the end-to-end latency of the application, including the scheduling of the LLM instances.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>OSDI</td>
<td>SJTU</td>
<td>Parrot: Efficient Serving of LLM-based Applications with Semantic Variable</td>
<td>Semantic Variable; application-level information; LLM applications as first-class citizens</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>OSDI</td>
<td>CUHK</td>
<td>Teola: Towards End-to-End Optimization of LLM-based Applications</td>
<td>mismatch between request-level scheduling and end-to-end  application performance; primitive-level dataflow graph; two-tier scheduling mechanism</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Yext</td>
<td>SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to Question Answering</td>
<td>constantly changing and sometimes adverse conditions; Dynamically Reconfigurable Horizontal Scaling Framework; dynamically adjust resource allocation based on query requirements</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Berkeley</td>
<td>Autellix: An Efficient Serving Engine for LLM Agents as General Programs</td>
<td>formalize agentic programs as dynamic, non-deterministic DAGs; non-clairvoyant scheduler; simple load-balancing policy to balance data locality and KV-cache recomputation</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>ICDCS</td>
<td>SJTU</td>
<td>LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM Applications</td>
<td>a DAG with regular stage, LLM stage, dynamic stage; bayesian network-based profiler; identify uncertainty-reducing stages</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>SJTU</td>
<td>Efficient Serving of LLM Applications with Probabilistic Demand Modeling</td>
<td>DAG-based scheduling; dynamic excution; cpu excutor warmup</td>
<td>3</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<h4 id="llm-speculative-inference">LLM Speculative Inference <a name="Speculative-Execution-LLM"></a><a class="headerlink" href="#llm-speculative-inference" title="Permanent link">&para;</a></h4>
<p>Refer to non-LLM <a href="./#Speculative-Execution-Non-LLM">speculative execution</a>.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>F&amp;M College</td>
<td>AMUSD: Asynchronous Multi-Device Speculative Decoding for LLM Acceleration</td>
<td>simultaneous and independent predictions; asynchronous speculative decoding; rollback mechanism</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Purdue</td>
<td>Constrained Decoding with Speculative Lookaheads</td>
<td>computational expense of generating lookaheads; speculated lookaheads; task specific reward function</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Rutgers</td>
<td>Interactive Speculative Planning: Enhance Agent Efficiency through Co-design of System and User Interface</td>
<td>active user intervention; speculative planning algorithm; UI-level rescheduling algorithm</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>USTC</td>
<td>Parallel Speculative Decoding with Adaptive Draft Length</td>
<td>adaptive draft length; pre-verify and post-verify; draft-then-verify framework; mutual waiting problem</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>SEU</td>
<td>SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative Decoding</td>
<td>reasoning tree construction; parallel drafting with speculative decoding; FCFS queue verification</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h5 id="spec-others">Spec + Others<a class="headerlink" href="#spec-others" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Huawei</td>
<td>Speculative MoE: Communication Efficient Parallel MoE Inference with Speculative Token and Expert Pre-scheduling</td>
<td>speculative MoE; speculative token shuffling; speculative expert pre-grouping</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>INFOCOM</td>
<td>UoA</td>
<td>SPIN: Accelerating Large Language Model Inference with Heterogeneous Speculative Models</td>
<td>internal neurons sparsification; model-agnostic acceleration framework; dynamic early-exit thresholds; multi-layered feature fusion</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXic</td>
<td>SUST</td>
<td>FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference</td>
<td>SPEC on memory limited dedvices; Efficient draft management with tree pruning and early stop reduces redundancy and maintains causal relationships</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="llm-serving-outages-and-incidents">LLM Serving Outages and Incidents<a class="headerlink" href="#llm-serving-outages-and-incidents" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Vrije Universiteit Amsterdam</td>
<td>An Empirical Characterization of Outages and Incidents in Public Services for Large Language Models</td>
<td>empirical characterization of outages; failure recovery optimization; public LLM service reliability</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="energy-optimized-llm-scheduling">Energy-Optimized LLM Scheduling<a class="headerlink" href="#energy-optimized-llm-scheduling" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UvA</td>
<td>GREEN-CODE: Optimizing Energy Efficiency in Large Language Models for Code Generation</td>
<td>dynamic early exit; energy-aware code generation; reinforcement learning for llms</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="multi-llm-scheduling">Multi-LLM Scheduling<a class="headerlink" href="#multi-llm-scheduling" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UCLA</td>
<td>Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving</td>
<td>Long-tail model popularity; Frequent idle periods; Rapid workload fluctuations</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="dnn-scheduling">DNN Scheduling<a class="headerlink" href="#dnn-scheduling" title="Permanent link">&para;</a></h3>
<p>Solution: optimizing data parallelism and model parallelism while minimizing communication overhead between nodes, effectively managing limited GPU memory and other resources to achieve scalability and high throughput.</p>
<p>Refer to <a href="./#LLM-Related-Scheduling">LLM-Related Scheduling</a> for the LLM-related scheduling algorithms.</p>
<h4 id="task-offloading">Task Offloading<a class="headerlink" href="#task-offloading" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>USTC</td>
<td>Collaborative Inference for Large Models with Task Offloading and Early Exiting</td>
<td>early exit mechanism; jointly optimize its offloading strategy and the confidence threshold; distributed task offloading algorithm</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>ETHZ</td>
<td>OptiPIM: Optimizing Processing-in-Memory Acceleration Using Integer Linear Programming</td>
<td>integer linear programming for offload optimization; PIM-friendly mapping representation; accurate cost modeling for data layout</td>
<td>4</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="thermal-aware-scheduling">Thermal-Aware Scheduling<a class="headerlink" href="#thermal-aware-scheduling" title="Permanent link">&para;</a></h3>
<p>Challenge: balancing real-time performance with thermal safety under tight power budgets</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2012</td>
<td>TVLSI</td>
<td>SU</td>
<td>A Multi-Agent Framework for Thermal Aware Task Migration in Many-Core Systems</td>
<td>steady-state temperature-based migration; temperature prediction-based migration; master-slave communication</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2015</td>
<td>TVLSI</td>
<td>UCR</td>
<td>Task Migrations for Distributed Thermal Management Considering Transient Effects</td>
<td>effective initial temperature; frequency-domain moment matching</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2019</td>
<td>TVLSI</td>
<td>Soton</td>
<td>Predictive Thermal Management for Energy-Efficient Execution of Concurrent Applications on Heterogeneous Multicores</td>
<td>temperature predictor with error correction algorithm; dynamic temperature management</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2019</td>
<td>TECS</td>
<td>UMich</td>
<td>Thermal-Aware Scheduling for Integrated CPUsGPU Platforms</td>
<td>thermally-balanced task-to-core assignment; CPUsGPU thermal coupling model</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2022</td>
<td>TPDS</td>
<td>SUT</td>
<td>TherMa-MiCs: Thermal-Aware Scheduling for Fault-Tolerant Mixed-Criticality Systems</td>
<td>thermal safe power abstraction; maximum safe simultaneous active cores (MSSAC) factor; offline&amp;online co-scheduling</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="dark-silicon-optimization">Dark Silicon Optimization<a class="headerlink" href="#dark-silicon-optimization" title="Permanent link">&para;</a></h3>
<p>Challenge: maximizing performance and reliability within dark silicon constraints</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2015</td>
<td>DATE</td>
<td>NYU</td>
<td>Variability-Aware Dark Silicon Management in On-Chip Many-Core Systems</td>
<td>variability-aware dark core patterning; joint thread-to-core mapping and power-state control; temperature-dependent leakage feedback modeling</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2018</td>
<td>DATE</td>
<td>NTU</td>
<td>HiMap: A Hierarchical Mapping Approach for Enhancing Lifetime Reliability of Dark Silicon Manycore Systems</td>
<td>two-level hierarchical mapping for lifetime reliability; block-based selection; cluster-based adjustment</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2019</td>
<td>DAC</td>
<td>NTU</td>
<td>LifeGuard: A Reinforcement Learning-Based Task Mapping Strategy for Performance-Centric Aging Management</td>
<td>performance- and aging-aware objective formulation; reactiondiffusion (RD)-based NBTI aging model</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="rl-based-scheduling">RL-based Scheduling<a class="headerlink" href="#rl-based-scheduling" title="Permanent link">&para;</a></h3>
<p>Solution: learn high-quality scheduling policies via trial and error without relying on minimal manual interventions or expert knowledge</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2020</td>
<td>SC</td>
<td>UNCC</td>
<td>RLScheduler: An Automated HPC Batch Job Scheduler Using Reinforcement Learning</td>
<td>kernel-based neural network; Actor-Critic structure for efficient training; trajectory filtering mechanism</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2021</td>
<td>ICPADS</td>
<td>UCAS</td>
<td>Deep Reinforcement Agent for Failure-aware Job scheduling in High-Performance Computing</td>
<td>usage status encoding; make-span based reward function; iterative inference for multi-gpu scene</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2022</td>
<td>TPDS</td>
<td>IIT</td>
<td>DRAS: Deep Reinforcement Learning for Cluster Scheduling in High Performance Computing</td>
<td>hierarchical neural network to incorporate HPC key features;three-phase training process for rapid convergence</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2022</td>
<td>HPDC</td>
<td>UNCC</td>
<td>SchedInspector: A Batch Job Scheduling Inspector Using Reinforcement Learning</td>
<td>queue delays for overall loss evaluation; cluster availability; percentage reward function</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="rl-based-mapping">RL-based Mapping<a class="headerlink" href="#rl-based-mapping" title="Permanent link">&para;</a></h3>
<p>Solution: learn high-quality mapping policies via trial and error without relying on minimal manual interventions or expert knowledge</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>HPCA</td>
<td>CQU&amp;THU</td>
<td>E2EMap: End-to-End Reinforcement Learning for CGRA Compilation via Reverse Mapping</td>
<td>reverse mapping for exploration space reduction; GCN-based state encoding; masking function to avoid invalid action</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>ASPLOS</td>
<td>NUS</td>
<td>Enhancing CGRA Efficiency Through Aligned Compute and Communication Provisioning</td>
<td>hierarchical execution with motifs; plaid collective unit; motif generation algorithm</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>TPDS</td>
<td>THU</td>
<td>Raccoon: Lightweight Support for Comprehensive Control Flows in Reconfigurable Spatial Architectures</td>
<td>loop unit; predicate control unit; loop- and branch-level ISA; workqueue/subgraph partition algorithm</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<h2 id="general-optimizations-for-deep-learning-systems">General optimizations for Deep Learning Systems<a class="headerlink" href="#general-optimizations-for-deep-learning-systems" title="Permanent link">&para;</a></h2>
<p>Solution: general optimizations for deep learning systems.</p>
<p>If the paper is focusing on an above-mentioned specific scene (e.g., memory, scheduling, IO, etc.), it will be put in the corresponding section.</p>
<h3 id="llm-training-systems">LLM Training Systems<a class="headerlink" href="#llm-training-systems" title="Permanent link">&para;</a></h3>
<p>Solution: arrange model parameters and data across multiple devices, reduce the time spent communicating, scale up smoothly as models and data keep growingall while staying efficient and speeding up training.</p>
<h4 id="general-optimizations">General Optimizations<a class="headerlink" href="#general-optimizations" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>THU</td>
<td>Enhancing Memory Efficiency in Large Language Model Training Through Chronos-aware Pipeline Parallelism</td>
<td>chronos-aware pipeline parallelism; temporal locality optimization; activation balancing</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>NUS</td>
<td>PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization</td>
<td>selective offload strategy; memory offload optimization; pipeline parallelism scalability; lifespan-based offloading</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UCSD</td>
<td>WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training</td>
<td>workload-aware variable-length document packing; per-document sharding strategy; adaptive sharding selection mechanism; delay execution of extremely long documents</td>
<td>4</td>
<td>5</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>EuroSys</td>
<td>UToronto</td>
<td>Mist: Efficient Distributed Training of Large Language Models via Memory-Parallelism Co-Optimization</td>
<td>fine-grained overlap-centric scheduling; symbolic-based performance analysis; imbalance-aware hierarchical tuning</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>ByteDance</td>
<td>veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD</td>
<td>guaranteed single device consistency; zero overhead tensor alloc mode; fused communication</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>PPoPP</td>
<td>THU</td>
<td>WeiPipe: Weight Pipeline Parallelism for Communication-Effective Long-Context Large Model Training</td>
<td>weight-passing pipeline parallelism; long-context training; communication optimization</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="optimizations-on-special-scene">Optimizations on Special Scene<a class="headerlink" href="#optimizations-on-special-scene" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>HKU</td>
<td>Hecate: Unlocking Efficient Sparse Model Training via Fully Sharded Sparse Data Parallelism</td>
<td>Fully Sharded Sparse Data Parallelism (FSSDP); sparsely materializes MoE parameters; two sparse collective communications</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>SJTU</td>
<td>PipeWeaver: Addressing Data Dynamicity in Large Multimodal Model Training with Dynamic Interleaved Pipeline</td>
<td>dynamic interleaved pipeline; hierarchical schedule space for rapid pipeline schedule search; spatialtemporal subgraph reuse</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="experiments">Experiments<a class="headerlink" href="#experiments" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>JSC</td>
<td>Memory and Bandwidth are All You Need for Fully Sharded Data Parallel</td>
<td>an extensive analysis of the FSDP training distribution strategy; a grid search methodology; both simulation and empirical results</td>
<td>2</td>
<td>4</td>
<td>1</td>
</tr>
</tbody>
</table>
<h4 id="multi-modal-optimizations">Multi-Modal Optimizations<a class="headerlink" href="#multi-modal-optimizations" title="Permanent link">&para;</a></h4>
<p>Challenge: multimodal data is more complex and requires more resources to train.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>Meta</td>
<td>Characterizing and Efficiently Accelerating Multimodal Generation Model Inference</td>
<td>system performance characterization for multi-modal generative AI tasks; optimized baseline for inference optimization</td>
<td>4</td>
<td>3</td>
<td>1</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>ByteDance</td>
<td>OrchMLLM: Orchestrate Multimodal Data with Batch Post-Balancing to Accelerate Multimodal Large Language Model Training</td>
<td>multimodal mini-batch imbalance; batch post-balancing algorithm; node-wise all-to-all communicator for practical rearrangement of mini-batches</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>ICT</td>
<td>ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism</td>
<td>unified prefix cache fusing vision and text tokens; modality-aware load balancer for bursty vision traffic</td>
<td>2</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>NSDI</td>
<td>OSU&amp;&amp;Amazon</td>
<td>DISTMMAccelerating Distributed Multimodal Model Training</td>
<td>Heterogeneous Submodules;Modality-aware Partitioner;Data Load Balancer;Heterogeneity-aware Placement Manager;Pipeline Executor</td>
<td>5</td>
<td>5</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>ATC</td>
<td>Harvard&amp;&amp;ByteDance&amp;&amp;USC</td>
<td>Optimus: Accelerating Large-Scale Multimodal LLM Training by Bubble Exploitation</td>
<td>Separate Parallel Plans;Dual-Stage Dependency Management;Kernel-Level Bubble Scheduling;Schedule encoder computations within LLM bubbles</td>
<td>4</td>
<td>5</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="kernel-level-optimizations">Kernel-Level Optimizations<a class="headerlink" href="#kernel-level-optimizations" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>HUST</td>
<td>CFP: Low-overhead Profiling-based Intra-operator Parallelism Generation by Preserving Communication-Free Structures</td>
<td>model segment profile-based cost model; communication-free tensor partition propagation property; extracting a set of unique model segments; Communication-Free Preserve</td>
<td>4</td>
<td>5</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="sft-and-fine-tuning-optimizations">SFT and Fine-tuning Optimizations<a class="headerlink" href="#sft-and-fine-tuning-optimizations" title="Permanent link">&para;</a></h4>
<p>Challenge: SFT plays an important role to enhance model's ability on specific tasks. Thus SFT has a strong taks's pattern, like the distribution of input-output length.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>PKU</td>
<td>LobRA: Multi-tenant Fine-tuning over Heterogeneous Data</td>
<td>Deploys different model replicas to handle varied workloads; balancing different LoRAs' workload caused by dataset</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="fault-tolerance-optimizations">Fault tolerance Optimizations<a class="headerlink" href="#fault-tolerance-optimizations" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>PPoPP</td>
<td>University of Oregon</td>
<td>ATTNChecker: Highly-Optimized Fault Tolerant Attention for Large Language Model Training</td>
<td>Algorithm-Based Fault Tolerance (ABFT); soft error correction; training reliability; INF/NaN error handling</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UCR</td>
<td>FT-Transformer: Resilient and Reliable Transformer with End-to-End Fault Tolerant Attention</td>
<td>Flashattn+ABFT; GPU based runtime optimization</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2021</td>
<td>SC</td>
<td>CMU</td>
<td>Arithmetic-intensity-guided fault tolerance for neural network inference on GPUs</td>
<td>do ABFT on every thread computation to reduce memory sync; arithmetic-intensity-guided fault detection</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h5 id="ml-sparse-related-optimizaions">ML Sparse related optimizaions<a class="headerlink" href="#ml-sparse-related-optimizaions" title="Permanent link">&para;</a></h5>
<p>Challenge: ML models show that some errors are not critical to the model's performance. Can use this to reduce the fault tolerance overhead.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>HPDC</td>
<td>GMU</td>
<td>FT2: First-Token-Inspired Online Fault Tolerance on Critical Layers for Generative Large Language Models</td>
<td>inference-oriented error detection; clip based fault recovery; first token get range</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Gensyn</td>
<td>All is Not Lost: LLM Recovery without Checkpoints</td>
<td>replace two layer's value to recover from fault; out-of-order pipeline execution to tolerate crash</td>
<td>2</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UCAS</td>
<td>ApproxABFT: Approximate Algorithm-Based Fault Tolerance for Neural Network Processing</td>
<td>adaptive error tolerance thresholds; reduction in error detection overhead</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2023</td>
<td>VLSI</td>
<td>ICT</td>
<td>Soft Error Reliability Analysis of Vision Transformers</td>
<td>error injection experiment on ViT; block-wise ABFT optimizations</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="llm-inference-systems">LLM Inference Systems<a class="headerlink" href="#llm-inference-systems" title="Permanent link">&para;</a></h3>
<p>Focusing on the optimizations for LLM inference systems.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>DeepSeek</td>
<td>Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures</td>
<td>software-hardware co-design for deepseek-v3; insight into hardware for ai architectures</td>
<td>5</td>
<td>5</td>
<td>4</td>
</tr>
<tr>
<td>2024</td>
<td>Mlsys</td>
<td>SJTU</td>
<td>FlashDecoding++: Faster Large Language Model Inference on GPUs</td>
<td>asynchronized softmax with unified max value; flat GEMM optimization with double buffering; heuristic dataflow with hardware resource adaptation</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>MICRO</td>
<td>UC Irvine</td>
<td>SCAR: Scheduling Multi-Model AI Workloads on Heterogeneous Multi-Chiplet Module Accelerators</td>
<td>two-level scheduling framework; greedy layer packing algorithm for time window assignment; inter-chiplet pipelining and dynamic chiplet regrouping</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="slo-aware-systems">SLO-Aware Systems<a class="headerlink" href="#slo-aware-systems" title="Permanent link">&para;</a></h4>
<p>Challenge: providing service for users to meet specific latency requirements with limited resources.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Berkeley</td>
<td>AdaServe: SLO-Customized LLM Serving with Fine-Grained Speculative Decoding</td>
<td>fine-grained speculative decoding; token tree verification; slo customization</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UIUC</td>
<td>HyGen: Efficient LLM Serving via Elastic Online-Offline Request Co-location</td>
<td>online-offline request co-location; interference-aware profiler; latency predictor; adaptive scheduler</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>PKU</td>
<td>Memory Offloading for Large Language Model Inference with Latency SLO Guarantees</td>
<td>effectively captures the tension between meeting SLOs and maximizing host memory usage; dynamic offloading interval; per-bus coordinator</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Huawei</td>
<td>Hybrid Offline-online Scheduling Method for Large Language Model Inference Optimization</td>
<td>hybrid offline-online scheduling; preemptive scheduling for hardware utilization; lagrangian method for cost efficiency evaluation</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>ASPLOS</td>
<td>BUAA</td>
<td>Past-Future Scheduler for LLM Serving under SLA Guarantees</td>
<td>lightLLM; predict future system memory usage; reduce evict by better request scheduling</td>
<td>3</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="surveys">Surveys<a class="headerlink" href="#surveys" title="Permanent link">&para;</a></h4>
<h5 id="system-optimization-surveys">System Optimization Surveys<a class="headerlink" href="#system-optimization-surveys" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>NEU</td>
<td>LLM Inference Serving: Survey of Recent Advances and Opportunities</td>
<td>KV cache and memory management; LLM computation optimization; Cloud LLM deployment; focus on system-level enhancements</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>CUHK</td>
<td>A Survey on Inference Optimization Techniques for Mixture of Experts Models</td>
<td>model compression; expert skip; expert merge; sparse to dense; expert parallel; expert offloading</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>PolyU</td>
<td>A Survey on Large Language Model Acceleration based on KV Cache Management</td>
<td>cache selection; budget allocation; cache merging; cache quantization; cache low-rank decomposition; attention grouping and sharing; memory management; hardware-aware design</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>THU</td>
<td>Beyond A Single AI Cluster: A Survey of Decentralized LLM Training</td>
<td>resource-driven paradigm; community-driven decentralization; organizational decentralization; decentralized LLM training taxonomy</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>FIU</td>
<td>Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions</td>
<td>distributed solutions for LMs; workload imbalance in LLM training; M-ICL; model security enhancement</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h5 id="application-surveys">Application Surveys<a class="headerlink" href="#application-surveys" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>PKU</td>
<td>Retrieval-Augmented Generation for AI-Generated Content: A Survey</td>
<td>Query Transformation; Data Augmentation; Recursive Retrieval; Chunk Optimization; Retriever Finetuning; Hybrid Retrieval; Re-ranking; Retrieval Transformation; Prompt Engineering; Decoding Tuning; Generator Finetuning; Output Rewrite; Adaptive Retrieval; Iterative RAG</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>WHU</td>
<td>A survey on LLM-based multi-agent systems: workflow, infrastructure, and challenges</td>
<td>personalized characteristics; perceive environmental information; utilize memory mechanisms; mutual interaction; agent self-reflection</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>PolyU</td>
<td>Deploying Foundation Model Powered Agent Services: A Survey</td>
<td>FM-powered agent services within the edge-cloud environment; low-level hardware perspective; high-level software perspective</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="multimodal-systems">Multimodal Systems<a class="headerlink" href="#multimodal-systems" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UWMadison</td>
<td>LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models</td>
<td>query-block distributed exchange; shared visual token recomputation; sequence-parallelism with minimal communication overhead</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Microsoft</td>
<td>Towards Efficient Large Multimodal Model Serving</td>
<td>fine-grained stage-aware resource management; multimodal workload-specific scheduling; model architecture-specific optimizations</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Huawei</td>
<td>Efficiently Serving Large Multimedia Models Using EPD Disaggregation</td>
<td>encode-prefill-decode disaggregation; multimodal cache; intra-request parallel</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>TU/e</td>
<td>Fine-tuning Multimodal Transformers on Edge: A Parallel Split Learning Approach</td>
<td>Multimodal Parallel Split Learning; computation-efficient training; server-side loss aggregation mechanism</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>HUST</td>
<td>FastCache: Optimizing Multimodal LLM Serving through Lightweight KV-Cache Compression Framework</td>
<td>resource-aware KV-cache memory pool; multimodal KV-cache compression; modality-specific compression</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="mixture-of-experts-llm-systems">Mixture-of-Experts LLM Systems<a class="headerlink" href="#mixture-of-experts-llm-systems" title="Permanent link">&para;</a></h4>
<p>Challenge: efficiently coordinating and scaling expert models across multiple nodes, leading to issues like uneven workload distribution, high communication overhead, and difficulty in fault tolerance.</p>
<h5 id="expert-offloading-and-placement">Expert Offloading and Placement<a class="headerlink" href="#expert-offloading-and-placement" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>DATE</td>
<td>Berkeley</td>
<td>DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient MoE Inference</td>
<td>data-aware offloading; predictive pre-calculation; sequence-specific expert allocation</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Stevens Tech</td>
<td>fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts Serving</td>
<td>expert map; iteration-level probability distributions; track fine-grained input semantic embeddings; semantic-based and trajectorybased</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Georgia Tech</td>
<td>MoETuner: Optimized Mixture of Expert Serving with Balanced Expert Placement and Token Routing</td>
<td>ILP for expert placement; cross-layer dependencies; minimizing total dispatched token number</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>EuroMLSys</td>
<td>EPFL</td>
<td>Accelerating MoE Model Inference with Expert Sharding</td>
<td>expert sharding for load balancing; tensor sharding for moe experts; fused expert computations for reduced kernel launches</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>DAC</td>
<td>PKU</td>
<td>HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient MoE Inference</td>
<td>dynamically balances workloads across GPUs and CPUs; impact-driven prefetching; MoE-specialized cache management</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<h5 id="batching-and-scheduling">Batching and Scheduling<a class="headerlink" href="#batching-and-scheduling" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Alibaba</td>
<td>Static Batching of Irregular Workloads on GPUs: Framework and Application to Efficient MoE Model Inference</td>
<td>statically batching irregular workloads; batch-task-tile partition; decompress the mapping and dispatch the workload</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Edinburgh</td>
<td>MoE-Gen: High-Throughput MoE Inference on a Single GPU with Module-Based Batching</td>
<td>module-based batching; high-throughput MoE inference; full KV-cache offloading</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>KTH</td>
<td>Priority-Aware Preemptive Scheduling for Mixed-Priority Workloads in MoE Inference</td>
<td>fine-grained preemption; priority-aware scheduling; per-expert queues; expert-level preemption</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UMich</td>
<td>MoE-Lens: Towards the Hardware Limit of High-Throughput MoE LLM Serving Under Resource Constraints</td>
<td>two-stage performance modeling; analyzes the theoretical performance upper bound; captures how system execution mechanisms</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>Arxiv</td>
<td>Nvidia</td>
<td>MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core</td>
<td>decouples parallelization strategies for attention and MoE layers; flexible and efficient token-level dispatcher; 5-D hybrid parallelism</td>
<td>4</td>
<td>5</td>
<td>2</td>
</tr>
</tbody>
</table>
<h5 id="memory-and-communication-efficiency">Memory and Communication Efficiency<a class="headerlink" href="#memory-and-communication-efficiency" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>ByteDance</td>
<td>Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts</td>
<td>fine-grained communication-computation overlapping for efficient MoE execution; dependency resolving method; adaptive workload assignment method; shared data buffers between communication and computation operations</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UVA</td>
<td>eMoE: Task-aware Memory Efficient Mixture-of-Experts-Based (MoE) Model Inference</td>
<td>expert prediction; task-aware expert loading; task-aware request scheduling</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>mobiCom</td>
<td>HKUST</td>
<td>D$^{2}$MoE: Dual Routing and Dynamic Scheduling for Efficient On-Device MoE-based LLM Serving</td>
<td>dually sparselygated Mixture-of-Experts; token-adaptive bit-width selection; matryoshka weight quantization; bit-width-aware I/O-compute pipeline</td>
<td>3</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>ODSI</td>
<td>SJTU</td>
<td>Fast and Live Model Auto Scaling with O(1) Host Caching</td>
<td>auto-scaling with minimal caching; optimize parameter loading; enabling fine-grained layer-level scale</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2023</td>
<td>ASPLOS</td>
<td>Google</td>
<td>TelaMalloc: Efficient On-Chip Memory Allocation for Production Machine Learning Accelerators</td>
<td>hybrid heuristic-solver memory allocator for ML accelerators; contention-aware phased allocation strategy</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>MICRO</td>
<td>Duke</td>
<td>LLM.265: Video Codecs are Secretly Tensor Codecs</td>
<td>use video compress hardware on GPU to compress LLM tensor; existing idle hardware video encoders and decoders (NVENC/NVDEC) found on modern GPUs</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
</tbody>
</table>
<h5 id="architectural-innovations">Architectural Innovations<a class="headerlink" href="#architectural-innovations" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Shanghai AI</td>
<td>Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts</td>
<td>linear sequence modeling with MoE; sparse activation via moe layers; hybrid models combining linear-moe and transformer-moe layers</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Berkeley</td>
<td>HeterMoE: Efficient Training of Mixture-of-Experts Models on Heterogeneous GPUs</td>
<td>zebra parallelism; attention-expert disaggregation; asymmetric expert assignment mechanism; gather and squeeze strategy</td>
<td>4</td>
<td>5</td>
<td>3</td>
</tr>
</tbody>
</table>
<h5 id="compute-kernel-level-optimizations">Compute-Kernel-Level Optimizations<a class="headerlink" href="#compute-kernel-level-optimizations" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>SJTU</td>
<td>Samoyeds: Accelerating MoE Models with Structured Sparsity Leveraging Sparse Tensor Cores</td>
<td>dual-side structured sparsity; sparse-sparse matrix multiplication kernel; vector-wise + 2:4 hybrid sparsity; token-aware activation compression</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>Mips</td>
<td>Cornell</td>
<td>FlashMoE: Fast Distributed MoE in a Single Kernel</td>
<td>mega kernel for MoE model; persistent kernel to reduce CPU overhead</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="long-sequence-llm-systems">Long Sequence LLM Systems<a class="headerlink" href="#long-sequence-llm-systems" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>OSDI</td>
<td>SJTU &amp; Alibaba</td>
<td>Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache</td>
<td>inefficient model parallelism intra-instance; inefficient resource management inter-instance; KV cache scheduling</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>PKU</td>
<td>ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length on More Than 12,000 GPUs</td>
<td>hybrid data parallelism; data-aware sharding; a heuristic algorithm that reorganizes data assignment based on the characteristics of data and pipeline parallelism</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>ICML</td>
<td>ByteDance</td>
<td>ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</td>
<td>offload value cache to CPU and keep outliers on GPU; landmark-guided sparse KV selection per chunk</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h5 id="sparse-attention">Sparse Attention<a class="headerlink" href="#sparse-attention" title="Permanent link">&para;</a></h5>
<p>Solution: handle the prompt token by token introduce high latency, trying to use sparse attention to reduce the computation and memory burden. This can be achieved by not using the full attention matrix, but only the upper triangular part.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>CWRU</td>
<td>Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques</td>
<td>sparse attention with graph computing perspective; work-optimal graph algorithms; achieve true sparsity</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>MLSys</td>
<td>MIT</td>
<td>LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention</td>
<td>unified sparse attention; hybrid static and dynamic sparsity; hierarchical kv cache management with query-centric pruning</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h5 id="ring-computation">Ring Computation<a class="headerlink" href="#ring-computation" title="Permanent link">&para;</a></h5>
<p>Solution: use the device layout to reduce the communication overhead. The key idea is to parallel the computation and communication.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>Nips</td>
<td>UCB</td>
<td>Ring Attention with Blockwise Transformers for Near-Infinite Context</td>
<td>divide the input into blocks and each block is processed by a single GPU; ring-type device layout</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>SJTU</td>
<td>TokenRing: An Efficient Parallelism Framework for Infinite-Context LLMs via Bidirectional Communication</td>
<td>communication-oriented parallelism framework; inter-node P2P bidirectional communication bandwidth; optimization of attention block communication</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="p-d-disaggregated-systems">P-D Disaggregated Systems<a class="headerlink" href="#p-d-disaggregated-systems" title="Permanent link">&para;</a></h4>
<p>Solution: prefill and decode have different computation characteristics, so we can disaggregate them to different GPUs.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>OSDI</td>
<td>PKU</td>
<td>DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving</td>
<td>goodput-optimized; prefill-decoding interference;novel placement algorithm for p-d schema</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>ISCA</td>
<td>UW</td>
<td>Splitwise: Efficient Generative LLM Inference Using Phase Splitting</td>
<td>optimized cache context transfer; performance per dollar; performance per watt; exploration of homogeneous and heterogeneous cluster deployments</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>CMU</td>
<td>A System for Microserving of LLMs</td>
<td>fine-grained sub-request level actions; dynamic reconfiguration according to workloads; unified KV cache abstraction</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>PKU</td>
<td>ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud Environments</td>
<td>two-level hierarchical optimization; tabu search algorithm for GPU partition; a lightweight re-scheduling mechanism</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h5 id="p-d-disaggregated-system-optimizations">P-D Disaggregated System Optimizations<a class="headerlink" href="#p-d-disaggregated-system-optimizations" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>ByteDance</td>
<td>KVDirect: Distributed Disaggregated LLM Inference</td>
<td>tensor-centric communication mechanism; pull-based KV cache transfer; dynamic GPU resource scheduling via RDMA</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>SYSU</td>
<td>Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation</td>
<td>attention disaggregation and offloading mechanism; low-latency decoding synchronization; resource-efficient prefill colocation; load-aware offloading scheduling</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Alibaba</td>
<td>FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache Transfer and Load-Aware Scheduling</td>
<td>analyze the communication patterns; KV cache structure adjustment method; load-aware scheduling</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>NUS &amp; USTC</td>
<td>DynaServe: Unified and Elastic Tandem-Style Execution for Dynamic Disaggregated LLM Serving</td>
<td>a novel Tandem Serving execution model; two virtual subrequests; explicitly permit the two subrequests to execute on either GPU instance</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<h5 id="a-f-disaggregated-systems">A-F Disaggregated Systems<a class="headerlink" href="#a-f-disaggregated-systems" title="Permanent link">&para;</a></h5>
<p>Challenge: In the decode stage, <strong>the attention need memory access while the FFN only need compute</strong>. The increasingly sparse FFN in MoE models make the GPU computation ultilization low. Disaggregate the attention and FFN module to different can make better batching and heterogeneous devices implement.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>ByteDance</td>
<td>MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism</td>
<td>the first the disaggregated attention and FFN disaggregate; ping-pong pipeline parallel; high performance M2N communication</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>StepFun</td>
<td>Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding</td>
<td>detailed AFD math analysis; co-design of model and system; low decode latenty implementation</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>SJTU</td>
<td>Expert-as-a-Service: Towards Efficient Scalable and Robust Large-scale MoE Serving</td>
<td>treat expert on independent servers; expert as a service enable scalable MoE serving</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="throughput-optimized-systems">Throughput-Optimized Systems<a class="headerlink" href="#throughput-optimized-systems" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>HKUST</td>
<td>Improving the End-to-End Efficiency of Offline Inference for Multi-LLM Applications Based on Sampling and Simulation</td>
<td>sampling-then-simulation cost model; model-level pipeline parallelism; minimumtotal-latency application scheduling</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>EuroSys</td>
<td>SJTU</td>
<td>TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via Preemptive Scheduling</td>
<td>brust scene target; tradeoff between TTFT and throughout; slowdown some TOT to human's read speed</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="mtp-llm-systems">MTP LLM Systems<a class="headerlink" href="#mtp-llm-systems" title="Permanent link">&para;</a></h4>
<p>Challenge: Traditional autoregressive LLM is slow, MTP, multi-token parallel, can speed up the inference. The challenge include how to tradeoff between throughput and accuracy.</p>
<h5 id="speculative-decodeing">Speculative Decodeing<a class="headerlink" href="#speculative-decodeing" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>PMLR</td>
<td>Google</td>
<td>Fast Inference from Transformers via Speculative Decoding</td>
<td>full-accuracy multi token generation acceleration; use little draft model to generate spec sequence and origin mdoel to evulate in parallel</td>
<td>3</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2024</td>
<td>ICML</td>
<td>PKU</td>
<td>EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty</td>
<td>use hidden state instead of token to spec; static draft tree based spec chain</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2024</td>
<td>EMNLP</td>
<td>PKU</td>
<td>EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees</td>
<td>use dynamic draft tree to reduce search place</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>ACL</td>
<td>PKU</td>
<td>EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test</td>
<td>solve the problem of scaling draft model to better predict; more training data to get high accuracy</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UCB</td>
<td>Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding</td>
<td>use sparse attention as draft model; top-k based sparse attention; reasoning model inference</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="diffusion-llm-systems">Diffusion LLM Systems<a class="headerlink" href="#diffusion-llm-systems" title="Permanent link">&para;</a></h4>
<p>Challenge: traditional LLM use an auto-regressive model to generate the text, which is slow. Diffusion LLM have a parallel generation ability, which is fast.</p>
<h4 id="kv-cache-in-dllm-systems">KV Cache in dLLM Systems<a class="headerlink" href="#kv-cache-in-dllm-systems" title="Permanent link">&para;</a></h4>
<p>Challenge: The initial implementation of LLaDA lacks KV cache support, which is the primary reason for its slower inference compared to AR models.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>SJTU</td>
<td>dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching</td>
<td>training-free adaptive caching mechanism; V-verify mechanism for caching</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>NVIDIA</td>
<td>Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding</td>
<td>block-wise approximate KV Cache for diffusion LLMs; confidence-aware parallel decoding strategy</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Fudan</td>
<td>Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction</td>
<td>dynamic bidirectional cache eviction; delayed bidirectional sparse caching; attention-guided token pruning</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="parallel-decoding-in-dllm-systems">Parallel Decoding in dLLM Systems<a class="headerlink" href="#parallel-decoding-in-dllm-systems" title="Permanent link">&para;</a></h4>
<p>Challenge: Denoising involves multiple stages, often requiring hundreds of iterations to achieve acceptable text quality, which significantly slows down inference.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>NeurIPS</td>
<td>Univ. of Virginia</td>
<td>Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion</td>
<td>speculative diffusion Decoding; discrete diffusion models as non-autoregressive drafters; parallelized draft sequence generation</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Cornell</td>
<td>Remasking Discrete Diffusion Models with Inference-Time Scaling</td>
<td>ReMDM sampler for iterative refinement; inference-time compute scaling via token remasking; custom remasking backward process for pre-trained models</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>SJTU</td>
<td>Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing</td>
<td>First dLLM faster than AR LLM; fuse KV-cached autoregression with parallel diffusion decoding; inter-block parallelism without full denoise</td>
<td>4</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>PKU</td>
<td>Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models</td>
<td>adaptive length prediction for prefill optimization; jump-share speculative decoding strategy</td>
<td>4</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="visual-auto-regressive-models-serving-system">Visual Auto-Regressive Models Serving System<a class="headerlink" href="#visual-auto-regressive-models-serving-system" title="Permanent link">&para;</a></h4>
<p>Challenge: as the VAR model moves to large scales (e.g. 1024x1024), the number of tokens to be processed explodes, making final steps expensive compared to the initial coarse steps.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>ICCV</td>
<td>THU</td>
<td>FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning</td>
<td>token pruning cache for resolution scaling; frequency-based pivotal token selection; cached token restoration from previous scales</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UESTC</td>
<td>SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping</td>
<td>frequency-aware adaptive acceleration; step-skipping for step redundancy; unconditional branch replacement for branch redundancy</td>
<td>3</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="fair-serving-systems">Fair Serving Systems<a class="headerlink" href="#fair-serving-systems" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Virginia Tech</td>
<td>Ensuring Fair LLM Serving Amid Diverse Applications</td>
<td>multi-tenant LLM platform; overload and interaction-driven throttling; weighted service counter</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UIUC</td>
<td>Hierarchical Autoscaling for Large Language Model Serving with Chiron</td>
<td>hierarchical backpressure; interactive requests and batch requests; mixed instances</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Berkeley</td>
<td>Locality-aware Fair Scheduling in LLM Serving</td>
<td>deficit-based longest prefix matching; distributed deficit-round coordination; prefix-aware fairness bound analysis</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="agent-colocating-serving">Agent Colocating Serving<a class="headerlink" href="#agent-colocating-serving" title="Permanent link">&para;</a></h4>
<p>Challenge: The agent here refers to a module can response to specific tasks. In this scene, systems are required to provide service for both models and the system</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2019</td>
<td>HPDC</td>
<td>NKU</td>
<td>GAugur: Quantifying Performance Interference of Colocated Games for Improving Resource Utilization in Cloud Gaming</td>
<td>colocating games on cloud server; game resource allocation</td>
<td>2</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Google</td>
<td>Scaling Instructable Agents Across Many Simulated Worlds</td>
<td>use one GPU for each cloud game; action prediction for gaming agent; TPU acceleration</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>ByteDance</td>
<td>Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds</td>
<td>gaming LLM with VLA model and graphic agent; inference optimizaion with spec and streaming response; reply for mouse click</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>GWU</td>
<td>Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments</td>
<td>colocating multi agent LLM; dynamic adjusting GPU resource</td>
<td>2</td>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="rl-system">RL System<a class="headerlink" href="#rl-system" title="Permanent link">&para;</a></h3>
<p>Challenge: RL contains both backward and frontward. This need to handle the interaction between multi step and task, posing challenge to parallelism and synchronization.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2018</td>
<td>OSDI</td>
<td>UCB</td>
<td>Ray: A Distributed Framework for Emerging AI Applications</td>
<td>unified actor and task model; stateless architecture via global control store; bottom-up distributed scheduling</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>NTU</td>
<td>ReSpec: Towards Optimizing Speculative Decoding in Reinforcement Learning Systems</td>
<td>spec in RL LLM acceleration; dynamic batch size adjustment in runtime; lets the drafter chase high-reward modes only</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="rlhf-system">RLHF System<a class="headerlink" href="#rlhf-system" title="Permanent link">&para;</a></h4>
<p>Challenge: RL system includes both training and inference. On top of that, multi agents(LLMs) when running in parallel, which makes the data flow more complex.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>EuroSys</td>
<td>HKU</td>
<td>HybridFlow: A Flexible and Efficient RLHF Framework</td>
<td>auto-mapping model placement; 3D-HybridEngine to reduce the communication overhead; hybrid programming</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Alibaba</td>
<td>Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library</td>
<td>bind many LLMs in one device cluster; fix the batch problem of long tail requests; reuse many utils in HybridFlow</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>FDU</td>
<td>DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training</td>
<td>distributed controller for better scalability; fully decoupled DAG-defined RL pipeline; linear scalability</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="agentic-rl-system">Agentic RL System<a class="headerlink" href="#agentic-rl-system" title="Permanent link">&para;</a></h4>
<p>Challenge: multi-agent systems require efficient communication and coordination among agents, which can lead to increased complexity and overhead in system design and implementation.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>HKUST</td>
<td>RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure</td>
<td>disaggregated infrastructure for agentic RL; workload-aware GPU mapping; satefulness aware computation</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="communication-computation-overlap">Communication-Computation Overlap<a class="headerlink" href="#communication-computation-overlap" title="Permanent link">&para;</a></h3>
<p>Challenge: effectively hiding communication latency by overlapping it with computation, which requires careful scheduling and resource management to avoid bottlenecks and ensure that both communication and computation proceed efficiently without stalling each other.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>NSDI</td>
<td>KAIST</td>
<td>ARK: GPU-driven Code Execution for Distributed Deep Learning</td>
<td>communication-motivated DL system; pipeline DMA engine; GPU-direct-controlled DMA</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>ASPLOS</td>
<td>PKU</td>
<td>Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning</td>
<td>communication partition abstraction; hybrid LLM training tasks; 3-level decompose</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>ASPLOS</td>
<td>UWMadison</td>
<td>T3: Transparent Tracking &amp; Triggering for Fine-grained Overlap of Compute &amp; Collectives</td>
<td>lightweight track and trigger; pre-programmed DMA commands; atomic memory update</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>ASPLOS</td>
<td>UIUC</td>
<td>Two-Face: Combining Collective and One-Sided Communication for Efficient Distributed SpMM</td>
<td>distributed SpMM; sparsity-aware partition; Synchronous Stripes and Asynchronous Stripes</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>AMD</td>
<td>Optimizing ML Concurrent Computation and Communication with GPU DMA Engines</td>
<td>concurrent computation and communication; compute and memory interference among concurrent kernels; schedule prioritization and careful resource partitioning</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="configuration-optimization">Configuration Optimization<a class="headerlink" href="#configuration-optimization" title="Permanent link">&para;</a></h3>
<p>Challenge: the configuration space is too large to be searched manually.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2019</td>
<td>SIGGRAPH</td>
<td>Facebook</td>
<td>Learning to Optimize Halide with Tree Search and Random Programs</td>
<td>learning-based auto-scheduling; cost model training on random programs; beam search over schedule space</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2020</td>
<td>ASPLOS</td>
<td>PKU</td>
<td>FlexTensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous System</td>
<td>tvm auto schedule; RL based stragety find; auto optimizing in large configuration space</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>OSDI</td>
<td>PKU</td>
<td>Mirage: A Multi-Level Superoptimizer for Tensor Programs</td>
<td>auto algebraically transfer tensor; using DAG to search configuration space; auto generate kernel function</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="diffusion-model-serving-systems">Diffusion Model Serving Systems<a class="headerlink" href="#diffusion-model-serving-systems" title="Permanent link">&para;</a></h3>
<p>Challenge: serving diffusion models need sequential iterative process, which creates prohibitive latency for generating high-resolution images in real-time and is inherently difficult to parallelize.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>CVPR</td>
<td>MIT</td>
<td>DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models</td>
<td>displaced patch parallelism; asynchronous GroupNorm for distributed normalization</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Tencent</td>
<td>PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference</td>
<td>patch-level pipeline parallelism; reusing one-step stale feature maps to exploit temporal redundancy; simultaneous image patch and model layer partitioning</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UMASS</td>
<td>HADIS: Hybrid Adaptive Diffusion Model Serving for Efficient Text-to-Image Generation</td>
<td>offline analysis based router choose; LIP based resource allocation</td>
<td>2</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Tencent</td>
<td>xDiT: an Inference Engine for Diffusion Transformers (DiTs) with Massive Parallelism</td>
<td>hybrid of sequence parallelism &amp; PipeFusion &amp; classifier-free guidance parallelism; patch parallelism for the VAE decoder to prevent out-of-memory errors</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="multi-agent-systems">Multi Agent Systems<a class="headerlink" href="#multi-agent-systems" title="Permanent link">&para;</a></h3>
<p>Challenge: multi-agent systems require efficient communication and coordination among agents, which can lead to increased complexity and overhead in system design and implementation.</p>
<h4 id="dsl-for-agent-representation">DSL for agent representation<a class="headerlink" href="#dsl-for-agent-representation" title="Permanent link">&para;</a></h4>
<p>Challenge: designing a domain-specific language (DSL) for representing agents can help to simplify the development and deployment of multi-agent systems.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UTA</td>
<td>Software-Defined Agentic Serving</td>
<td>use LLM to generate the cpp code of agent network; dynamic agent graph support</td>
<td>2</td>
<td>2</td>
<td>1</td>
</tr>
</tbody>
</table>
<h4 id="cpu-based-optimizations-for-large-scale-multi-agent-systems">CPU based Optimizations for Large Scale Multi-Agent Systems<a class="headerlink" href="#cpu-based-optimizations-for-large-scale-multi-agent-systems" title="Permanent link">&para;</a></h4>
<p>Challenge: Large number will lead to high communication overhead on CPU based systems. The database, scheduler and communication need to be optimized.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>RUC</td>
<td>YuLan-OneSim: Towards the Next Generation of Social Simulator with Large Language Models</td>
<td>vllm based LLM running; CPU overhead optimization for communication between agents; agent num is about 100000</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>AiLab</td>
<td>OASIS: Open Agent Social Interaction Simulations with One Million Agents</td>
<td>agent simulation with large scale; run LLM on local; optimization for large scale agent simulation; agent num is about 1 million</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>ZJU</td>
<td>Agent-Kernel: A MicroKernel Multi-Agent System Framework for Adaptive Social Simulation Powered by LLMs</td>
<td>micro-kernel MAS; decouple system function to enable more flexible and adaptive computation</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="gpu-based-optimizations-for-multi-agent-systems">GPU based Optimizations for Multi-Agent Systems<a class="headerlink" href="#gpu-based-optimizations-for-multi-agent-systems" title="Permanent link">&para;</a></h4>
<p>Challenge: Using GPU to run multi-agent systems can reduce the communication overhead. However, the GPU memory is limited, which requires optimization for memory management and communication.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>MIT</td>
<td>On the limits of agency in agent-based models</td>
<td>GPU optimization for agent simulation; use tensor for agent status expression; optimization for on-GPU operation</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>THU</td>
<td>AgentSociety: Large-Scale Simulation of LLM-Driven Generative Agents Advances Understanding of Human Behaviors and Society</td>
<td>large simulation for limited TCP connection port; use ray for distributed execution; change information subscribe method</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="reuse-optimizations-for-multi-agent-systems">Reuse Optimizations for Multi-Agent Systems<a class="headerlink" href="#reuse-optimizations-for-multi-agent-systems" title="Permanent link">&para;</a></h4>
<p>Challenge: in multi-agent systems, many agents may share similar characteristics or behaviors. By identifying and reusing these commonalities, we can reduce redundant computations and improve overall system efficiency.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>TMLR</td>
<td>Tencent</td>
<td>Affordable Generative Agents</td>
<td>based on Generative Aggets; system optimization for LLM behavior reuse; optimization for token consumption</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>individual</td>
<td>Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware</td>
<td>KVC injection for agent memory reuse; CUDA stream</td>
<td>1</td>
<td>2</td>
<td>1</td>
</tr>
</tbody>
</table>
<h4 id="environment-optimizations-for-multi-agent-systems">Environment Optimizations for Multi-Agent Systems<a class="headerlink" href="#environment-optimizations-for-multi-agent-systems" title="Permanent link">&para;</a></h4>
<p>Challenge: The training and inference of multi-agent systems often require complex environments, which can lead to high computational and memory overhead.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2026</td>
<td>arXiv</td>
<td>Alibaba</td>
<td>MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era</td>
<td>abstracts agent training infrastructure into three independent services; CPU based optimization; container management</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="dsl-expression-for-agents">DSL Expression for Agents<a class="headerlink" href="#dsl-expression-for-agents" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2026</td>
<td>arXiv</td>
<td>UT-Austin</td>
<td>Nalar: An agent serving framework</td>
<td>frontend agent expression for dynamic graph; agent based scheduling</td>
<td>3</td>
<td>2</td>
<td>1</td>
</tr>
</tbody>
</table>
<h3 id="gpu-kernel-optimizations-for-systems">GPU Kernel Optimizations for Systems<a class="headerlink" href="#gpu-kernel-optimizations-for-systems" title="Permanent link">&para;</a></h3>
<p>Challenge: GPU kernel optimizations can significantly improve the performance of systems by reducing computation time and memory usage.</p>
<h4 id="tiling-optimizations">Tiling Optimizations<a class="headerlink" href="#tiling-optimizations" title="Permanent link">&para;</a></h4>
<p>Solution: Tiling is a common optimization technique that can improve data locality and reduce memory access overhead.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022</td>
<td>NIPS</td>
<td>Stanford</td>
<td>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</td>
<td>Generalized Acceleration of Attention Mechanisms; Change attention to utilize the SRAM on GPU; use recompute to reduce IO burden</td>
<td>4</td>
<td>5</td>
<td>4</td>
</tr>
<tr>
<td>2023</td>
<td>ICLR</td>
<td>Stanford</td>
<td>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</td>
<td>optimize the thread block parallelization of attention; parallel memory access; reduce no-malmul operation</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>Nips</td>
<td>Stanford</td>
<td>FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision</td>
<td>Hopper architecture based optimization; fp8 quantization; backward support</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="gpu-level-kernel-svcheduling-optimizations">GPU level kernel svcheduling optimizations<a class="headerlink" href="#gpu-level-kernel-svcheduling-optimizations" title="Permanent link">&para;</a></h4>
<p>Solution: GPU kernel scheduling optimizations can improve the performance of systems by reducing kernel launch overhead</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>PKU</td>
<td>Hummingbird: SLO-Oriented GPU Preemption at Microsecond-scale</td>
<td>SLO based kernel preemption; kernel split for scheduling</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="January 21, 2026 12:55:22 UTC">January 21, 2026</span>
  </span>

    
    
    
    
  </aside>


  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.instant", "navigation.instant.progress", "navigation.tracking", "navigation.sections", "navigation.expand", "navigation.indexes", "toc.follow", "navigation.top", "content.action.edit", "content.action.view", "search.highlight", "search.share", "search.suggest"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
    
  </body>
</html>