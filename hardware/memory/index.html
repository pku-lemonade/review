
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Youwei Zhuo">
      
      
        <link rel="canonical" href="https://review.youwei.xyz/hardware/memory/">
      
      
        <link rel="prev" href="../parallel/">
      
      
        <link rel="next" href="../accelerators/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.22">
    
    
      
        <title>Memory Architecture - New Lemonade Review</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.84d31ad4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-JXHKVQ8LKS"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-JXHKVQ8LKS",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-JXHKVQ8LKS",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#memory-architecture" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="New Lemonade Review" class="md-header__button md-logo" aria-label="New Lemonade Review" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M368 80c-3.2 0-6.2.4-8.9 1.3C340 86.8 313 92 284.8 84.6c-57.4-14.9-124.6 7.4-174.7 57.5S37.7 259.4 52.6 316.8c7.3 28.2 2.2 55.2-3.3 74.3-.8 2.8-1.3 5.8-1.3 8.9 0 17.7 14.3 32 32 32 3.2 0 6.2-.4 8.9-1.3 19.1-5.5 46.1-10.7 74.3-3.3 57.4 14.9 124.6-7.4 174.7-57.5s72.4-117.3 57.5-174.7c-7.3-28.2-2.2-55.2 3.3-74.3.8-2.8 1.3-5.8 1.3-8.9 0-17.7-14.3-32-32-32m0-48c44.2 0 80 35.8 80 80 0 7.7-1.1 15.2-3.1 22.3-4.6 15.8-7.1 32.9-3 48.9 20.1 77.6-10.9 161.5-70 220.7s-143.1 90.2-220.7 70c-16-4.1-33-1.6-48.9 3-7.1 2-14.6 3.1-22.3 3.1-44.2 0-80-35.8-80-80 0-7.7 1.1-15.2 3.1-22.3 4.6-15.8 7.1-32.9 3-48.9-20.1-77.6 10.9-161.5 70-220.7s143.2-90.1 220.7-70c16 4.1 33 1.6 48.9-3 7.1-2 14.6-3.1 22.3-3.1M246.7 167c-52 15.2-96.5 59.7-111.7 111.7-3.7 12.7-17.1 20-29.8 16.3s-20-17-16.2-29.7c19.8-67.7 76.6-124.5 144.3-144.3 12.7-3.7 26.1 3.6 29.8 16.3s-3.6 26.1-16.3 29.8z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            New Lemonade Review
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Memory Architecture
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/pku-lemonade/review" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pku-lemonade/review
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="New Lemonade Review" class="md-nav__button md-logo" aria-label="New Lemonade Review" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M368 80c-3.2 0-6.2.4-8.9 1.3C340 86.8 313 92 284.8 84.6c-57.4-14.9-124.6 7.4-174.7 57.5S37.7 259.4 52.6 316.8c7.3 28.2 2.2 55.2-3.3 74.3-.8 2.8-1.3 5.8-1.3 8.9 0 17.7 14.3 32 32 32 3.2 0 6.2-.4 8.9-1.3 19.1-5.5 46.1-10.7 74.3-3.3 57.4 14.9 124.6-7.4 174.7-57.5s72.4-117.3 57.5-174.7c-7.3-28.2-2.2-55.2 3.3-74.3.8-2.8 1.3-5.8 1.3-8.9 0-17.7-14.3-32-32-32m0-48c44.2 0 80 35.8 80 80 0 7.7-1.1 15.2-3.1 22.3-4.6 15.8-7.1 32.9-3 48.9 20.1 77.6-10.9 161.5-70 220.7s-143.1 90.2-220.7 70c-16-4.1-33-1.6-48.9 3-7.1 2-14.6 3.1-22.3 3.1-44.2 0-80-35.8-80-80 0-7.7 1.1-15.2 3.1-22.3 4.6-15.8 7.1-32.9 3-48.9-20.1-77.6 10.9-161.5 70-220.7s143.2-90.1 220.7-70c16 4.1 33 1.6 48.9-3 7.1-2 14.6-3.1 22.3-3.1M246.7 167c-52 15.2-96.5 59.7-111.7 111.7-3.7 12.7-17.1 20-29.8 16.3s-20-17-16.2-29.7c19.8-67.7 76.6-124.5 144.3-144.3 12.7-3.7 26.1 3.6 29.8 16.3s-3.6 26.1-16.3 29.8z"/></svg>

    </a>
    New Lemonade Review
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/pku-lemonade/review" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pku-lemonade/review
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    README
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Hardware
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Hardware
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../processor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Processor Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../parallel/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Parallel and Multi-Processor Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Memory Architecture
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Memory Architecture
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#ndp-flash" class="md-nav__link">
    <span class="md-ellipsis">
      NDP: Flash
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NDP: Flash">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-application-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      General Application-Specific Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="General Application-Specific Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llm-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      LLM-Specific Optimization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ndp-dimm" class="md-nav__link">
    <span class="md-ellipsis">
      NDP: DIMM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NDP: DIMM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-application-specific-optimization_1" class="md-nav__link">
    <span class="md-ellipsis">
      General Application-Specific Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dnn-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      DNN-Specific Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-specific-optimization_1" class="md-nav__link">
    <span class="md-ellipsis">
      LLM-Specific Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-address-space" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Address Space
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-allocation-management" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Allocation &amp; Management
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pim-compiler-isa-extension" class="md-nav__link">
    <span class="md-ellipsis">
      PIM Compiler &amp; ISA Extension
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluation-simulators" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluation &amp; Simulators
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intra-dimm-communication" class="md-nav__link">
    <span class="md-ellipsis">
      Intra-DIMM Communication
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inter-dimm-communication" class="md-nav__link">
    <span class="md-ellipsis">
      Inter-DIMM Communication
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#concurrent-host-and-pim-operations" class="md-nav__link">
    <span class="md-ellipsis">
      Concurrent Host and PIM operations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizations-on-upmem-pim" class="md-nav__link">
    <span class="md-ellipsis">
      Optimizations on UPMEM-PIM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pim-in-cache-computing" class="md-nav__link">
    <span class="md-ellipsis">
      PIM: In-Cache-Computing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pim-ndp-benchmarks" class="md-nav__link">
    <span class="md-ellipsis">
      PIM &amp; NDP: Benchmarks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PIM &amp; NDP: Benchmarks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#benchmarks-for-conventional-computing" class="md-nav__link">
    <span class="md-ellipsis">
      Benchmarks for Conventional Computing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#benchmarks-for-quantum-computing" class="md-nav__link">
    <span class="md-ellipsis">
      Benchmarks for Quantum Computing
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ndp-cxl" class="md-nav__link">
    <span class="md-ellipsis">
      NDP: CXL
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ndp-3d-stacked-dram" class="md-nav__link">
    <span class="md-ellipsis">
      NDP: 3D-stacked DRAM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ndp-hbm" class="md-nav__link">
    <span class="md-ellipsis">
      NDP: HBM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NDP: HBM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#benchmark" class="md-nav__link">
    <span class="md-ellipsis">
      Benchmark
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ndp-heterogeneous-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      NDP: Heterogeneous Architecture
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#general-cim" class="md-nav__link">
    <span class="md-ellipsis">
      General CiM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="General CiM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#specific-application-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Specific Application &amp; Algorithm
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modeling-simulation" class="md-nav__link">
    <span class="md-ellipsis">
      Modeling &amp; Simulation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-dram" class="md-nav__link">
    <span class="md-ellipsis">
      CIM: DRAM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-sram" class="md-nav__link">
    <span class="md-ellipsis">
      CIM: SRAM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CIM: SRAM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sram-cim-general-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      SRAM CIM: General Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-specific-use-or-application" class="md-nav__link">
    <span class="md-ellipsis">
      SRAM CIM: Specific Use or Application
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-hardware-software-co-design" class="md-nav__link">
    <span class="md-ellipsis">
      SRAM CIM: Hardware-Software Co-Design
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-simulator-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      SRAM CIM: Simulator &amp; Modeling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-transformer-accelerator" class="md-nav__link">
    <span class="md-ellipsis">
      SRAM CIM: Transformer Accelerator
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-rram" class="md-nav__link">
    <span class="md-ellipsis">
      CIM: RRAM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CIM: RRAM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rram-cim-simulator" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Simulator
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-architecture-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Architecture optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-design-space-exploration" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Design Space Exploration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Modeling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-training-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Training optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-compiler" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Compiler
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-float-point-processing" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Float-Point processing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-convolutional-layer" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Convolutional Layer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="RRAM CiM: Convolutional Layer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rram-cim-mapping-for-cnn" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Mapping for CNN
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-transformer-accelerator" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CIM: Transformer Accelerator
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-special-usage" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Special Usage
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-matrix-equation-solver" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Matrix Equation Solver
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-hybrid-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      CIM: Hybrid Architecture
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CIM: Hybrid Architecture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hybrid-cim-sram-general-logic" class="md-nav__link">
    <span class="md-ellipsis">
      Hybrid CIM: SRAM + General Logic
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hybrid-cim-sram-rram" class="md-nav__link">
    <span class="md-ellipsis">
      Hybrid CIM: SRAM + RRAM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hybrid-cim-memristormram-sram" class="md-nav__link">
    <span class="md-ellipsis">
      Hybrid CIM: Memristor/MRAM + SRAM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hybrid-cim-analog-digital" class="md-nav__link">
    <span class="md-ellipsis">
      Hybrid CIM: Analog + Digital
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      CIM: Quantization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CIM: Quantization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cim-quantization-for-analog-cim" class="md-nav__link">
    <span class="md-ellipsis">
      CIM Quantization: For Analog CIM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-quantization-for-all-cim" class="md-nav__link">
    <span class="md-ellipsis">
      CIM Quantization: For all CIM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-digital-cim" class="md-nav__link">
    <span class="md-ellipsis">
      CIM: Digital CIM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nvm" class="md-nav__link">
    <span class="md-ellipsis">
      NVM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../accelerators/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Domain-Specific Accelerators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../network/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Interconnection Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../eda/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Electronic Design Automation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../fail/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Security and Reliability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../emerging/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Emerging Technologies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../perf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Performance Analysis (to be deleted)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../software/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Software
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Software
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/algorithm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Algorithms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/pl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Programming Languages
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/os/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Operating Systems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/ds/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributed Systems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/hpc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    High-Performance Computing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/perf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Performance Analysis
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#ndp-flash" class="md-nav__link">
    <span class="md-ellipsis">
      NDP: Flash
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NDP: Flash">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-application-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      General Application-Specific Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="General Application-Specific Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llm-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      LLM-Specific Optimization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ndp-dimm" class="md-nav__link">
    <span class="md-ellipsis">
      NDP: DIMM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NDP: DIMM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-application-specific-optimization_1" class="md-nav__link">
    <span class="md-ellipsis">
      General Application-Specific Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dnn-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      DNN-Specific Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-specific-optimization_1" class="md-nav__link">
    <span class="md-ellipsis">
      LLM-Specific Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-address-space" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Address Space
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-allocation-management" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Allocation &amp; Management
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pim-compiler-isa-extension" class="md-nav__link">
    <span class="md-ellipsis">
      PIM Compiler &amp; ISA Extension
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluation-simulators" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluation &amp; Simulators
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intra-dimm-communication" class="md-nav__link">
    <span class="md-ellipsis">
      Intra-DIMM Communication
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inter-dimm-communication" class="md-nav__link">
    <span class="md-ellipsis">
      Inter-DIMM Communication
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#concurrent-host-and-pim-operations" class="md-nav__link">
    <span class="md-ellipsis">
      Concurrent Host and PIM operations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizations-on-upmem-pim" class="md-nav__link">
    <span class="md-ellipsis">
      Optimizations on UPMEM-PIM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pim-in-cache-computing" class="md-nav__link">
    <span class="md-ellipsis">
      PIM: In-Cache-Computing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pim-ndp-benchmarks" class="md-nav__link">
    <span class="md-ellipsis">
      PIM &amp; NDP: Benchmarks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PIM &amp; NDP: Benchmarks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#benchmarks-for-conventional-computing" class="md-nav__link">
    <span class="md-ellipsis">
      Benchmarks for Conventional Computing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#benchmarks-for-quantum-computing" class="md-nav__link">
    <span class="md-ellipsis">
      Benchmarks for Quantum Computing
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ndp-cxl" class="md-nav__link">
    <span class="md-ellipsis">
      NDP: CXL
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ndp-3d-stacked-dram" class="md-nav__link">
    <span class="md-ellipsis">
      NDP: 3D-stacked DRAM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ndp-hbm" class="md-nav__link">
    <span class="md-ellipsis">
      NDP: HBM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NDP: HBM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#benchmark" class="md-nav__link">
    <span class="md-ellipsis">
      Benchmark
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ndp-heterogeneous-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      NDP: Heterogeneous Architecture
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#general-cim" class="md-nav__link">
    <span class="md-ellipsis">
      General CiM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="General CiM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#specific-application-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Specific Application &amp; Algorithm
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modeling-simulation" class="md-nav__link">
    <span class="md-ellipsis">
      Modeling &amp; Simulation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-dram" class="md-nav__link">
    <span class="md-ellipsis">
      CIM: DRAM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-sram" class="md-nav__link">
    <span class="md-ellipsis">
      CIM: SRAM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CIM: SRAM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sram-cim-general-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      SRAM CIM: General Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-specific-use-or-application" class="md-nav__link">
    <span class="md-ellipsis">
      SRAM CIM: Specific Use or Application
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-hardware-software-co-design" class="md-nav__link">
    <span class="md-ellipsis">
      SRAM CIM: Hardware-Software Co-Design
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-simulator-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      SRAM CIM: Simulator &amp; Modeling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-transformer-accelerator" class="md-nav__link">
    <span class="md-ellipsis">
      SRAM CIM: Transformer Accelerator
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-rram" class="md-nav__link">
    <span class="md-ellipsis">
      CIM: RRAM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CIM: RRAM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rram-cim-simulator" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Simulator
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-architecture-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Architecture optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-design-space-exploration" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Design Space Exploration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Modeling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-training-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Training optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-compiler" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Compiler
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-float-point-processing" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Float-Point processing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-convolutional-layer" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Convolutional Layer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="RRAM CiM: Convolutional Layer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rram-cim-mapping-for-cnn" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Mapping for CNN
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-transformer-accelerator" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CIM: Transformer Accelerator
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-special-usage" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Special Usage
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-matrix-equation-solver" class="md-nav__link">
    <span class="md-ellipsis">
      RRAM CiM: Matrix Equation Solver
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-hybrid-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      CIM: Hybrid Architecture
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CIM: Hybrid Architecture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hybrid-cim-sram-general-logic" class="md-nav__link">
    <span class="md-ellipsis">
      Hybrid CIM: SRAM + General Logic
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hybrid-cim-sram-rram" class="md-nav__link">
    <span class="md-ellipsis">
      Hybrid CIM: SRAM + RRAM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hybrid-cim-memristormram-sram" class="md-nav__link">
    <span class="md-ellipsis">
      Hybrid CIM: Memristor/MRAM + SRAM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hybrid-cim-analog-digital" class="md-nav__link">
    <span class="md-ellipsis">
      Hybrid CIM: Analog + Digital
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      CIM: Quantization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CIM: Quantization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cim-quantization-for-analog-cim" class="md-nav__link">
    <span class="md-ellipsis">
      CIM Quantization: For Analog CIM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-quantization-for-all-cim" class="md-nav__link">
    <span class="md-ellipsis">
      CIM Quantization: For all CIM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-digital-cim" class="md-nav__link">
    <span class="md-ellipsis">
      CIM: Digital CIM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nvm" class="md-nav__link">
    <span class="md-ellipsis">
      NVM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/pku-lemonade/review/edit/main/docs/hardware/memory.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/pku-lemonade/review/raw/main/docs/hardware/memory.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<h1 id="memory-architecture">Memory Architecture<a class="headerlink" href="#memory-architecture" title="Permanent link">&para;</a></h1>
<h2 id="ndp-flash">NDP: Flash<a class="headerlink" href="#ndp-flash" title="Permanent link">&para;</a></h2>
<h3 id="general-application-specific-optimization">General Application-Specific Optimization<a class="headerlink" href="#general-application-specific-optimization" title="Permanent link">&para;</a></h3>
<p>Solution: Intergrate the compute unit into the SSD controller to process the capacity-sensitive applications.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>HPCA</td>
<td>UCLA</td>
<td>BeaconGNN: Large-Scale GNN Acceleration with Out-of-Order Streaming In-Storage Computing</td>
<td>DirectGraph format for out-of-order sampling; die-level processing units; channel-level command router</td>
<td>4</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>ETHZ</td>
<td>REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing</td>
<td>In-Storage processing</td>
<td>2</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>UCSD</td>
<td>In-Storage Acceleration of Retrieval Augmented Generation as a Service</td>
<td>metamorphic in-storage accelerator; Metadata Navigation Unit for dynamic data access</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arxiv</td>
<td>ETHZ</td>
<td>MARS: Processing-In-Memory Acceleration of Raw Signal Genome Analysis Inside the Storage Subsystem</td>
<td>PIM module inside the SSD controller; early signal quantization; read filtering</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>ICT</td>
<td>Cambricon-LLM: A Chiplet-Based Hybrid Architecture for On-Device Inference of 70B LLM</td>
<td>chiplet-based NPU &amp; NAND flash hybrid architecture; Hardware-aware tiling for NPU-flash workload distribution</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="llm-specific-optimization">LLM-Specific Optimization<a class="headerlink" href="#llm-specific-optimization" title="Permanent link">&para;</a></h4>
<p>Solution: Store weights in flash memory as read-only to prevent failures caused by write operations.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>Seoul National</td>
<td>AiF: Accelerating On-Device LLM Inference Using In-Flash Processing</td>
<td>in-flash GEMV computation; charge-recycling read to skip precharge/discharge steps in flash memory</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>THU</td>
<td>Lincoln: Real-Time 50~100B LLM Inference on Consumer Devices with LPDDR-Interfaced, Compute-Enabled Flash Memory</td>
<td>flash-on-LPDDR-interface for prefill phase; hybrid-bonding-based near-Flash computing for generation phase</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>PKU</td>
<td>InstAttention: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference</td>
<td>offloading decoding-phase attention computation to computational SSDs; SparF Attention flash-aware sparse algorithm</td>
<td>4</td>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="ndp-dimm">NDP: DIMM<a class="headerlink" href="#ndp-dimm" title="Permanent link">&para;</a></h3>
<p>Challenge: Memory wall causing high latency of data transfer between CPU and memory.</p>
<p>Solution: Put the compute unit in the memory or near the memory to reduce the data transfer overhead.</p>
<h4 id="general-application-specific-optimization_1">General Application-Specific Optimization<a class="headerlink" href="#general-application-specific-optimization_1" title="Permanent link">&para;</a></h4>
<p>Challenge: Existing NDP architecture are designed for general-purpose computing; not efficient for specific tasks like graph processing.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022</td>
<td>ISCA</td>
<td>Micron</td>
<td>To PIM or Not for Emerging General Purpose Processing in DDR Memory Systems</td>
<td>vector engine inside NDP bank; intelligent code offload decision</td>
<td>2</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>ISCA</td>
<td>Samsung</td>
<td>pSyncPIM: Partially Synchronous Execution of Sparse Matrix Operations for All-Bank PIM Architectures</td>
<td>partially synchronous PIM control; predicated execution; sparse matrix distribution &amp; compaction</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>ATC</td>
<td>RUC</td>
<td>Turbocharge ANNS on Real Processing-in-Memory by Enabling Fine-Grained Per-PIM-Core Scheduling</td>
<td>per-PU scheduling; persistent PIM kernel; per-PU dispatching with selective replication</td>
<td>3</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>UC Davis</td>
<td>NOVA: A Novel Vertex Management Architecture for Scalable Graph Processing</td>
<td>message-driven processors capable of executing algorithms; a direct-mapped cache with a write-back policy; support both asynchronous and bulk synchronous parallel execution models</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="dnn-specific-optimization">DNN-Specific Optimization<a class="headerlink" href="#dnn-specific-optimization" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021</td>
<td>HPCA</td>
<td>Seoul National</td>
<td>GradPIM: A Practical Processing-in-DRAM Architecture for Gradient Descent</td>
<td>fixed-function PIM architecture for DNN gradient descent; non-invasive PIM operations using reserved DDR commands</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2022</td>
<td>PACT</td>
<td>PKU</td>
<td>GNNear: Accelerating Full-Batch Training of Graph Neural Networks with Near-Memory Processing</td>
<td>splitting reduce operations to NDP units; narrow-shard strategy for data reuse; hybrid graph partition strategy for load balancing</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>ASPLOS</td>
<td>PKU</td>
<td>PIM-DL: Expanding the Applicability of Commodity DRAM-PIMs for Deep Learning via Algorithm-System Co-Optimization</td>
<td>algorithm for DNN to look-up-table conversion; auto-tuner for optimizing LUT-NN mapping on DRAM-PIMs</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="llm-specific-optimization_1">LLM-Specific Optimization<a class="headerlink" href="#llm-specific-optimization_1" title="Permanent link">&para;</a></h4>
<p>Challenge: LLM inference is fundamentally bottlenecked by memory bandwidth; HBM is expensive and not scalable.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>npj Unconv. Comput.</td>
<td>UMich</td>
<td>PIM-GPT: a hybrid process in memory accelerator for autoregressive transformers</td>
<td>hybrid system to accelerate GPT inference; mapping scheme for data locality and workload distribution</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>DAC</td>
<td>Seoul National</td>
<td>MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models</td>
<td>activation movement strategy to replace costly parameter movement; dynamic GPU-MoNDE load balancing for hot/cold experts</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>DAC</td>
<td>Hunan Univ.</td>
<td>A Real-time Execution System of Multimodal Transformer through PIM-GPU Collaboration</td>
<td>dynamic strategy for PIM-GPU task offloading; variable-length-aware PIM allocation optimizer; extended TVM backend for PIM-GPU command generation</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>MICRO</td>
<td>KAIST</td>
<td>PIMBA: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving</td>
<td>Unified PIM acceleration for both transformer and post-transformer LLMs; access interleaving technique for shared State-update Processing Unit</td>
<td>4</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>MICRO</td>
<td>Samsung</td>
<td>Duplex: A Device for Large Language Models with Mixture of Experts, Grouped Query Attention, and Continuous Batching</td>
<td>replace the GPU HBM memory die with HBM-PIM die; expert and attention co-processing for dynamic workload splitting within MoE/attn layers</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="memory-address-space">Memory Address Space<a class="headerlink" href="#memory-address-space" title="Permanent link">&para;</a></h4>
<p>Challenge: Host pages need to enable interleaving to improve concurrent throughput, while PIM pages need to disable it to maintain better locality, creating a conflict.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>DAC</td>
<td>Georgia Tech</td>
<td>vPIM: Efficient Virtual Address Translation for Scalable Processing-in-Memory Architectures</td>
<td>network-contention-aware hashing to minimize cross-stack page table walks; pre-translation using repurposed PIM cores to move page table walks off the critical path</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>ISCA</td>
<td>SJTU</td>
<td>UM-PIM: DRAM-based PIM with Uniform &amp; Shared Memory Space</td>
<td>Uniform shared CPU-PIM memory; dual-track memory management; zero-copy data re-layout</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="memory-allocation-management">Memory Allocation &amp; Management<a class="headerlink" href="#memory-allocation-management" title="Permanent link">&para;</a></h4>
<p>Challenge: Existing NDP architecture has numerous independent memory spaces; lacks unified management; and features inefficient memory allocation.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>ISCA</td>
<td>KAIST</td>
<td>PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for Processing-In-Memory (PIM) Architectures</td>
<td>PIM-specific memory allocator; hierarchical memory allocation scheme; hardware metadata cache</td>
<td>4</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>ETHZ</td>
<td>PUMA: Efficient and Low-Cost Memory Allocation and Alignment Support for Processing-Using-Memory Architectures</td>
<td>aligned memory allocator for PUM; DRAM-aware memory allocation</td>
<td>2</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>MICRO</td>
<td>KAIST</td>
<td>PIM-MMU: A Memory Management Unit for Accelerating Data Transfers in Commercial PIM Systems</td>
<td>data copy engine for host-PIM transfers; PIM-aware memory scheduler for MLP maximization; memory remapping unit for dual address mapping</td>
<td>2</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Amazon</td>
<td>DL-PIM: Improving Data Locality in Processing-in-Memory Systems</td>
<td>subscription-based architecture to proactively move data; distributed address-indirection hardware lookup table</td>
<td>3</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="pim-compiler-isa-extension">PIM Compiler &amp; ISA Extension<a class="headerlink" href="#pim-compiler-isa-extension" title="Permanent link">&para;</a></h4>
<p>Challenge: Existing compilers are not optimized for locality-aware PIM architectures and require specialized programming models to fully utilize PIM capabilities.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2015</td>
<td>ISCA</td>
<td>Seoul National</td>
<td>PIM-Enabled Instructions: A Low-Overhead; Locality-Aware Processing-in-Memory Architecture</td>
<td>PIM-Enabled Instructions for ISA extension; PIM directory for atomicity and coherence; single-cache-block restriction</td>
<td>3</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2020</td>
<td>ISCA</td>
<td>UCSB</td>
<td>iPIM: Programmable In-Memory Image Processing Accelerator Using Near-Bank Architecture</td>
<td>Single-Instruction-Multiple-Bank ISA; register allocation; instruction reordering</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>POSTECH</td>
<td>ATIM: Autotuning Tensor Programs for Processing-in-DRAM</td>
<td>autotuning framework for DRAM PIM; search-based optimizing tensor compiler; balanced evolutionary search algorithm</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="evaluation-simulators">Evaluation &amp; Simulators<a class="headerlink" href="#evaluation-simulators" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>THU</td>
<td>UniNDP: A Unified Compilation and Simulation Tool for Near DRAM Processing Architectures</td>
<td>unified NDP hardware abstraction; NDP compiler optimization; instruction-driven NDP simulator</td>
<td>3</td>
<td>5</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>ETHZ</td>
<td>EasyDRAM: An FPGA-based Infrastructure for Fast and Accurate End-to-End Evaluation of Emerging DRAM Techniques</td>
<td>FPGA-based DRAM evaluation framework; C++ high-level language for description; time scaling for accurate modeling</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="intra-dimm-communication">Intra-DIMM Communication<a class="headerlink" href="#intra-dimm-communication" title="Permanent link">&para;</a></h4>
<p>Challenge: High latency of intra-DIMM (cross-bank) communication via host CPU forwarding.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>ISCA</td>
<td>THU</td>
<td>NDPBridge: Enabling Cross-Bank Coordination in Near-DRAM-Bank Processing Architectures</td>
<td>gather &amp; scatter messages via buffer chip; task-based message-passing model; hierarchical, data-transfer-aware load balancing</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>Samsung</td>
<td>Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory Scatter-Gather</td>
<td>In-DRAM fine-grained scatter-gather via data bus offsets; fine-grained cache architecture using fg-tags; Standard DDR command interpretation for FIM control; Combined graph tiling with fine-grained memory access</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>ETHZ</td>
<td>PIMDAL: Mitigating the Memory Bottleneck in Data Analytics using a Real Processing-in-Memory System</td>
<td>PIMDAL library for DB operators; quicksort/mergesort/hashing on UPMEM PIM; scatter/gather/async transfers for PIM communication</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Seoul National</td>
<td>PID-Comm: A Fast and Flexible Collective Communication Framework for Commodity Processing-in-DIMM Devices</td>
<td>Virtual hypercube PIM model; PE-assisted data reordering; in-register and cross-domain data modulation</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>KAIST</td>
<td>PIMnet: A Domain-Specific Network for Efficient Collective Communication in Scalable PIM</td>
<td>domain-specific PIM interconnect; hierarchical network for PIM packaging; PIM-controlled deterministic scheduling</td>
<td>2</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="inter-dimm-communication">Inter-DIMM Communication<a class="headerlink" href="#inter-dimm-communication" title="Permanent link">&para;</a></h4>
<p>Challenge: High latency of inter-DIMM (cross-DIMM) communication via host CPU forwarding.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2017</td>
<td>MEMSYS</td>
<td>UCLA</td>
<td>AIM: Accelerating Computational Genomics through Scalable and Noninvasive Accelerator-Interposed Memory</td>
<td>placing FPGA chip between DIMM and the conventional memory network; multi-drop bus for inter-accelerator communication</td>
<td>1</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2023</td>
<td>ASPLOS</td>
<td>THU</td>
<td>ABNDP: Co-optimizing Data Access and Load Balance in Near-Data Processing</td>
<td>Traveller Cache; hybrid task scheduling; hybrid scheduling leveraging distributed cache</td>
<td>4</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2023</td>
<td>HPCA</td>
<td>PKU</td>
<td>DIMM-Link: Enabling Efficient Inter-DIMM Communication for Near-Memory Processing</td>
<td>high-speed hardware link bridges between DIMMs; direct intra-group P2P communication &amp; broadcast; hybrid routing mechanism for inter-group communication</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>SJTU</td>
<td>AsyncDIMM: Achieving Asynchronous Execution in DIMM-Based Near-Memory Processing</td>
<td>Offload-Schedule-Return mechanism; switch-recovery scheduling; explicit/implicit synchronization</td>
<td>2</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2018</td>
<td>MICRO</td>
<td>UIUC</td>
<td>Application-Transparent Near-Memory Processing Architecture with Memory Channel Network</td>
<td>integrates a processor on a buffered DIMM; application-transparent near-memory processing; leverages memory channels for high-bandwidth/low-latency inter-processor communication</td>
<td>3</td>
<td>4</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="concurrent-host-and-pim-operations">Concurrent Host and PIM operations<a class="headerlink" href="#concurrent-host-and-pim-operations" title="Permanent link">&para;</a></h4>
<p>Challenge: High latency of concurrent host CPU/GPU and PIM operations via host CPU forwarding.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>IEEE CA</td>
<td>KAIST</td>
<td>Analysis of Data Transfer Bottlenecks in Commercial PIM Systems: A Study With UPMEM-PIM</td>
<td>runtime data transposition causing high CPU overhead; PIM-integrated system memory mapping impact</td>
<td>2</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>ASPLOS</td>
<td>KAIST</td>
<td>NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing</td>
<td>dual row buffer architecture; sub-batch interleaving; greedy min-load bin packing algorithm</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>ICT</td>
<td>Make LLM Inference Affordable to Everyone: Augmenting GPU Memory with NDP-DIMM</td>
<td>activation sparsity-based hot(GPU)/cold(NDP) neuron partitioning; offline ILP + online predictor for neuron partition; window-based online remapping for GPU-NDP &amp; NDP-NDP load balance</td>
<td>2</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>Univ. of Virginia</td>
<td>Membrane: Accelerating Database Analytics with Bank-Level DRAM-PIM Filtering</td>
<td>bank-level DRAM-PIM filtering; CPU-PIM cooperative query execution; denormalization for PIM-amenable filtering</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="optimizations-on-upmem-pim">Optimizations on UPMEM-PIM<a class="headerlink" href="#optimizations-on-upmem-pim" title="Permanent link">&para;</a></h4>
<p>Challenge: The original UMPEM API library is not well-suited for all workloads especially for those with cross-bank communication.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>arXiv</td>
<td>ETHZ</td>
<td>A Framework for High-throughput Sequence Alignment using Real Processing-in-Memory Systems</td>
<td>Alignment-in-Memory framework; hybrid WRAM-MRAM sketch data management for PIM</td>
<td>2</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>ETHZ</td>
<td>PIMDAL: Mitigating the Memory Bottleneck in Data Analytics using a Real Processing-in-Memory System</td>
<td>PIMDAL library on UPMEM PIM system for data analytics; scatter/gather-aware transfers for inter-PIM communication; Apache Arrow for host memory management</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="pim-in-cache-computing">PIM: In-Cache-Computing<a class="headerlink" href="#pim-in-cache-computing" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Torino</td>
<td>ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions</td>
<td>ARCANE in-cache NMC coprocessor architecture; software-defined matrix ISA for NMC abstraction; cache-integrated control runtime for NMC management</td>
<td>3</td>
<td>4</td>
<td>4</td>
</tr>
</tbody>
</table>
<h3 id="pim-ndp-benchmarks">PIM &amp; NDP: Benchmarks<a class="headerlink" href="#pim-ndp-benchmarks" title="Permanent link">&para;</a></h3>
<p>Challenge: Conventional parallel computing benchmarks are not suitable for PIM/NDP.</p>
<h4 id="benchmarks-for-conventional-computing">Benchmarks for Conventional Computing<a class="headerlink" href="#benchmarks-for-conventional-computing" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021</td>
<td>ATC</td>
<td>UBC</td>
<td>A Case Study of Processing-in-Memory in off-the-Shelf Systems</td>
<td>benchmark</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022</td>
<td>IEEE Access</td>
<td>ETH</td>
<td>Benchmarking a New Paradigm: Experimental Analysis and Characterization of a Real Processing-in-Memory System</td>
<td>benchmark suite PrIM</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>CAL</td>
<td>KAIST</td>
<td>Analysis of Data Transfer Bottlenecks in Commercial PIM Systems: A Study With UPMEM-PIM</td>
<td>low MLP; manual data placement; unbalanced thread allocation and scheduling</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>IEEE Access</td>
<td>Lisbon</td>
<td>NDPmulator: Enabling Full-System Simulation for Near-Data Accelerators From Caches to DRAM</td>
<td>simulator PiMulator based on Ramulator &amp; gem5; full system support; multiple ISA support</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>HPCA</td>
<td>KAIST</td>
<td>Pathfinding Future PIM Architectures by Demystifying a Commercial PIM Technology</td>
<td>simulator uPIMulator</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="benchmarks-for-quantum-computing">Benchmarks for Quantum Computing<a class="headerlink" href="#benchmarks-for-quantum-computing" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>ASPDAC</td>
<td>NUS</td>
<td>PIMutation: Exploring the Potential of PIM Architecture for Quantum Circuit Simulation</td>
<td>PIMutation framework for quantum circuit simulation; gate merging optimization; row swapping instead of matrix multiplication; vector partitioning for separable states; leveraging UPMEM PIM architecture</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="ndp-cxl">NDP: CXL<a class="headerlink" href="#ndp-cxl" title="Permanent link">&para;</a></h3>
<p>Challenge: No direct physical connectivity between the banks in the DIMM-based NDP architecture. Limited number of DDR channels causing poor scalability.</p>
<p>Solution: Introduce CXL-based interconnects to enable direct communication between memory banks; Use CXL memory pools and CXL switches to enable scalable NDP architecture.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022</td>
<td>MICRO</td>
<td>UCSB</td>
<td>BEACON: Scalable Near-Data-Processing Accelerators for Genome Analysis near Memory Pool with the CXL Support</td>
<td>scalable hardware accelerator inside CXL switch or bank; lossless memory expansion for CXL memory pools</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>ICS</td>
<td>Samsung</td>
<td>CLAY: CXL-based Scalable NDP Architecture Accelerating Embedding Layers</td>
<td>direct interconnect between DRAM clusters; dedicated memory address mapping scheme; Multi-CLAY system support through customized CXL switch</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>MICRO</td>
<td>SK Hyrix</td>
<td>Low-overhead General-purpose Near-Data Processing in CXL Memory Expanders</td>
<td>CXL.mem protocol instead of CXL.io (DMA) for low-latency; lightweight threads to reduce address calculation overhead</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>Seoul National</td>
<td>COSMOS: A CXL-Based Full In-Memory System for Approximate Nearest Neighbor Search</td>
<td>CXL core-based ANNS task offload; rank-level parallel distance computation; adjacency-aware data placement algorithm</td>
<td>2</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>ASPLOS</td>
<td>UMich</td>
<td>PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference</td>
<td>hierarchical CXL PIM-PNM compute architecture; use die-shot to estimate area cost; multiple LLM parallelism policies</td>
<td>2</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="ndp-3d-stacked-dram">NDP: 3D-stacked DRAM<a class="headerlink" href="#ndp-3d-stacked-dram" title="Permanent link">&para;</a></h3>
<p>Challenge: No direct physical connectivity between the banks in the DIMM-based NDP architecture.</p>
<p>Solution: Use TSVs to provide TB/s level bandwidth in inter-bank communication &amp; band-to-logic layer communication.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2013</td>
<td>PACT</td>
<td>KAIST</td>
<td>Memory-centric System Interconnect Design with Hybrid Memory Cubes</td>
<td>memory-centric network; distributor-based topology for reduced latency; non-minimal routing for higher throughput</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>DAC</td>
<td>SNU</td>
<td>MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models</td>
<td>NDP for MoE; activation movement; GPU-MoNDE load-balancing scheme</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>ASPLOS</td>
<td>PKU</td>
<td>SpecPIM: Accelerating Speculative Inference on PIM-Enabled System via Architecture-Dataflow Co-Exploration</td>
<td>algorithmic and architectural heterogeneity; PIM resource allocation; multi-model collaboration workflow</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="ndp-hbm">NDP: HBM<a class="headerlink" href="#ndp-hbm" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021</td>
<td>ISCA</td>
<td>Samsung</td>
<td>Hardware Architecture and Software Stack for PIM Based on Commercial DRAM Technology Industrial Product</td>
<td>drop-in replacement for standard HBM2; bank-level parallelism using standard DRAM commands; address aligned mode to tolerate host-side command reordering</td>
<td>3</td>
<td>5</td>
<td>3</td>
</tr>
<tr>
<td>2022</td>
<td>Hot Chips</td>
<td>Samsung</td>
<td>Aquabolt-XL HBM2-PIM, LPDDR5-PIM With In-Memory Processing, and AXDIMM With Acceleration Buffer</td>
<td>HBM2-PIM with bank-level SIMD programmable computing units; Acceleration DIMM with acceleration buffers for rank-level parallelism</td>
<td>2</td>
<td>5</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="benchmark">Benchmark<a class="headerlink" href="#benchmark" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2019</td>
<td>DAC</td>
<td>ETHZ</td>
<td>NAPEL: Near-Memory Computing Application Performance Prediction via Ensemble Learning</td>
<td>simulator Ramulator-PIM; tracefile from Ramulator &amp; run on zsim</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2021</td>
<td>CAL</td>
<td>UVA</td>
<td>MultiPIM: A Detailed and Configurable Multi-Stack Processing-In-Memory Simulator</td>
<td>simulator MultiPIM; multi-stack &amp; virtual memory support; parallel offloading</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="ndp-heterogeneous-architecture">NDP: Heterogeneous Architecture<a class="headerlink" href="#ndp-heterogeneous-architecture" title="Permanent link">&para;</a></h4>
<p>Challenge: Different PIM architectures have different characteristics and performance trade-offs; communicating between different PIM architectures is challenging.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>HUST</td>
<td>HeterRAG: Heterogeneous Processing-in-Memory Acceleration for Retrieval-augmented Generation</td>
<td>combine DIMM-PIM and HBM-PIM for acceleration; locality-aware retrieval and generation; fine-grained parallel pipelining</td>
<td>2</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>NUS</td>
<td>LEAP: LLM Inference on Scalable PIM-NoC Architecture with Balanced Dataflow and Fine-Grained Parallelism</td>
<td>data dynamicity-aware task assignment to PIM or NoC; fine-grained model partitioning and heuristically optimized spatial mapping strategy</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>THU</td>
<td>CompAir: Synergizing Complementary PIMs and In-Transit NoC Computation for Efficient LLM Acceleration</td>
<td>heterogeneous DRAM-PIM and SRAM-PIM architecture with hybrid bonding; in-transit NoC computation with Curry ALU; hierarchical ISA for hybrid PIM systems</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="general-cim">General CiM<a class="headerlink" href="#general-cim" title="Permanent link">&para;</a></h3>
<h4 id="specific-application-algorithm">Specific Application &amp; Algorithm<a class="headerlink" href="#specific-application-algorithm" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>ISVLSI</td>
<td>USC</td>
<td>Multi-Objective Neural Architecture Search for In-Memory Computing</td>
<td>neural architecture search methodology; integration of Hyperopt, PyTorch and MNSIM</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Intel</td>
<td>CiMNet: Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware</td>
<td>framework that jointly searches for optimal sub-networks and hardware configurations for CiM architectures; multi-objective evolutionary search method</td>
<td>4</td>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>AICAS</td>
<td>UVA</td>
<td>Optimizing and Exploring System Performance in Compact Processing-in-Memory-based Chips</td>
<td>Pipeline Method for Compact PIM Designs; Dynamic Duplication Method (DDM); Maximum NN Size Estimation &amp; Deployment in Compact PIM Design</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="modeling-simulation">Modeling &amp; Simulation<a class="headerlink" href="#modeling-simulation" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2018</td>
<td>TCAD</td>
<td>ASU</td>
<td>NeuroSim: A Circuit-Level Macro Model for Benchmarking Neuro-Inspired Architectures in Online Learning</td>
<td>estimate the circuit-level performance of neuro-inspired architectures; estimates the area, latency, dynamic energy, and leakage power; Support both SRAM and eNVM; tested on 2-layer MLP NN, MNIST</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2019</td>
<td>IEDM</td>
<td>Georgia Tech</td>
<td>DNN+NeuroSim: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators with Versatile Device Technologies</td>
<td>a python wrapper to interface NeuroSim; for inference only</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2020</td>
<td>TCAD</td>
<td>ZJU</td>
<td>Eva-CiM: A System-Level Performance and Energy Evaluation Framework for Computing-in-Memory Architectures</td>
<td>models for capturing memory access and dependency-aware ISA traces; models for quantifying interactions between the host CPU and the CiM module</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022</td>
<td>ICCAD</td>
<td>Purdue</td>
<td>Design Space and Memory Technology Co-Exploration for In-Memory Computing Based Machine Learning Accelerators</td>
<td>simulation framework to evaluate the systemlevel performance of IMC architecture; area-aware weight mapping strategy</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>ISPASS</td>
<td>MIT</td>
<td>CiMLoop: A Flexible, Accurate, and Fast Compute-In-Memory Modeling Tool</td>
<td>flexible specification to describe CiM systems; accurate model/fast statistical model of data-value-dependent component energy</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>ASPDAC</td>
<td>HKUST</td>
<td>MICSim: A Modular Simulator for Mixed-signal Compute-in-Memory based AI Accelerator</td>
<td>modulared Neurosim; data statistic-based average-mode instead of trace-based mode</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="cim-dram">CIM: DRAM<a class="headerlink" href="#cim-dram" title="Permanent link">&para;</a></h3>
<p>Solution: Rather than placing logic units into DRAM; modify the physical structure of DRAM/eDRAM to enable in-memory computing.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021</td>
<td>ICCD</td>
<td>ASU</td>
<td>CIDAN: Computing in DRAM with Artificial Neurons</td>
<td>Threshold Logic Processing Element (TLPE) for in-memory computation; Four-bank activation window; Configurable threshold functions; Energy-efficient bitwise operations; Integration with DRAM architecture</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022</td>
<td>HPCA</td>
<td>UCSD</td>
<td>TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer</td>
<td>token-based dataflow for general Transformer-based models; ring-based data broadcast in modified HBM</td>
<td>4</td>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>2024</td>
<td>A-SSCC</td>
<td>UNIST</td>
<td>A 273.48 TOPS/W and 1.58 Mb/mm2 Analog-Digital Hybrid CIM Processor with Transpose Ternary-eDRAM Bitcell</td>
<td>analog DRAM CIM for partial sum and digital adder</td>
<td>1</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>KAIST</td>
<td>RED: Energy Optimization Framework for eDRAM-based PIM with Reconfigurable Voltage Swing and Retention-aware Scheduling</td>
<td>RED framework for energy optimization; reconfigurable eDRAM design; retention-aware scheduling; trade-off analysis between RBL voltage swing, sense amplifier power, and retention time; refresh skipping and sense amplifier power gating</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UTokyo</td>
<td>MVDRAM: Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM Acceleration</td>
<td>GeMV operations for end-to-end low-bit LLM inference using unmodified DRAM; processor-DRAM co-design; on-the-fly vector encoding; horizontal matrix layout</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Purdue</td>
<td>HALO: Memory-Centric Heterogeneous Accelerator with 2.5D Integration for Low-Batch LLM Inference</td>
<td>heterogeneous CiD/CiM accelerator; phase-aware mapping strategy</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="cim-sram">CIM: SRAM<a class="headerlink" href="#cim-sram" title="Permanent link">&para;</a></h3>
<p>Challenge: Memory wall causing high latency of data transfer between CPU and memory; DIMM-based NDP causing high energy consumption; area overhead and low performance efficiency.</p>
<p>Solution: Generally modify the physical structure of SRAM to enable in-memory computing; rather than placing logic units into SRAM.</p>
<h4 id="sram-cim-general-architecture">SRAM CIM: General Architecture<a class="headerlink" href="#sram-cim-general-architecture" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>ISCAS</td>
<td>NYCU</td>
<td>CIMR-V: An End-to-End SRAM-based CIM Accelerator with RISC-V for AI Edge Device</td>
<td>incorporates CIM layer fusion, convolution/max pooling pipeline, and weight fusion; weight fusion: pipelining the CIM convolution and weight loading</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2018</td>
<td>JSSC</td>
<td>MIT</td>
<td>CONV-SRAM: An Energy-Efficient SRAM With In-Memory Dot-Product Computation for Low-Power Convolutional Neural Networks</td>
<td>SRAM-embedded convolution (dot-product) computation architecture for BNN; support multi-bit input-output</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>ESSCIRC</td>
<td>THU</td>
<td>A 65nm 8b-Activation 8b-Weight SRAM-Based Charge-Domain Computing-in-Memory Macro Using A Fully-Parallel Analog Adder Network and A Single-ADC Interface</td>
<td>SRAM-based CD-CiM architecture; charge-domain analog adder tree; ReLU-optimized ADC</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2021</td>
<td>ISSCC</td>
<td>TSMC</td>
<td>An 89TOPS/W and 16.3TOPS/mm2 All-Digital SRAM-Based Full-Precision Compute-In Memory Macro in 22nm for Machine-Learning Edge Applications</td>
<td>programmable bit-widths for both input and weights; SRAM and CIM mode</td>
<td>2</td>
<td>5</td>
<td>1</td>
</tr>
<tr>
<td>2021</td>
<td>JSSC</td>
<td>KAIST</td>
<td>Z-PIM: A Sparsity-Aware Processing-in-Memory Architecture With Fully Variable Weight Bit-Precision for Energy-Efficient Deep Neural Networks</td>
<td>bit-serial operation to support variable weight bit-precision; data mapping and computation flow for sparsity handling</td>
<td>3</td>
<td>4</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="sram-cim-specific-use-or-application">SRAM CIM: Specific Use or Application<a class="headerlink" href="#sram-cim-specific-use-or-application" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>TCAS-I</td>
<td>UIC</td>
<td>MC-CIM: Compute-in-Memory With Monte-Carlo Dropouts for Bayesian Edge Intelligence</td>
<td>SRAM-based CIM macros to accelerate Monte-Carlo dropout; compute reuse between consecutive iterations</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>DAC</td>
<td>GWU</td>
<td>Addition is Most You Need: Efficient Floating-Point SRAM Compute-in-Memory by Harnessing Mantissa Addition</td>
<td>decomposing FP mantissa multiplication into sub-ADD and sub-MUL; hybrid-domain SRAM CIM architecture</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>A-SSCC</td>
<td>Georgia Tech</td>
<td>A 28nm 1.80Mb/mm2 Digital/Analog Hybrid SRAM-CIM Macro Using 2D-Weighted Capacitor Array for Complex Number Mac Operations</td>
<td>Hybrid DCIM/ACIM SRAM; lightweight correction schemes; complex CIM-SRAM units</td>
<td>2</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>GWU</td>
<td>Unicorn-CIM: Uncovering the Vulnerability and Improving the Resilience of High-Precision Compute-in-Memory</td>
<td>SRAM-CIM for FP DNNs; a fault-injection framework for FP DNNs; a ECC scheme for FP DNNs</td>
<td>3</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>ISCAS</td>
<td>KAUST</td>
<td>Reconfigurable Precision INT4-8/FP8 Digital Compute-in-Memory Macro for AI Acceleration</td>
<td>parallel-input approach; mantissa parallel-alignment technique</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="sram-cim-hardware-software-co-design">SRAM CIM: Hardware-Software Co-Design<a class="headerlink" href="#sram-cim-hardware-software-co-design" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022</td>
<td>TCAD</td>
<td>NTHU</td>
<td>MARS: Multi-macro Architecture SRAM CIM-Based Accelerator with Co-designed Compressed Neural Networks</td>
<td>sparsity algorithm designed for SRAM CiM; quantization algorithm with BN fusion</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2023</td>
<td>TCAD</td>
<td>UCSB</td>
<td>SDP: Co-Designing Algorithm, Dataflow, and Architecture for In-SRAM Sparse NN Acceleration</td>
<td>double-broadcast hybridgrained pruning method; bit-serial booth inSRAM (BBS) multiplication dataflow</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>TCAD</td>
<td>BUAA</td>
<td>DDC-PIM: Efficient Algorithm/Architecture Co-Design for Doubling Data Capacity of SRAM-Based Processing-in-Memory</td>
<td>doubling the equivalent data capacity of SRAM-based PIM; FCC algorithm to obtain bitwise complementary filters</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>TCASAI</td>
<td>Purdue</td>
<td>Algorithm Hardware Co-Design for ADC-Less Compute In-Memory Accelerator</td>
<td>reduce ADC overhead in analog CiM architectures; Quantization-Aware Training; Partial Sum Quantization; ADC-Less hybrid analog-digital CiM hardware architecture HCiM</td>
<td>3</td>
<td>3</td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>TCAD</td>
<td>BUAA</td>
<td>Efficient SRAM-PIM Co-design by Joint Exploration of Value-Level and Bit-Level Sparsity</td>
<td>hybrid-grained pruning algorithm; customized Dyadic Block PIM (DB-PIM) architecture</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="sram-cim-simulator-modeling">SRAM CIM: Simulator &amp; Modeling<a class="headerlink" href="#sram-cim-simulator-modeling" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2020</td>
<td>ISCAS</td>
<td>JCU</td>
<td>MemTorch: A Simulation Framework for Deep Memristive Cross-Bar Architectures</td>
<td>supports both GPUs and CPUs; integrates directly with PyTorch; simulate non-idealities of memristive devices within cross-bar, tested on VGG-16, CIFAR-10</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2021</td>
<td>TCAD</td>
<td>Geogia Tech</td>
<td>DNN+NeuroSim V2.0: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators for On-Chip Training</td>
<td>non-ideal device properties of NVMS' effect for on-chip training</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>DAC</td>
<td>BUAA</td>
<td>CIMFlow: An Integrated Framework for Systematic Design and Evaluation of Digital CIM Architectures</td>
<td>workflow for implementing and evaluating DNN workloads on digital CIM architectures; CIM-specific ISA design; compilation flow built on the MLIR infrastructure</td>
<td>4</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="sram-cim-transformer-accelerator">SRAM CIM: Transformer Accelerator<a class="headerlink" href="#sram-cim-transformer-accelerator" title="Permanent link">&para;</a></h4>
<p>Challenge: Transformer architecture is widely used in NLP and CV tasks. Existing SRAM CIM architectures are not suitable for transformer acceleration.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>DATE</td>
<td>PKU</td>
<td>Leveraging Compute-in-Memory for Efficient Generative Model Inference in TPUs</td>
<td>architecture model and simulator for CIM-based TPUs; designed for LLM inference</td>
<td>4</td>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>2023</td>
<td>arXiv</td>
<td>Keio</td>
<td>An 818-TOPS/W CSNR-31dB SQNR-45dB 10-bit Capacitor-Reconfiguring Computing-in-Memory Macro with Software-Analog Co-Design for Transformers</td>
<td>Capacitor-Reconfiguring analog CIM architecture</td>
<td>1</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Purdue</td>
<td>Hardware-Software Co-Design for Accelerating Transformer Inference Leveraging Compute-in-Memory</td>
<td>SRAM based softmax-friendly CIM architecture for transformer; finer-granularity pipelining strategy</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>PKU</td>
<td>Leveraging Compute-in-Memory for Efficient Generative Model Inference in TPUs</td>
<td>Energy-efficient CIM core integration in TPUs (replace the original MXU); CIM-MXU with systolic data path; Array dimension scaling for CIM-MXU; Area-efficient CIM macro design; Mapping engine for generative model inference</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>JSSC</td>
<td>THU</td>
<td>MulTCIM: Digital Computing-in-Memory-Based Multimodal Transformer Accelerator With Attention-Token-Bit Hybrid Sparsity</td>
<td>long reuse elimination scheduler (LRES) to dynamically reshape the attention matrix; runtime token pruner (RTP) to remove insignificant tokens; modal-adaptive CIM network (MACN) to dynamically divide CIM cores into Pipeline; effective-bits-balanced CIM (EBBCIM) macro architecture</td>
<td>5</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="cim-rram">CIM: RRAM<a class="headerlink" href="#cim-rram" title="Permanent link">&para;</a></h3>
<p>Challenge: RRAM devices are non-volatile and have high density; suitable for CIM applications. However; RRAM devices have non-ideal effects that can cause significant performance degradation.</p>
<h4 id="rram-cim-simulator">RRAM CiM: Simulator<a class="headerlink" href="#rram-cim-simulator" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2018</td>
<td>TCAD</td>
<td>THU</td>
<td>MNSIM: Simulation Platform for Memristor-Based Neuromorphic Computing System</td>
<td>reference design for largescale neuromorphic accelerator and can also be customized; behavior-level computing accuracy model</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>TCAD</td>
<td>THU</td>
<td>MNSIM 2.0: A Behavior-Level Modeling Tool for Processing-In-Memory Architectures</td>
<td>integrated PIM-oriented NN model training and quantization flow; unified PIM memory array model; support for mixed-precision NN operations</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>DATE</td>
<td>UCAS</td>
<td>PIMSIM-NN: An ISA-based Simulation Framework for Processing-in-Memory Accelerators</td>
<td>event-driven simulation approach; can evaluate the optimizations of software and hardware independently</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-architecture">RRAM CiM: Architecture<a class="headerlink" href="#rram-cim-architecture" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2019</td>
<td>ASPLOS</td>
<td>Purdue &amp; HP</td>
<td>PUMA: A Programmable Ultra-efficient Memristor-based Accelerator for Machine Learning Inference</td>
<td>Programmable and general-purpose ReRAM based ML Accelerator; Supports an instruction set; Has potential for DNN training; Provides simulator that accepts model</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2018</td>
<td>ICRC</td>
<td>Purdue &amp; HP</td>
<td>Hardware-Software Co-Design for an Analog-Digital Accelerator for Machine Learning</td>
<td>compiler to translate model to ISA; ONNX interpreter to support models in common DL frame work; simulator to evaluate performance</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>NANOARCH</td>
<td>HUST</td>
<td>Heterogeneous Instruction Set Architecture for RRAM-enabled In-memory Computing</td>
<td>General ISA for RRAM CiM &amp; digital heterogeneous architecture; a tile-processing unit-array three-level architecture</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>VLSI-SoC</td>
<td>RWTH Aachen University</td>
<td>Architecture-Compiler Co-design for ReRAM-Based Multi-core CIM Architectures</td>
<td>inference latency predictions and analysis of the crossbar utilization for CNN</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>CAS</td>
<td>A Fully Hardware Implemented Accelerator Design in ReRAM Analog Computing without ADCs</td>
<td>Based on Stochastic Binary Neural Networks; Winner-Take-All (WTA) strategy; Hardware implemented sigmoid and softmax</td>
<td>4</td>
<td>3</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-architecture-optimization">RRAM CiM: Architecture optimization<a class="headerlink" href="#rram-cim-architecture-optimization" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>MICRO</td>
<td>HUST</td>
<td>DRCTL: A Disorder-Resistant Computation  Translation Layer Enhancing the Lifetime and  Performance of Memristive CIM Architecture</td>
<td>address conversion method for dynamic scheduling; hierarchical wear-leveling (HWL) strategy for reliability improvement; data layout-aware selective remapping (LASR) to improve communication locality and reduce latency</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>DATE</td>
<td>RWTH Aachen University</td>
<td>CLSA-CIM: A Cross-Layer Scheduling Approach for Computing-in-Memory Architectures</td>
<td>algorithm to decide which parts of NN are duplicated to reduce inference latency; cross layer scheduling on tiled CIM architectures</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>TC</td>
<td>SJTU</td>
<td>ERA-BS: Boosting the Efficiency of ReRAM-Based  PIM Accelerator With Fine-Grained  Bit-Level Sparsity</td>
<td>bit-level sparsity in both weights and activations; bit-flip scheme; dynamic activation sparsity exploitation scheme</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>TETCI</td>
<td>TU Delft</td>
<td>Accurate and Energy-Efficient Bit-Slicing for RRAM-Based Neural Networks</td>
<td>unbalanced bit-slicing scheme for higher accuracy; holistic solution using 2's compliment</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>Science</td>
<td>USC</td>
<td>Programming memristor arrays with arbitrarily high precision for analog computing</td>
<td>represent high-precision numbers using multiple relatively low-precision analog devices;using RRAM CIM to solve PDEs</td>
<td>5</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-design-space-exploration">RRAM CiM: Design Space Exploration<a class="headerlink" href="#rram-cim-design-space-exploration" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>RWTH Aachen</td>
<td>Optimizing Binary and Ternary Neural Network Inference on RRAM Crossbars using CIM-Explorer</td>
<td>Tensor Virtual Machine (TVM)-based compiler; implementation of different mapping techniques; DSE flow to analyze the impact of parameters</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-modeling">RRAM CiM: Modeling<a class="headerlink" href="#rram-cim-modeling" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>AICAS</td>
<td>RWTH Aachen University</td>
<td>A Calibratable Model for Fast Energy Estimation of MVM Operations on RRAM Crossbars</td>
<td>system energy model for MVM on ReRAM crossbars; methodology to study the effect of the selection transistor and wire parasitics in 1T1R crossbar arrays</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>MIT</td>
<td>Modeling Analog-Digital-Converter Energy and Area for Compute-In-Memory Accelerator Design</td>
<td>architecture-level model that estimates ADC energy and area</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-training-optimization">RRAM CiM: Training optimization<a class="headerlink" href="#rram-cim-training-optimization" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021</td>
<td>TCAD</td>
<td>SJTU</td>
<td>ITT-RNA: Imperfection Tolerable Training for RRAM-Crossbar-Based Deep Neural-Network Accelerator</td>
<td>prevent the large-weight synapses from being mapped to the imperfect memristor cells; off-device training algorithm to alleviate the accumulation of errors across multiple layers; bit-wise mechanism to compensate the resistance variations</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2023</td>
<td>arXiv</td>
<td>UND</td>
<td>U-SWIM: Universal Selective Write-Verify for Computing-in-Memory Neural Accelerators</td>
<td>only do write-verify for important weights; based on weight second derivatives as a guide</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2023</td>
<td>Adv. Mater.</td>
<td>UMich</td>
<td>Bulk‐Switching Memristor‐Based Compute‐In‐Memory Module for Deep Neural Network Training</td>
<td>Bulk-ReRAM based digital-CIM hybrid architecture for training; CIM for forward, digital for backward</td>
<td>4</td>
<td>4</td>
<td>1</td>
</tr>
<tr>
<td>2024</td>
<td>APIN</td>
<td>SWU</td>
<td>Multi-optimization scheme for in-situ training of memristor neural network based on contrastive learning</td>
<td>optimizations to the deployment method, loss function and gradient calculation; compensation measures for non-ideal effects</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>TNNLS</td>
<td>SNU</td>
<td>Efficient Hybrid Training Method for Neuromorphic Hardware Using Analog Nonvolatile Memory</td>
<td>Hybrid offline-online training method</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-compiler">RRAM CiM: Compiler<a class="headerlink" href="#rram-cim-compiler" title="Permanent link">&para;</a></h4>
<p>Challenge: Compiler for RRAM CIM is not well studied. Existing compilers are either for specific architecture or not efficient.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>TACO</td>
<td>HUST</td>
<td>A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures</td>
<td>compilation tool to migrate legacy programs to CPU/CIM heterogeneous architectures; a model to quantify the performance gain</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>DAC</td>
<td>CAS</td>
<td>PIMCOMP: A Universal Compilation Framework for Crossbar-based PIM DNN Accelerators</td>
<td>compiler based on Crossbar/IMA/Tile/Chip hierarchy; low latency and high throughput mode; genetic algorithm to optimize weight replication and core mapping; scheduling algorithms for complex DNN</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>ASPLOS</td>
<td>CAS</td>
<td>CIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators</td>
<td>compilation stack for various CIM accelerators; multi-level DNN scheduling approach</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-float-point-processing">RRAM CiM: Float-Point processing<a class="headerlink" href="#rram-cim-float-point-processing" title="Permanent link">&para;</a></h4>
<p>Challenge: Raw RRAM devices are not suitable for floating-point operations; while floating point data is common in DNNs (e.g. FP32).</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>SC</td>
<td>UCLA</td>
<td>ReFloat: Low-Cost Floating-Point Processing in ReRAM for Accelerating Iterative Linear Solvers</td>
<td>data format and accelerator architecture</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>DATE</td>
<td>UESTC</td>
<td>AFPR-CIM: An Analog-Domain Floating-Point RRAM -based Compute- In- Memory Architecture with Dynamic Range Adaptive FP-ADC</td>
<td>all-analog domain CIM architecture for FP8 calculations; adaptive dynamic range FP-ADC &amp; FP-DAC</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>GWU</td>
<td>A Hybrid-Domain Floating-Point Compute-in-Memory Architecture for Efficient Acceleration of High-Precision Deep Neural Networks</td>
<td>SRAM based hybrid-domain FP CIM architecture; detailed circuit schematics and physical layouts</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-convolutional-layer">RRAM CiM: Convolutional Layer<a class="headerlink" href="#rram-cim-convolutional-layer" title="Permanent link">&para;</a></h4>
<p>Challenge: Convolutional layer is the most compute-intensive layer in CNNs. RRAM CIM architecture is quite suitable for convolutional layer operations but face challenges related to non-ideal effects and performance degradation.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2020</td>
<td>Nature</td>
<td>THU</td>
<td>Fully hardware-implemented memristor convolutional neural network</td>
<td>fabrication of high-yield, high-performance and uniform memristor crossbar arrays; hybrid-training method; replication of multiple identical kernels for processing different inputs in parallel</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2019</td>
<td>TED</td>
<td>PKU</td>
<td>Convolutional Neural Networks Based on RRAM Devices for Image Recognition and Online Learning Tasks</td>
<td>RRAM-based hardware implementation of CNN; expand kernel to the size of image</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>TVLSI</td>
<td>NBU</td>
<td>A 578-TOPS/W RRAM-Based Binary Convolutional Neural Network Macro for Tiny AI Edge Devices</td>
<td>ReRAM XNOR cell; BCNN CIM macro with FPGA as the control core</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h5 id="rram-cim-mapping-for-cnn">RRAM CiM: Mapping for CNN<a class="headerlink" href="#rram-cim-mapping-for-cnn" title="Permanent link">&para;</a></h5>
<p>Challenge: Efficient mapping of CNN layers onto RRAM CIM architecture is crucial for performance.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2020</td>
<td>TCAS-I</td>
<td>Georgia Tech</td>
<td>Optimizing Weight Mapping and Data Flow for Convolutional Neural Networks on Processing-in-Memory Architectures</td>
<td>weight mapping to avoid multiple access to input; pipeline architecture for conv layer calculation</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2021</td>
<td>TCAD</td>
<td>SJTU</td>
<td>Efficient and Robust RRAM-Based Convolutional Weight Mapping With Shifted and Duplicated Kernel</td>
<td>shift and duplicate kernel (SDK) convolutional weight mapping architecture; parallel-window size allocation algorithm; kernel synchronization method</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>VLSI-SoC</td>
<td>Aachen</td>
<td>Mapping of CNNs on multi-core RRAM-based CIM architectures</td>
<td>architecture optimized for communication; compiler algorithms for conv2D layer; cycle-accurate simulator</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>TODAES</td>
<td>UCAS</td>
<td>Mathematical Framework for Optimizing Crossbar Allocation for ReRAM-based CNN Accelerators</td>
<td>formulate a crossbar allocation problem for ReRAM-based CNN accelerators; dynamic programming based solver; models the performance considering allocation problem</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>IEEE Access</td>
<td>UTehran</td>
<td>SCiMA: A Systolic CiM-Based Accelerator With a New Weight Mapping for CNNs—A Virtual Framework Approach</td>
<td>kernel-major inter-crossbar weight mapping (KM-InterCWM) for convolution layers; structured pruning techniques; system-level virtual framework</td>
<td>4</td>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-transformer-accelerator">RRAM CIM: Transformer Accelerator<a class="headerlink" href="#rram-cim-transformer-accelerator" title="Permanent link">&para;</a></h4>
<p>Challenge: RRAM's cross-bar architecture is suitable for matrix operations.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>VLSI</td>
<td>Purdue</td>
<td>X-Former: In-Memory Acceleration of Transformers</td>
<td>in-memory accelerate attention layers; intralayer sequence blocking dataflow; provides a simulator</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>TODAES</td>
<td>HUST</td>
<td>A Cascaded ReRAM-based Crossbar Architecture for Transformer Neural Network Acceleration</td>
<td>cascaded crossbar arrays that uses transimpedance amplifiers; data mapping scheme to store signed operands; ADC virtualization scheme</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>VLSI</td>
<td>HUST</td>
<td>An RRAM-Based Computing-in-Memory Architecture and Its Application in Accelerating Transformer Inference</td>
<td>RRAM-based in-memory floating-point computation architecture (RIME); pipelined implementations of MatMul and softmax</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2020</td>
<td>ICCAD</td>
<td>Duke</td>
<td>ReTransformer: ReRAM-based processing-in-memory architecture for transformer acceleration</td>
<td>MatMul does matrix decomposition in scaled dot-product attention; in-memory logic techniques for softmax; sub-matrix pipeline</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2022</td>
<td>TCAD</td>
<td>KAIST</td>
<td>A Framework for Accelerating Transformer-Based Language Model on ReRAM-Based Architecture</td>
<td>window self-attention and window-size search algorithm; ReRAM hardware design optimized for this algorithm</td>
<td>4</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>2020</td>
<td>ICCD</td>
<td>LSU</td>
<td>ATT: A Fault-Tolerant ReRAM Accelerator for Attention-based Neural Networks</td>
<td>ReRAM-based accelerator with pipeline for AttNNs; heuristic redundancy algorithm</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>UCSD</td>
<td>Hybrid SLC-MLC RRAM Mixed-Signal Processing-in-Memory Architecture for Transformer Acceleration via Gradient Redistribution</td>
<td>architectural and circuit-level hardware designs supporting importance-based data flow with hybrid SLC-MLC ReRAM; gradient redistribution technique</td>
<td>3</td>
<td>2</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-special-usage">RRAM CiM: Special Usage<a class="headerlink" href="#rram-cim-special-usage" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>GLSVLSI</td>
<td>Yale</td>
<td>Examining the Role and Limits of Batchnorm Optimization to Mitigate Diverse Hardware-noise in In-memory Computing</td>
<td>non-idealities; circuit-level parasitic resistances and device-level non-idealities; crossbar-aware fine-tuning of batchnorm parameters</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2019</td>
<td>ASPDAC</td>
<td>POSTECH</td>
<td>In-memory batch-normalization for resistive memory based binary neural network hardware</td>
<td>in-memory batchnormalization schemes; integrate BN layers on crossbar</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>TRETS</td>
<td>UFRGS</td>
<td>Reprogrammable Non-Linear Circuits Using ReRAM for NN Accelerators</td>
<td>perform typical non-linear operations using ReRAM</td>
<td>4</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2019</td>
<td>Adv. Funct. Mater.</td>
<td>HUST</td>
<td>Functional Demonstration of a Memristive Arithmetic Logic Unit (MemALU) for In‐Memory Computing</td>
<td>non-volatile Boolean logic using RRAM crossbar;reconfigurable boolean logic gates</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-matrix-equation-solver">RRAM CiM: Matrix Equation Solver<a class="headerlink" href="#rram-cim-matrix-equation-solver" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>DATE</td>
<td>PKU</td>
<td>BlockAMC: Scalable In-Memory Analog Matrix Computing for Solving Linear Systems</td>
<td>Novel scalable algorithm for matrix equation solving; reconfigurable BlockAMC macros design</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>Sci.Adv.</td>
<td>HUST</td>
<td>Fully analog iteration for solving matrix equations with in-memory computing</td>
<td>Analog Iteration with Digital Refinement solver</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>Nat.Elec.</td>
<td>PKU</td>
<td>Precise and scalable analogue matrix equation solving using resistive random-access memory chips</td>
<td>Mixed-Precision Iterative Algorithm for High-Precision Analogue Computing; Scalable Hardware Implementation with BlockAMC algorithm</td>
<td>3</td>
<td>5</td>
<td>4</td>
</tr>
</tbody>
</table>
<h3 id="cim-hybrid-architecture">CIM: Hybrid Architecture<a class="headerlink" href="#cim-hybrid-architecture" title="Permanent link">&para;</a></h3>
<p>Solution: Use hybrid architecture (like SRAM + RRAM) to overcome the limitations of single device (e.g. RRAM's non-ideal effects).</p>
<h4 id="hybrid-cim-sram-general-logic">Hybrid CIM: SRAM + General Logic<a class="headerlink" href="#hybrid-cim-sram-general-logic" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>GLSVLSI</td>
<td>USC</td>
<td>Heterogeneous Integration of In-Memory Analog Computing Architectures with Tensor Processing Units</td>
<td>hybrid TPU-IMAC architecture; TPU for conv, CIM for fc</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>ASPLOS</td>
<td>CAS</td>
<td>PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System</td>
<td>dynamic parallelism-aware task scheduling for llm decoding; online kernel characterization for heterogeneous architectures; hybrid PIM units for compute-bound and memory-bound kernels</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="hybrid-cim-sram-rram">Hybrid CIM: SRAM + RRAM<a class="headerlink" href="#hybrid-cim-sram-rram" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>Science</td>
<td>NTHU</td>
<td>Fusion of memristor and digital compute-in-memory processing for energy-efficient edge computing</td>
<td>Fusion of ReRAM and SRAM CiM; ReRAM SLC &amp; MLC Hybrid; Current quantization; Weight shifting with compensation</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>IPDPS</td>
<td>Georgia Tech</td>
<td>Harmonica: Hybrid Accelerator to Overcome Imperfections of Mixed-signal DNN Accelerators</td>
<td>select and transfer imperfectionsensitive weights to digital accelerator; hybrid quantization(weights on analog part is more quantized)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>ICCAD</td>
<td>SJTU</td>
<td>TL-nvSRAM-CIM: Ultra-High-Density Three-Level ReRAM-Assisted Computing-in-nvSRAM with DC-Power Free Restore and Ternary MAC Operations</td>
<td>DCpower-free weight-restore from ReRAM; ternary SRAM-CIM mechanism with differential computing scheme</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="hybrid-cim-memristormram-sram">Hybrid CIM: Memristor/MRAM + SRAM<a class="headerlink" href="#hybrid-cim-memristormram-sram" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>Nature</td>
<td>TSMC</td>
<td>A mixed-precision memristor and SRAM compute-in-memory AI processor</td>
<td>layer based INT-FP hybrid architure; kernel-based mix-CIM (SRAM/ReRAM/digital hybrid architecture)</td>
<td>5</td>
<td>5</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>DAC</td>
<td>Chung-Ang Univ.</td>
<td>HH-PIM: Dynamic Optimization of Power and Performance with Heterogeneous-Hybrid PIM for Edge AI Devices</td>
<td>heterogeneous-hybrid PIM with HP/LP modules and MRAM/SRAM; dynamic data placement algorithm for energy optimization; dual PIM controller design</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>AaltoU</td>
<td>Acore-CIM: build accurate and reliable mixed-signal CIM cores with RISC-V controlled self-calibration</td>
<td>reliability-focused MAC cell; proof-of-concept SoC composed of a CIM core and a RISC-V control processor; automated Built-In Self-Calibration (BISC) routine</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="hybrid-cim-analog-digital">Hybrid CIM: Analog + Digital<a class="headerlink" href="#hybrid-cim-analog-digital" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>arXiv</td>
<td>HP</td>
<td>RACE-IT: A Reconfigurable Analog CAM-Crossbar Engine for In-Memory Transformer Acceleration</td>
<td>Compute Analog Content Addressable Memory (Compute-ACAM) structure; accelerator based on crossbars and Compute-ACAMs; encoding-based optimization</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2024</td>
<td>VLSI</td>
<td>FDU</td>
<td>HARDSEA: Hybrid Analog-ReRAM Clustering and Digital-SRAM In-Memory Computing Accelerator for Dynamic Sparse Self-Attention in Transformer</td>
<td>product-quantization-based sparse self-attention algorithm; ADC-free ReRAM-CIM macro; ReRAM-CIM for front-end attention sparsification, SRAM-CIM for back-end sparse attention</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>ASP-DAC</td>
<td>Keio</td>
<td>OSA-HCIM: On-The-Fly Saliency-Aware Hybrid SRAM CIM with Dynamic Precision Configuration</td>
<td>On-the-fly Saliency-Aware precision configuration scheme; Hybrid CIM Array for DCIM and ACIM using split-port SRAM</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>South Carolina</td>
<td>PIM-LLM: A High-Throughput Hybrid PIM Architecture for 1-bit LLMs</td>
<td>hybrid PIM-Digital architecture; analog PIM for low-precision MatMul; digital systolic array for high-precision matMul</td>
<td>4</td>
<td>3</td>
<td>1</td>
</tr>
<tr>
<td>2024</td>
<td>ESSERC</td>
<td>UCSD</td>
<td>An Analog and Digital Hybrid Attention Accelerator for Transformers with Charge-based In-memory Computing</td>
<td>analog CIM for low-score tokens, digital processor for high</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="cim-quantization">CIM: Quantization<a class="headerlink" href="#cim-quantization" title="Permanent link">&para;</a></h3>
<p>Challenge: Limited by the precision &amp; area &amp; power trade-off of the ADC; certain CIM devices like RRAM are not suitable for high-precision computation (e.g. FP32). Quantization is needed to reduce the precision of the data.</p>
<h4 id="cim-quantization-for-analog-cim">CIM Quantization: For Analog CIM<a class="headerlink" href="#cim-quantization-for-analog-cim" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>ISLPED</td>
<td>Purdue</td>
<td>Partial-Sum Quantization for Near ADC-Less Compute-In-Memory Accelerators</td>
<td>ADC-Less and near ADC-Less CiM accelerators; CiM hardware aware DNN quantization methodology</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>AICAS</td>
<td>TU Delft</td>
<td>Mapping-aware Biased Training for Accurate Memristor-based Neural Networks</td>
<td>favorability constraint analysis to find important weight values; mapping-aware biased training to restrict weight values to low variance RRAM states</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>TCAD</td>
<td>BUAA</td>
<td>CIMQ: A Hardware-Efficient Quantization Framework for Computing-In-Memory-Based Neural Network Accelerators</td>
<td>bit-level sparsity induced activation quantization; quantizing partial sums to decrease required resolution of ADCs; arraywise quantization granularity</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>TCAD</td>
<td>BUAA</td>
<td>CIM²PQ: An Arraywise and Hardware-Friendly Mixed Precision Quantization Method for Analog Computing-In-Memory</td>
<td>mixed precision quantization method based on evolutionary algorithm; arraywise quantization granularity; evaluation method to obtain the performance of strategy on the CIM</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>ICCAD</td>
<td>TU Delft</td>
<td>Hardware-Aware Quantization for Accurate Memristor-Based Neural Networks</td>
<td>analysis of fixed-point quantization impact on conductance variation; weight quantization tuning technique; approach to reduce the residual error</td>
<td>3</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="cim-quantization-for-all-cim">CIM Quantization: For all CIM<a class="headerlink" href="#cim-quantization-for-all-cim" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2018</td>
<td>CVPR</td>
<td>Google</td>
<td>Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</td>
<td>integer-only inference arithmetic; quantizes both weights and activations as 8-bit integers, bias 32-bit; provides both quantized inference framework and training frame work</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>ICCD</td>
<td>SJTU</td>
<td>PSQ: An Automatic Search Framework for Data-Free Quantization on PIM-based Architecture</td>
<td>post-training quantization framework without retraining; hardware-aware block reassembly</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UHK</td>
<td>Binary Weight Multi-Bit Activation Quantization for Compute-in-Memory CNN Accelerators</td>
<td>a quantization framework that considers CIM's mixed-signal constraints; closed-form layer-specific weight binarization method; differentiable function for uniform multi-bit quantization</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="cim-digital-cim">CIM: Digital CIM<a class="headerlink" href="#cim-digital-cim" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>ISCAS</td>
<td>CAS</td>
<td>StreamDCIM: A Tile-based Streaming Digital CIM Accelerator with Mixed-stationary Cross-forwarding Dataflow for Multimodal Transformer</td>
<td>tile-based reconfigurable CIM macro microarchitecture; mixed-stationary cross-forwarding dataflow; ping-pong-like finegrained compute-rewriting pipeline</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="nvm">NVM<a class="headerlink" href="#nvm" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2020</td>
<td>GLSVLSI</td>
<td>UND</td>
<td>Benchmarking Computing-in-Memory for Design Space Exploration</td>
<td>uniform benchmarking of CiM designs based on different memory technologies</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>ISCAS</td>
<td>UMCP</td>
<td>On-Chip Adaptation for Reducing Mismatch in Analog Non-Volatile Device Based Neural Networks</td>
<td>float-gate transistors based; hot-electron injection to address the issue of mismatch and variation</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>DATE</td>
<td>UniBo</td>
<td>End-to-End DNN Inference on a Massively Parallel Analog In Memory Computing Architecture</td>
<td>many-core heterogeneous architecture; general-purpose system based on RISC-V cores and nvAIMC cores; based on Phase-Change Memory(PCM);</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="October 27, 2025 11:28:37 UTC">October 27, 2025</span>
  </span>

    
    
    
    
  </aside>


  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.instant.progress", "navigation.tracking", "navigation.sections", "navigation.expand", "navigation.indexes", "toc.follow", "navigation.top", "content.action.edit", "content.action.view", "search.highlight", "search.share", "search.suggest"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>