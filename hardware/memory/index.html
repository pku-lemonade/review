
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Youwei Zhuo">
      
      
        <link rel="canonical" href="https://review.youwei.xyz/hardware/memory/">
      
      
        <link rel="prev" href="../parallel/">
      
      
        <link rel="next" href="../accelerators/">
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Memory Architecture - New Lemonade Review</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-JXHKVQ8LKS"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-JXHKVQ8LKS",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-JXHKVQ8LKS",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#memory-architecture" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="New Lemonade Review" class="md-header__button md-logo" aria-label="New Lemonade Review" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M368 80c-3.2 0-6.2.4-8.9 1.3C340 86.8 313 92 284.8 84.6c-57.4-14.9-124.6 7.4-174.7 57.5S37.7 259.4 52.6 316.8c7.3 28.2 2.2 55.2-3.3 74.3-.8 2.8-1.3 5.8-1.3 8.9 0 17.7 14.3 32 32 32 3.2 0 6.2-.4 8.9-1.3 19.1-5.5 46.1-10.7 74.3-3.3 57.4 14.9 124.6-7.4 174.7-57.5s72.4-117.3 57.5-174.7c-7.3-28.2-2.2-55.2 3.3-74.3.8-2.8 1.3-5.8 1.3-8.9 0-17.7-14.3-32-32-32m0-48c44.2 0 80 35.8 80 80 0 7.7-1.1 15.2-3.1 22.3-4.6 15.8-7.1 32.9-3 48.9 20.1 77.6-10.9 161.5-70 220.7s-143.1 90.2-220.7 70c-16-4.1-33-1.6-48.9 3-7.1 2-14.6 3.1-22.3 3.1-44.2 0-80-35.8-80-80 0-7.7 1.1-15.2 3.1-22.3 4.6-15.8 7.1-32.9 3-48.9-20.1-77.6 10.9-161.5 70-220.7s143.2-90.1 220.7-70c16 4.1 33 1.6 48.9-3 7.1-2 14.6-3.1 22.3-3.1M246.7 167c-52 15.2-96.5 59.7-111.7 111.7-3.7 12.7-17.1 20-29.8 16.3s-20-17-16.2-29.7c19.8-67.7 76.6-124.5 144.3-144.3 12.7-3.7 26.1 3.6 29.8 16.3s-3.6 26.1-16.3 29.8z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            New Lemonade Review
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Memory Architecture
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/pku-lemonade/review" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pku-lemonade/review
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="New Lemonade Review" class="md-nav__button md-logo" aria-label="New Lemonade Review" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M368 80c-3.2 0-6.2.4-8.9 1.3C340 86.8 313 92 284.8 84.6c-57.4-14.9-124.6 7.4-174.7 57.5S37.7 259.4 52.6 316.8c7.3 28.2 2.2 55.2-3.3 74.3-.8 2.8-1.3 5.8-1.3 8.9 0 17.7 14.3 32 32 32 3.2 0 6.2-.4 8.9-1.3 19.1-5.5 46.1-10.7 74.3-3.3 57.4 14.9 124.6-7.4 174.7-57.5s72.4-117.3 57.5-174.7c-7.3-28.2-2.2-55.2 3.3-74.3.8-2.8 1.3-5.8 1.3-8.9 0-17.7-14.3-32-32-32m0-48c44.2 0 80 35.8 80 80 0 7.7-1.1 15.2-3.1 22.3-4.6 15.8-7.1 32.9-3 48.9 20.1 77.6-10.9 161.5-70 220.7s-143.1 90.2-220.7 70c-16-4.1-33-1.6-48.9 3-7.1 2-14.6 3.1-22.3 3.1-44.2 0-80-35.8-80-80 0-7.7 1.1-15.2 3.1-22.3 4.6-15.8 7.1-32.9 3-48.9-20.1-77.6 10.9-161.5 70-220.7s143.2-90.1 220.7-70c16 4.1 33 1.6 48.9-3 7.1-2 14.6-3.1 22.3-3.1M246.7 167c-52 15.2-96.5 59.7-111.7 111.7-3.7 12.7-17.1 20-29.8 16.3s-20-17-16.2-29.7c19.8-67.7 76.6-124.5 144.3-144.3 12.7-3.7 26.1 3.6 29.8 16.3s-3.6 26.1-16.3 29.8z"/></svg>

    </a>
    New Lemonade Review
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/pku-lemonade/review" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pku-lemonade/review
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    README
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    Hardware
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Hardware
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../processor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Processor Architecture
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../parallel/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Parallel and Multi-Processor Architecture
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Memory Architecture
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Memory Architecture
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#in-stroage-nand-flash-processing" class="md-nav__link">
    <span class="md-ellipsis">
      
        In-Stroage (NAND Flash) Processing
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="In-Stroage (NAND Flash) Processing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-application-targeted-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        General Application Targeted Optimization
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="General Application Targeted Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llm-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM-Specific Optimization
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dimm-pims" class="md-nav__link">
    <span class="md-ellipsis">
      
        DIMM-PIMs
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DIMM-PIMs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-application-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        General Application-Specific Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dnn-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        DNN-Specific Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#graph-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Graph-Specific Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#moe-llm-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        MoE-LLM Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rag-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        RAG-Specific Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-address-space" class="md-nav__link">
    <span class="md-ellipsis">
      
        Memory Address Space
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-allocation-management" class="md-nav__link">
    <span class="md-ellipsis">
      
        Memory Allocation &amp; Management
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pim-compiler-data-layout" class="md-nav__link">
    <span class="md-ellipsis">
      
        PIM Compiler &amp; Data Layout
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluation-simulators" class="md-nav__link">
    <span class="md-ellipsis">
      
        Evaluation &amp; Simulators
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intra-dimm-communication" class="md-nav__link">
    <span class="md-ellipsis">
      
        Intra-DIMM Communication
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inter-dimm-communication" class="md-nav__link">
    <span class="md-ellipsis">
      
        Inter-DIMM Communication
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cpu-pim-heterogeneous-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        CPU-PIM Heterogeneous Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#npu-pim-heterogeneous-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        NPU-PIM Heterogeneous Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-pim-heterogeneous-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPU-PIM Heterogeneous Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizations-on-upmem-pim" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optimizations on UPMEM-PIM
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#in-cache-computing" class="md-nav__link">
    <span class="md-ellipsis">
      
        In-Cache-Computing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pim-ndp-benchmarks" class="md-nav__link">
    <span class="md-ellipsis">
      
        PIM &amp; NDP Benchmarks
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PIM &amp; NDP Benchmarks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#benchmarks-for-conventional-computing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Benchmarks for Conventional Computing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#benchmarks-for-quantum-computing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Benchmarks for Quantum Computing
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cxl-based-pim" class="md-nav__link">
    <span class="md-ellipsis">
      
        CXL-Based PIM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chiplet-based-pim" class="md-nav__link">
    <span class="md-ellipsis">
      
        Chiplet-Based PIM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-stacked-pim" class="md-nav__link">
    <span class="md-ellipsis">
      
        3D-Stacked PIM
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3D-Stacked PIM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hybrid-bounding-based-pim" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hybrid Bounding-Based PIM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hmc-or-hmc-like-pim" class="md-nav__link">
    <span class="md-ellipsis">
      
        HMC or HMC-like PIM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hbm2-pim" class="md-nav__link">
    <span class="md-ellipsis">
      
        HBM2-PIM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hbm3-pim" class="md-nav__link">
    <span class="md-ellipsis">
      
        HBM3-PIM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#benchmarks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Benchmarks
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pim-heterogeneous-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        PIM: Heterogeneous Architecture
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#general-cim" class="md-nav__link">
    <span class="md-ellipsis">
      
        General CiM
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="General CiM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#specific-application-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Specific Application &amp; Algorithm
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modeling-simulation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Modeling &amp; Simulation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compiler" class="md-nav__link">
    <span class="md-ellipsis">
      
        Compiler
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-dram" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM: DRAM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-sram" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM: SRAM
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CIM: SRAM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sram-cim-general-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        SRAM CIM: General Architecture
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-reconfigurable-macro" class="md-nav__link">
    <span class="md-ellipsis">
      
        SRAM CIM: Reconfigurable Macro
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-specific-use-or-application" class="md-nav__link">
    <span class="md-ellipsis">
      
        SRAM CIM: Specific Use or Application
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-hardware-software-co-design" class="md-nav__link">
    <span class="md-ellipsis">
      
        SRAM CIM: Hardware-Software Co-Design
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-simulator-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      
        SRAM CIM: Simulator &amp; Modeling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-transformer-accelerator" class="md-nav__link">
    <span class="md-ellipsis">
      
        SRAM CIM: Transformer Accelerator
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-rram" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM: RRAM
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CIM: RRAM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rram-cim-simulator" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Simulator
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Architecture
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-architecture-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Architecture optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-design-space-exploration" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Design Space Exploration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Modeling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-training-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Training optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-float-point-processing" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Float-Point processing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-convolutional-layer" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Convolutional Layer
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="RRAM CiM: Convolutional Layer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rram-cim-mapping-for-cnn" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Mapping for CNN
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-transformer-accelerator" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CIM: Transformer Accelerator
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-special-usage" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Special Usage
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-matrix-equation-solver" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Matrix Equation Solver
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-hybrid-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM: Hybrid Architecture
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CIM: Hybrid Architecture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hybrid-cim-sram-general-logic" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hybrid CIM: SRAM + General Logic
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hybrid-cim-sram-rram" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hybrid CIM: SRAM + RRAM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hybrid-cim-memristormram-sram" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hybrid CIM: Memristor/MRAM + SRAM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hybrid-cim-analog-digital" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hybrid CIM: Analog + Digital
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM: Quantization
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CIM: Quantization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cim-quantization-partial-sum-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM: Quantization: Partial Sum Quantization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-quantization-for-analog-cim" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM Quantization: For Analog CIM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-quantization-for-all-cim" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM Quantization: For all CIM
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-digital-cim" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM: Digital CIM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nvm" class="md-nav__link">
    <span class="md-ellipsis">
      
        NVM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prefetching" class="md-nav__link">
    <span class="md-ellipsis">
      
        Prefetching
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../accelerators/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Domain-Specific Accelerators
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../network/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Interconnection Networks
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../eda/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Electronic Design Automation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../fail/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Security and Reliability
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../emerging/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Emerging Technologies
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../perf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Performance Analysis (to be deleted)
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../software/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    Software
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Software
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/algorithm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Algorithms
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/pl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Programming Languages
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/os/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Operating Systems
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/ds/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Distributed Systems
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/hpc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    High-Performance Computing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software/perf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Performance Analysis
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#in-stroage-nand-flash-processing" class="md-nav__link">
    <span class="md-ellipsis">
      
        In-Stroage (NAND Flash) Processing
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="In-Stroage (NAND Flash) Processing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-application-targeted-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        General Application Targeted Optimization
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="General Application Targeted Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llm-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM-Specific Optimization
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dimm-pims" class="md-nav__link">
    <span class="md-ellipsis">
      
        DIMM-PIMs
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DIMM-PIMs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-application-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        General Application-Specific Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dnn-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        DNN-Specific Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#graph-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Graph-Specific Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#moe-llm-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        MoE-LLM Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rag-specific-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        RAG-Specific Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-address-space" class="md-nav__link">
    <span class="md-ellipsis">
      
        Memory Address Space
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-allocation-management" class="md-nav__link">
    <span class="md-ellipsis">
      
        Memory Allocation &amp; Management
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pim-compiler-data-layout" class="md-nav__link">
    <span class="md-ellipsis">
      
        PIM Compiler &amp; Data Layout
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluation-simulators" class="md-nav__link">
    <span class="md-ellipsis">
      
        Evaluation &amp; Simulators
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intra-dimm-communication" class="md-nav__link">
    <span class="md-ellipsis">
      
        Intra-DIMM Communication
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inter-dimm-communication" class="md-nav__link">
    <span class="md-ellipsis">
      
        Inter-DIMM Communication
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cpu-pim-heterogeneous-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        CPU-PIM Heterogeneous Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#npu-pim-heterogeneous-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        NPU-PIM Heterogeneous Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-pim-heterogeneous-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPU-PIM Heterogeneous Systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizations-on-upmem-pim" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optimizations on UPMEM-PIM
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#in-cache-computing" class="md-nav__link">
    <span class="md-ellipsis">
      
        In-Cache-Computing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pim-ndp-benchmarks" class="md-nav__link">
    <span class="md-ellipsis">
      
        PIM &amp; NDP Benchmarks
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PIM &amp; NDP Benchmarks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#benchmarks-for-conventional-computing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Benchmarks for Conventional Computing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#benchmarks-for-quantum-computing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Benchmarks for Quantum Computing
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cxl-based-pim" class="md-nav__link">
    <span class="md-ellipsis">
      
        CXL-Based PIM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chiplet-based-pim" class="md-nav__link">
    <span class="md-ellipsis">
      
        Chiplet-Based PIM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-stacked-pim" class="md-nav__link">
    <span class="md-ellipsis">
      
        3D-Stacked PIM
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3D-Stacked PIM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hybrid-bounding-based-pim" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hybrid Bounding-Based PIM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hmc-or-hmc-like-pim" class="md-nav__link">
    <span class="md-ellipsis">
      
        HMC or HMC-like PIM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hbm2-pim" class="md-nav__link">
    <span class="md-ellipsis">
      
        HBM2-PIM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hbm3-pim" class="md-nav__link">
    <span class="md-ellipsis">
      
        HBM3-PIM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#benchmarks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Benchmarks
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pim-heterogeneous-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        PIM: Heterogeneous Architecture
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#general-cim" class="md-nav__link">
    <span class="md-ellipsis">
      
        General CiM
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="General CiM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#specific-application-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Specific Application &amp; Algorithm
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modeling-simulation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Modeling &amp; Simulation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compiler" class="md-nav__link">
    <span class="md-ellipsis">
      
        Compiler
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-dram" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM: DRAM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-sram" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM: SRAM
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CIM: SRAM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sram-cim-general-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        SRAM CIM: General Architecture
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-reconfigurable-macro" class="md-nav__link">
    <span class="md-ellipsis">
      
        SRAM CIM: Reconfigurable Macro
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-specific-use-or-application" class="md-nav__link">
    <span class="md-ellipsis">
      
        SRAM CIM: Specific Use or Application
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-hardware-software-co-design" class="md-nav__link">
    <span class="md-ellipsis">
      
        SRAM CIM: Hardware-Software Co-Design
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-simulator-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      
        SRAM CIM: Simulator &amp; Modeling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sram-cim-transformer-accelerator" class="md-nav__link">
    <span class="md-ellipsis">
      
        SRAM CIM: Transformer Accelerator
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-rram" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM: RRAM
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CIM: RRAM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rram-cim-simulator" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Simulator
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Architecture
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-architecture-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Architecture optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-design-space-exploration" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Design Space Exploration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Modeling
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-training-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Training optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-float-point-processing" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Float-Point processing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-convolutional-layer" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Convolutional Layer
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="RRAM CiM: Convolutional Layer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rram-cim-mapping-for-cnn" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Mapping for CNN
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-transformer-accelerator" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CIM: Transformer Accelerator
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-special-usage" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Special Usage
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rram-cim-matrix-equation-solver" class="md-nav__link">
    <span class="md-ellipsis">
      
        RRAM CiM: Matrix Equation Solver
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-hybrid-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM: Hybrid Architecture
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CIM: Hybrid Architecture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hybrid-cim-sram-general-logic" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hybrid CIM: SRAM + General Logic
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hybrid-cim-sram-rram" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hybrid CIM: SRAM + RRAM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hybrid-cim-memristormram-sram" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hybrid CIM: Memristor/MRAM + SRAM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hybrid-cim-analog-digital" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hybrid CIM: Analog + Digital
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM: Quantization
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CIM: Quantization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cim-quantization-partial-sum-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM: Quantization: Partial Sum Quantization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-quantization-for-analog-cim" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM Quantization: For Analog CIM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-quantization-for-all-cim" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM Quantization: For all CIM
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cim-digital-cim" class="md-nav__link">
    <span class="md-ellipsis">
      
        CIM: Digital CIM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nvm" class="md-nav__link">
    <span class="md-ellipsis">
      
        NVM
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prefetching" class="md-nav__link">
    <span class="md-ellipsis">
      
        Prefetching
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/pku-lemonade/review/edit/main/docs/hardware/memory.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/pku-lemonade/review/raw/main/docs/hardware/memory.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<h1 id="memory-architecture">Memory Architecture<a class="headerlink" href="#memory-architecture" title="Permanent link">&para;</a></h1>
<h2 id="in-stroage-nand-flash-processing">In-Stroage (NAND Flash) Processing<a class="headerlink" href="#in-stroage-nand-flash-processing" title="Permanent link">&para;</a></h2>
<h3 id="general-application-targeted-optimization">General Application Targeted Optimization<a class="headerlink" href="#general-application-targeted-optimization" title="Permanent link">&para;</a></h3>
<p>Solution: Intergrate the compute unit into the SSD controller to process the capacity-sensitive applications.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>HPCA</td>
<td>UCLA</td>
<td>BeaconGNN: Large-Scale GNN Acceleration with Out-of-Order Streaming In-Storage Computing</td>
<td>DirectGraph format for out-of-order sampling; die-level processing units; channel-level command router</td>
<td>4</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>ETHZ</td>
<td>REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing</td>
<td>In-Storage processing</td>
<td>2</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>UCSD</td>
<td>In-Storage Acceleration of Retrieval Augmented Generation as a Service</td>
<td>metamorphic in-storage accelerator; Metadata Navigation Unit for dynamic data access</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arxiv</td>
<td>ETHZ</td>
<td>MARS: Processing-In-Memory Acceleration of Raw Signal Genome Analysis Inside the Storage Subsystem</td>
<td>PIM module inside the SSD controller; early signal quantization; read filtering</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>ICT</td>
<td>Cambricon-LLM: A Chiplet-Based Hybrid Architecture for On-Device Inference of 70B LLM</td>
<td>chiplet-based NPU &amp; NAND flash hybrid architecture; Hardware-aware tiling for NPU-flash workload distribution</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="llm-specific-optimization">LLM-Specific Optimization<a class="headerlink" href="#llm-specific-optimization" title="Permanent link">&para;</a></h4>
<p>Solution: Store weights in flash memory as read-only to prevent failures caused by write operations.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>Seoul National</td>
<td>AiF: Accelerating On-Device LLM Inference Using In-Flash Processing</td>
<td>in-flash GEMV computation; charge-recycling read to skip precharge/discharge steps in flash memory</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>THU</td>
<td>Lincoln: Real-Time 50~100B LLM Inference on Consumer Devices with LPDDR-Interfaced, Compute-Enabled Flash Memory</td>
<td>flash-on-LPDDR-interface for prefill phase; hybrid-bonding-based near-Flash computing for generation phase</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>PKU</td>
<td>InstAttention: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference</td>
<td>offloading decoding-phase attention computation to computational SSDs; SparF Attention flash-aware sparse algorithm</td>
<td>4</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Korea Univ.</td>
<td>Dissecting and Re-architecting 3D NAND Flash PIM Arrays for Efficient Single-Batch Token Generation in LLMs</td>
<td>re-architected 3D-NAND PIM array with H-tree network; QLC-SLC hybrid architecture for KV caching; static/dynamic MVM tiling and mapping</td>
<td>4</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="dimm-pims">DIMM-PIMs<a class="headerlink" href="#dimm-pims" title="Permanent link">&para;</a></h3>
<p>Challenge: Memory wall causing high latency of data transfer between CPU and memory.</p>
<p>Solution: Put the compute unit in the memory or near the memory to reduce the data transfer overhead.</p>
<h4 id="general-application-specific-optimization">General Application-Specific Optimization<a class="headerlink" href="#general-application-specific-optimization" title="Permanent link">&para;</a></h4>
<p>Challenge: Existing NDP architecture are designed for general-purpose computing; not efficient for specific tasks like graph processing.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022</td>
<td>ISCA</td>
<td>Micron</td>
<td>To PIM or Not for Emerging General Purpose Processing in DDR Memory Systems</td>
<td>vector engine inside NDP bank; intelligent code offload decision</td>
<td>2</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2023</td>
<td>EuroSys</td>
<td>Univ. of Virginia</td>
<td>NearPM: A Near-Data Processing System for Storage-Class Applications</td>
<td>partitioned persist ordering for asynchronous CPU-NDP execution; delayed synchronization for multi-device consistency</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>ISCA</td>
<td>Samsung</td>
<td>pSyncPIM: Partially Synchronous Execution of Sparse Matrix Operations for All-Bank PIM Architectures</td>
<td>partially synchronous PIM control; predicated execution; sparse matrix distribution &amp; compaction</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>ATC</td>
<td>RUC</td>
<td>Turbocharge ANNS on Real Processing-in-Memory by Enabling Fine-Grained Per-PIM-Core Scheduling</td>
<td>per-PU scheduling; persistent PIM kernel; per-PU dispatching with selective replication</td>
<td>3</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>UC Davis</td>
<td>NOVA: A Novel Vertex Management Architecture for Scalable Graph Processing</td>
<td>message-driven processors capable of executing algorithms; a direct-mapped cache with a write-back policy; support both asynchronous and bulk synchronous parallel execution models</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="dnn-specific-optimization">DNN-Specific Optimization<a class="headerlink" href="#dnn-specific-optimization" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021</td>
<td>HPCA</td>
<td>Seoul National</td>
<td>GradPIM: A Practical Processing-in-DRAM Architecture for Gradient Descent</td>
<td>fixed-function PIM architecture for DNN gradient descent; non-invasive PIM operations using reserved DDR commands</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>ASPLOS</td>
<td>PKU</td>
<td>PIM-DL: Expanding the Applicability of Commodity DRAM-PIMs for Deep Learning via Algorithm-System Co-Optimization</td>
<td>algorithm for DNN to look-up-table conversion; auto-tuner for optimizing LUT-NN mapping on DRAM-PIMs</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>National Tech Univ. of Athens</td>
<td>PIMfused: Near-Bank DRAM-PIM with Fused-layer Dataflow for CNN Data Transfer Optimization</td>
<td>hybrid dataflow combining fused-layer and layer-by-layer strategies</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2026</td>
<td>HPCA</td>
<td>Seoul National</td>
<td>RoMe: Row Granularity Access Memory System for Large Language Models</td>
<td>row-granularity access interface for LLM streaming; virtual bank to eliminate bank groups and pseudo channels; logic-die command generator for C/A pin reduction and simpler MC</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="graph-specific-optimization">Graph-Specific Optimization<a class="headerlink" href="#graph-specific-optimization" title="Permanent link">&para;</a></h4>
<p>Challenge: Graph processing is fundamentally limited by memory bandwidth and requires frequent random accesses, which are not efficiently supported by non-interleaved, bank-level PIM architectures.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022</td>
<td>PACT</td>
<td>PKU</td>
<td>GNNear: Accelerating Full-Batch Training of Graph Neural Networks with Near-Memory Processing</td>
<td>splitting reduce operations to NDP units; narrow-shard strategy for data reuse; hybrid graph partition strategy for load balancing</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>ZJU</td>
<td>GOPIM: GCN-Oriented Pipeline Optimization for PIM Accelerators</td>
<td>ML-based replica resource allocation for pipeline streamlining; interleaved mapping with adaptive selective vertex updating</td>
<td>2</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="llm-optimization">LLM Optimization<a class="headerlink" href="#llm-optimization" title="Permanent link">&para;</a></h4>
<p>Challenge: LLM inference is fundamentally bottlenecked by memory bandwidth; HBM is expensive and not scalable.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>npj Unconv. Comput.</td>
<td>UMich</td>
<td>PIM-GPT: a hybrid process in memory accelerator for autoregressive transformers</td>
<td>hybrid system to accelerate GPT inference; mapping scheme for data locality and workload distribution</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>MICRO</td>
<td>KAIST</td>
<td>PIMBA: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving</td>
<td>Unified PIM acceleration for both transformer and post-transformer LLMs; access interleaving technique for shared State-update Processing Unit</td>
<td>4</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>MICRO</td>
<td>Cornell</td>
<td>LongSight: Compute-Enabled Memory to Accelerate Large-Context LLMs via Sparse Attention</td>
<td>hybrid dense-sparse attention algorithm; KV cache offloading to CXL-PIM; sign-concordance filtering by iterative quantization</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>IEEE LCA</td>
<td>POSTECH</td>
<td>Cost-Effective Extension of DRAM-PIM for Group-Wise LLM Quantization</td>
<td>scale cascading for simplifying dequantization; zero-offset removal for reduced circuit complexity</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>IEEE LCA</td>
<td>Yonsei</td>
<td>RoPIM: A Processing-in-Memory Architecture for Accelerating Rotary Positional Embedding in Transformer Models</td>
<td>bank-level PIM accelerator for RoPE; row-level data mapping for single-token Q/K/V/S alignment; parallel data rearrangement via inverters</td>
<td>4</td>
<td>3</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="moe-llm-optimization">MoE-LLM Optimization<a class="headerlink" href="#moe-llm-optimization" title="Permanent link">&para;</a></h4>
<p>Challenge: MoE models often have higher Op/Byte ratios, making bank-level PIM easily compute-bound and limiting speedup.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>DAC</td>
<td>Seoul National</td>
<td>MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models</td>
<td>activation movement strategy to replace costly parameter movement; dynamic GPU-MoNDE load balancing for hot/cold experts</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>MICRO</td>
<td>Samsung</td>
<td>Duplex: A Device for Large Language Models with Mixture of Experts, Grouped Query Attention, and Continuous Batching</td>
<td>replace the GPU HBM memory die with HBM-PIM die; expert and attention co-processing for dynamic workload splitting within MoE/attn layers</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>ICCAD</td>
<td>PKU</td>
<td>HD-MoE: Hybrid and Dynamic Parallelism for Mixture-of-Expert LLMs with 3D Near-Memory Processing</td>
<td>LP-based hybrid TP and EP mapping; bayesian optimization for topology-aware link balancing; online dynamic expert placement with predictive pre-broadcast</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="rag-specific-optimization">RAG-Specific Optimization<a class="headerlink" href="#rag-specific-optimization" title="Permanent link">&para;</a></h4>
<p>Challenge: Retrieving the top-k results from a vectorized database is also a memory-bound operation.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>HUST</td>
<td>HeterRAG: Heterogeneous Processing-in-Memory Acceleration for Retrieval-augmented Generation</td>
<td>combine DIMM-PIM and HBM-PIM for acceleration; locality-aware retrieval and generation; fine-grained parallel pipelining</td>
<td>2</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>MICRO</td>
<td>Yonsei</td>
<td>Accelerating Retrieval Augmented Language Model via PIM and PNM Integration</td>
<td>heterogeneous architecture integrating PIM for LLMs and PNM for retrievers; RALM scheduling strategy with selective batching and early generation</td>
<td>4</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="memory-address-space">Memory Address Space<a class="headerlink" href="#memory-address-space" title="Permanent link">&para;</a></h4>
<p>Challenge: Host pages need to enable interleaving to improve concurrent throughput, while PIM pages need to disable it to maintain better locality, creating a conflict.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>DAC</td>
<td>Georgia Tech</td>
<td>vPIM: Efficient Virtual Address Translation for Scalable Processing-in-Memory Architectures</td>
<td>network-contention-aware hashing to minimize cross-stack page table walks; pre-translation using repurposed PIM cores to move page table walks off the critical path</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>ISCA</td>
<td>SJTU</td>
<td>UM-PIM: DRAM-based PIM with Uniform &amp; Shared Memory Space</td>
<td>Uniform shared CPU-PIM memory; dual-track memory management; zero-copy data re-layout</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="memory-allocation-management">Memory Allocation &amp; Management<a class="headerlink" href="#memory-allocation-management" title="Permanent link">&para;</a></h4>
<p>Challenge: Existing NDP architecture has numerous independent memory spaces; lacks unified management; and features inefficient memory allocation.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>ISCA</td>
<td>KAIST</td>
<td>PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for Processing-In-Memory (PIM) Architectures</td>
<td>PIM-specific memory allocator; hierarchical memory allocation scheme; hardware metadata cache</td>
<td>4</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>ETHZ</td>
<td>PUMA: Efficient and Low-Cost Memory Allocation and Alignment Support for Processing-Using-Memory Architectures</td>
<td>aligned memory allocator for PUM; DRAM-aware memory allocation</td>
<td>2</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>MICRO</td>
<td>KAIST</td>
<td>PIM-MMU: A Memory Management Unit for Accelerating Data Transfers in Commercial PIM Systems</td>
<td>data copy engine for host-PIM transfers; PIM-aware memory scheduler for MLP maximization; memory remapping unit for dual address mapping</td>
<td>2</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Amazon</td>
<td>DL-PIM: Improving Data Locality in Processing-in-Memory Systems</td>
<td>subscription-based architecture to proactively move data; distributed address-indirection hardware lookup table</td>
<td>3</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="pim-compiler-data-layout">PIM Compiler &amp; Data Layout<a class="headerlink" href="#pim-compiler-data-layout" title="Permanent link">&para;</a></h4>
<p>Challenge: Existing compilers are not optimized for locality-aware PIM architectures and require specialized programming models to fully utilize PIM capabilities.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2015</td>
<td>ISCA</td>
<td>Seoul National</td>
<td>PIM-Enabled Instructions: A Low-Overhead; Locality-Aware Processing-in-Memory Architecture</td>
<td>PIM-Enabled Instructions for ISA extension; PIM directory for atomicity and coherence; single-cache-block restriction</td>
<td>3</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2020</td>
<td>ISCA</td>
<td>UCSB</td>
<td>iPIM: Programmable In-Memory Image Processing Accelerator Using Near-Bank Architecture</td>
<td>Single-Instruction-Multiple-Bank ISA; register allocation; instruction reordering</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>POSTECH</td>
<td>ATIM: Autotuning Tensor Programs for Processing-in-DRAM</td>
<td>autotuning framework for DRAM PIM; search-based optimizing tensor compiler; balanced evolutionary search algorithm</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>ETHZ</td>
<td>OptiPIM: Optimizing Processing-in-Memory Acceleration Using Integer Linear Programming</td>
<td>layout-aware nested loop representation; Integer Linear Programming formulation for PIM mapping; analytical cost modeling for data layout enforcement</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="evaluation-simulators">Evaluation &amp; Simulators<a class="headerlink" href="#evaluation-simulators" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>THU</td>
<td>UniNDP: A Unified Compilation and Simulation Tool for Near DRAM Processing Architectures</td>
<td>unified NDP hardware abstraction; NDP compiler optimization; instruction-driven NDP simulator</td>
<td>3</td>
<td>5</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>ETHZ</td>
<td>EasyDRAM: An FPGA-based Infrastructure for Fast and Accurate End-to-End Evaluation of Emerging DRAM Techniques</td>
<td>FPGA-based DRAM evaluation framework; C++ high-level language for description; time scaling for accurate modeling</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="intra-dimm-communication">Intra-DIMM Communication<a class="headerlink" href="#intra-dimm-communication" title="Permanent link">&para;</a></h4>
<p>Challenge: High latency of intra-DIMM (cross-bank) communication via host CPU forwarding.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>ISCA</td>
<td>THU</td>
<td>NDPBridge: Enabling Cross-Bank Coordination in Near-DRAM-Bank Processing Architectures</td>
<td>gather &amp; scatter messages via buffer chip; task-based message-passing model; hierarchical, data-transfer-aware load balancing</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>Samsung</td>
<td>Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory Scatter-Gather</td>
<td>In-DRAM fine-grained scatter-gather via data bus offsets; fine-grained cache architecture using fg-tags; Standard DDR command interpretation for FIM control; Combined graph tiling with fine-grained memory access</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>ETHZ</td>
<td>PIMDAL: Mitigating the Memory Bottleneck in Data Analytics using a Real Processing-in-Memory System</td>
<td>PIMDAL library for DB operators; quicksort/mergesort/hashing on UPMEM PIM; scatter/gather/async transfers for PIM communication</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Seoul National</td>
<td>PID-Comm: A Fast and Flexible Collective Communication Framework for Commodity Processing-in-DIMM Devices</td>
<td>Virtual hypercube PIM model; PE-assisted data reordering; in-register and cross-domain data modulation</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>KAIST</td>
<td>PIMnet: A Domain-Specific Network for Efficient Collective Communication in Scalable PIM</td>
<td>domain-specific PIM interconnect; hierarchical network for PIM packaging; PIM-controlled deterministic scheduling</td>
<td>2</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="inter-dimm-communication">Inter-DIMM Communication<a class="headerlink" href="#inter-dimm-communication" title="Permanent link">&para;</a></h4>
<p>Challenge: High latency of inter-DIMM (cross-DIMM) communication via host CPU forwarding.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2017</td>
<td>MEMSYS</td>
<td>UCLA</td>
<td>AIM: Accelerating Computational Genomics through Scalable and Noninvasive Accelerator-Interposed Memory</td>
<td>placing FPGA chip between DIMM and the conventional memory network; multi-drop bus for inter-accelerator communication</td>
<td>1</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2023</td>
<td>ASPLOS</td>
<td>THU</td>
<td>ABNDP: Co-optimizing Data Access and Load Balance in Near-Data Processing</td>
<td>Traveller Cache; hybrid task scheduling; hybrid scheduling leveraging distributed cache</td>
<td>4</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2023</td>
<td>HPCA</td>
<td>PKU</td>
<td>DIMM-Link: Enabling Efficient Inter-DIMM Communication for Near-Memory Processing</td>
<td>high-speed hardware link bridges between DIMMs; direct intra-group P2P communication &amp; broadcast; hybrid routing mechanism for inter-group communication</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>SJTU</td>
<td>AsyncDIMM: Achieving Asynchronous Execution in DIMM-Based Near-Memory Processing</td>
<td>Offload-Schedule-Return mechanism; switch-recovery scheduling; explicit/implicit synchronization</td>
<td>2</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2018</td>
<td>MICRO</td>
<td>UIUC</td>
<td>Application-Transparent Near-Memory Processing Architecture with Memory Channel Network</td>
<td>integrates a processor on a buffered DIMM; application-transparent near-memory processing; leverages memory channels for high-bandwidth/low-latency inter-processor communication</td>
<td>3</td>
<td>4</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="cpu-pim-heterogeneous-systems">CPU-PIM Heterogeneous Systems<a class="headerlink" href="#cpu-pim-heterogeneous-systems" title="Permanent link">&para;</a></h4>
<p>Challenge: High latency of concurrent host CPU and PIM operations via host CPU forwarding.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>IEEE CA</td>
<td>KAIST</td>
<td>Analysis of Data Transfer Bottlenecks in Commercial PIM Systems: A Study With UPMEM-PIM</td>
<td>runtime data transposition causing high CPU overhead; PIM-integrated system memory mapping impact</td>
<td>2</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>ASPLOS</td>
<td>KAIST</td>
<td>NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing</td>
<td>dual row buffer architecture; sub-batch interleaving; greedy min-load bin packing algorithm</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>Univ. of Virginia</td>
<td>Membrane: Accelerating Database Analytics with Bank-Level DRAM-PIM Filtering</td>
<td>bank-level DRAM-PIM filtering; CPU-PIM cooperative query execution; denormalization for PIM-amenable filtering</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>MICRO</td>
<td>Inha University</td>
<td>ComPASS: A Compatible PIM Protocol Architecture and Scheduling Solution for Processor-PIM Collaboration</td>
<td>PIM-ACT new memory command for multi-bank PIM operations; PIM request generator to offload host processor; static and adaptive throughput balancers for PIM and non-PIM request scheduling</td>
<td>4</td>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="npu-pim-heterogeneous-systems">NPU-PIM Heterogeneous Systems<a class="headerlink" href="#npu-pim-heterogeneous-systems" title="Permanent link">&para;</a></h4>
<p>Challenge: Data transfer between NPU and PIM needs to go through the Host, causing high latency.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>MICRO</td>
<td>SJTU</td>
<td>HEAT: NPU-NDP HEterogeneous Architecture for Transformer-Empowered Graph Neural Networks</td>
<td>topology-aware mixed-precision encoding for transformer; subgraph bundling and reordering for GNN memory efficiency; decoupled dataflow for NPU-NDP concurrent execution</td>
<td>2</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>ICCAD</td>
<td>PKU</td>
<td>LP-Spec: Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization</td>
<td>GEMM-enhanced hybrid LPDDR5 PIM; near-data memory controller for concurrent NPU-PIM execution &amp; data reallocation; hardware-aware draft token pruning</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Cornell</td>
<td>P3-LLM: An Integrated NPU-PIM Accelerator for LLM Inference Using Hybrid Numerical Formats</td>
<td>low-precision PIM compute unit with temporal data reuse; operator fusion for quantized dataflow to minimize dequantization overhead</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="gpu-pim-heterogeneous-systems">GPU-PIM Heterogeneous Systems<a class="headerlink" href="#gpu-pim-heterogeneous-systems" title="Permanent link">&para;</a></h4>
<p>Challenge: Weight, activation and KV Cache data mapping in conventional GPU-PIM systems is naive, causing space &amp; bandwidth waste and load imbalance.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>DAC</td>
<td>Hunan Univ.</td>
<td>A Real-time Execution System of Multimodal Transformer through PIM-GPU Collaboration</td>
<td>dynamic strategy for PIM-GPU task offloading; variable-length-aware PIM allocation optimizer; extended TVM backend for PIM-GPU command generation</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>HPCA</td>
<td>ICT</td>
<td>Make LLM Inference Affordable to Everyone: Augmenting GPU Memory with NDP-DIMM</td>
<td>activation sparsity-based hot(GPU)/cold(NDP) neuron partitioning; offline ILP + online predictor for neuron partition; window-based online remapping for GPU-NDP &amp; NDP-NDP load balance</td>
<td>2</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Hanyang Univ.</td>
<td>Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits</td>
<td>hybrid TP-DP parallelism for GPU-PNM systems; token page selection; steady-token selection</td>
<td>2</td>
<td>3</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="optimizations-on-upmem-pim">Optimizations on UPMEM-PIM<a class="headerlink" href="#optimizations-on-upmem-pim" title="Permanent link">&para;</a></h4>
<p>Challenge: The original UMPEM API library is not well-suited for all workloads especially for those with cross-bank communication.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>arXiv</td>
<td>ETHZ</td>
<td>A Framework for High-throughput Sequence Alignment using Real Processing-in-Memory Systems</td>
<td>Alignment-in-Memory framework; hybrid WRAM-MRAM sketch data management for PIM</td>
<td>2</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>ETHZ</td>
<td>PIMDAL: Mitigating the Memory Bottleneck in Data Analytics using a Real Processing-in-Memory System</td>
<td>PIMDAL library on UPMEM PIM system for data analytics; scatter/gather-aware transfers for inter-PIM communication; Apache Arrow for host memory management</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="in-cache-computing">In-Cache-Computing<a class="headerlink" href="#in-cache-computing" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Torino</td>
<td>ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions</td>
<td>ARCANE in-cache NMC coprocessor architecture; software-defined matrix ISA for NMC abstraction; cache-integrated control runtime for NMC management</td>
<td>3</td>
<td>4</td>
<td>4</td>
</tr>
</tbody>
</table>
<h3 id="pim-ndp-benchmarks">PIM &amp; NDP Benchmarks<a class="headerlink" href="#pim-ndp-benchmarks" title="Permanent link">&para;</a></h3>
<p>Challenge: Conventional parallel computing benchmarks are not suitable for PIM/NDP.</p>
<h4 id="benchmarks-for-conventional-computing">Benchmarks for Conventional Computing<a class="headerlink" href="#benchmarks-for-conventional-computing" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021</td>
<td>ATC</td>
<td>UBC</td>
<td>A Case Study of Processing-in-Memory in off-the-Shelf Systems</td>
<td>benchmark</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022</td>
<td>IEEE Access</td>
<td>ETH</td>
<td>Benchmarking a New Paradigm: Experimental Analysis and Characterization of a Real Processing-in-Memory System</td>
<td>benchmark suite PrIM</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>CAL</td>
<td>KAIST</td>
<td>Analysis of Data Transfer Bottlenecks in Commercial PIM Systems: A Study With UPMEM-PIM</td>
<td>low MLP; manual data placement; unbalanced thread allocation and scheduling</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>IEEE Access</td>
<td>Lisbon</td>
<td>NDPmulator: Enabling Full-System Simulation for Near-Data Accelerators From Caches to DRAM</td>
<td>simulator PiMulator based on Ramulator &amp; gem5; full system support; multiple ISA support</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>HPCA</td>
<td>KAIST</td>
<td>Pathfinding Future PIM Architectures by Demystifying a Commercial PIM Technology</td>
<td>simulator uPIMulator</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="benchmarks-for-quantum-computing">Benchmarks for Quantum Computing<a class="headerlink" href="#benchmarks-for-quantum-computing" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>ASPDAC</td>
<td>NUS</td>
<td>PIMutation: Exploring the Potential of PIM Architecture for Quantum Circuit Simulation</td>
<td>PIMutation framework for quantum circuit simulation; gate merging optimization; row swapping instead of matrix multiplication; vector partitioning for separable states; leveraging UPMEM PIM architecture</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="cxl-based-pim">CXL-Based PIM<a class="headerlink" href="#cxl-based-pim" title="Permanent link">&para;</a></h3>
<p>Challenge: No direct physical connectivity between the banks in the DIMM-based NDP architecture. Limited number of DDR channels causing poor scalability.</p>
<p>Solution: Introduce CXL-based interconnects to enable direct communication between memory banks; Use CXL memory pools and CXL switches to enable scalable NDP architecture.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022</td>
<td>MICRO</td>
<td>UCSB</td>
<td>BEACON: Scalable Near-Data-Processing Accelerators for Genome Analysis near Memory Pool with the CXL Support</td>
<td>scalable hardware accelerator inside CXL switch or bank; lossless memory expansion for CXL memory pools</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>ICS</td>
<td>Samsung</td>
<td>CLAY: CXL-based Scalable NDP Architecture Accelerating Embedding Layers</td>
<td>direct interconnect between DRAM clusters; dedicated memory address mapping scheme; Multi-CLAY system support through customized CXL switch</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>MICRO</td>
<td>SK Hyrix</td>
<td>Low-overhead General-purpose Near-Data Processing in CXL Memory Expanders</td>
<td>CXL.mem protocol instead of CXL.io (DMA) for low-latency; lightweight threads to reduce address calculation overhead</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>Seoul National</td>
<td>COSMOS: A CXL-Based Full In-Memory System for Approximate Nearest Neighbor Search</td>
<td>CXL core-based ANNS task offload; rank-level parallel distance computation; adjacency-aware data placement algorithm</td>
<td>2</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>ASPLOS</td>
<td>UMich</td>
<td>PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference</td>
<td>hierarchical CXL PIM-PNM compute architecture; use die-shot to estimate area cost; multiple LLM parallelism policies</td>
<td>2</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="chiplet-based-pim">Chiplet-Based PIM<a class="headerlink" href="#chiplet-based-pim" title="Permanent link">&para;</a></h3>
<p>Challenge: The logic units of near-bank DRAM PIM are fabricated using the same process node as DRAM, which restricts their performance and efficiency.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022</td>
<td>IEEE TCAD</td>
<td>WSU</td>
<td>SWAP: A Server-Scale Communication-Aware Chiplet-Based Manycore PIM Accelerator</td>
<td>coarse-to-fine multi-objective optimization algorithm for Network-On-Package design; communication-aware irregular NoP topology</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Univ. of Virginia</td>
<td>Sangam: A Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing</td>
<td>decoupling logic and memory dies by chiplet; bank-level systolic arrays for flat-GEMM acceleration</td>
<td>4</td>
<td>3</td>
<td>4</td>
</tr>
</tbody>
</table>
<h3 id="3d-stacked-pim">3D-Stacked PIM<a class="headerlink" href="#3d-stacked-pim" title="Permanent link">&para;</a></h3>
<p>Challenge: There is no direct physical interconnection paths in DIMM-based, bank-level uniform NDP like UPMEM.</p>
<p>Solution: Put the logical, computational layer at the bottom of the die, and stack DRAM layers on top of it. Use TSVs to build thousands of physical paths between the logical and the DRAM layers.</p>
<h4 id="hybrid-bounding-based-pim">Hybrid Bounding-Based PIM<a class="headerlink" href="#hybrid-bounding-based-pim" title="Permanent link">&para;</a></h4>
<p>Solution: Hybrid Bounding provides massively increased interconnect density and bandwidth by direct copper-to-copper connection.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>ISCA</td>
<td>THU</td>
<td>Exploiting Similarity Opportunities of Emerging Vision AI Models on Hybrid Bonding Architecture</td>
<td>clustering similarity effect architecture for hybrid bonding DRAM; hotspot content SRAM for parallel similarity detection; progressive sparsity detection and balance for computation skipping and workload redistribution</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>PKU</td>
<td>H2-LLM: Hardware-Dataflow Co-Exploration for Heterogeneous Hybrid-Bonding-based Low-Batch LLM Inference</td>
<td>operator-channel binding; computation-bandwidth trade-off; dataflow-based DSE</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>MICRO</td>
<td>THU</td>
<td>3D-PATH: A Hierarchy LUT Processing-in-memory Accelerator with Thermal-aware Hybrid Bonding Integration</td>
<td>sparse-aware hierarchical slow-fast LUT design; multiplier-free floating-point operation by LUT; hotspot-aware hardware with self-throttling sense amplifier</td>
<td>4</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>MICRO</td>
<td>Univ. of British Columbia</td>
<td>RayN: Ray Tracing Acceleration with Near-memory Computing</td>
<td>ray tracing units in 3D stacked DRAM logic layer; BLAS Breaking algorithm to partition BVH tree for load balancing; hybrid memory controller for concurrent GPU and near-memory access</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>ICCAD</td>
<td>PKU</td>
<td>FENIX: Flexible and Efficient Hybrid HE/MPC Acceleration with Near-Memory Processing</td>
<td>fine-grained oblivious transfer partitioning to overlap HE and OT operations; batch-aware flexible encoding to reduce rotation overhead; near-bank NMP to offload memory-bound HE/OT primitives</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="hmc-or-hmc-like-pim">HMC or HMC-like PIM<a class="headerlink" href="#hmc-or-hmc-like-pim" title="Permanent link">&para;</a></h4>
<p>Challenge: No direct physical connectivity between the banks in the DIMM-based NDP architecture.</p>
<p>Solution: Use TSVs to provide TB/s level bandwidth in inter-bank communication &amp; band-to-logic layer communication.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2013</td>
<td>PACT</td>
<td>KAIST</td>
<td>Memory-centric System Interconnect Design with Hybrid Memory Cubes</td>
<td>memory-centric network; distributor-based topology for reduced latency; non-minimal routing for higher throughput</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2021</td>
<td>DAC</td>
<td>UCSD</td>
<td>MAT: Processing In-Memory Acceleration for Long-Sequence Attention</td>
<td>iterative tiled processing to reduce memory footprint of O(N^2) score matrices; late softmax update to enable pipelined attention; dynamic programming-based sample scheduling for optimal PIM resource utilization</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>DAC</td>
<td>SNU</td>
<td>MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models</td>
<td>NDP for MoE; activation movement; GPU-MoNDE load-balancing scheme</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>ASPLOS</td>
<td>PKU</td>
<td>SpecPIM: Accelerating Speculative Inference on PIM-Enabled System via Architecture-Dataflow Co-Exploration</td>
<td>algorithmic and architectural heterogeneity; PIM resource allocation; multi-model collaboration workflow</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>MICRO</td>
<td>UCSD</td>
<td>Stratum: System-Hardware Co-Design with Tiered Monolithic 3D-Stackable DRAM for Efficient MoE Serving</td>
<td>monolithic 3D-Stackable DRAM without TSV; in-memory tiering for vertical latency variation; topic-based expert usage prediction for MoE serving</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="hbm2-pim">HBM2-PIM<a class="headerlink" href="#hbm2-pim" title="Permanent link">&para;</a></h4>
<p>Solution: HBM2-PIM is the first commercial HBM-PIM product, and 4 out of 8 DRAM layers are PIM-enabled layers, while the other 4 layers are standard DRAM layers.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021</td>
<td>ISCA</td>
<td>Samsung</td>
<td>Hardware Architecture and Software Stack for PIM Based on Commercial DRAM Technology Industrial Product</td>
<td>drop-in replacement for standard HBM2; bank-level parallelism using standard DRAM commands; address aligned mode to tolerate host-side command reordering</td>
<td>3</td>
<td>5</td>
<td>3</td>
</tr>
<tr>
<td>2022</td>
<td>Hot Chips</td>
<td>Samsung</td>
<td>Aquabolt-XL HBM2-PIM, LPDDR5-PIM With In-Memory Processing, and AXDIMM With Acceleration Buffer</td>
<td>HBM2-PIM with bank-level SIMD programmable computing units; Acceleration DIMM with acceleration buffers for rank-level parallelism</td>
<td>2</td>
<td>5</td>
<td>3</td>
</tr>
<tr>
<td>2023</td>
<td>MICRO</td>
<td>Yonsei</td>
<td>AESPA: Asynchronous Execution Scheme to Exploit Bank-Level Parallelism of Processing-in-Memory</td>
<td>Single-Instruction Long-Data execution model; asynchronous bank operation via long-data commands; column-major GEMV dataflow with shared accumulators</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="hbm3-pim">HBM3-PIM<a class="headerlink" href="#hbm3-pim" title="Permanent link">&para;</a></h4>
<p>Solution: HBM3-PIM further increases the capacity and the parallelism by stacking more DRAM layers and all the layers are the PIM-enabled layer.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>ASPLOS</td>
<td>Seoul National</td>
<td>AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference</td>
<td>bank-level GEMV &amp; buffer-die Softmax units; head-level pipelining; feedforward co-processing optimizations</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>ICS</td>
<td>Korea Univ.</td>
<td>SparsePIM: An Efficient HBM-Based PIM Architecture for Sparse Matrix-Vector Multiplications</td>
<td>DRAM row-aligned format; bounded cap K-means clustering for load balancing; bank group accumulators</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="benchmarks">Benchmarks<a class="headerlink" href="#benchmarks" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2019</td>
<td>DAC</td>
<td>ETHZ</td>
<td>NAPEL: Near-Memory Computing Application Performance Prediction via Ensemble Learning</td>
<td>simulator Ramulator-PIM; tracefile from Ramulator &amp; run on zsim</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2021</td>
<td>CAL</td>
<td>UVA</td>
<td>MultiPIM: A Detailed and Configurable Multi-Stack Processing-In-Memory Simulator</td>
<td>simulator MultiPIM; multi-stack &amp; virtual memory support; parallel offloading</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="pim-heterogeneous-architecture">PIM: Heterogeneous Architecture<a class="headerlink" href="#pim-heterogeneous-architecture" title="Permanent link">&para;</a></h4>
<p>Challenge: Different PIM architectures have different characteristics and performance trade-offs; communicating between different PIM architectures is challenging.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>NUS</td>
<td>LEAP: LLM Inference on Scalable PIM-NoC Architecture with Balanced Dataflow and Fine-Grained Parallelism</td>
<td>data dynamicity-aware task assignment to PIM or NoC; fine-grained model partitioning and heuristically optimized spatial mapping strategy</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>THU</td>
<td>CompAir: Synergizing Complementary PIMs and In-Transit NoC Computation for Efficient LLM Acceleration</td>
<td>heterogeneous DRAM-PIM and SRAM-PIM architecture with hybrid bonding; in-transit NoC computation with Curry ALU; hierarchical ISA for hybrid PIM systems</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="general-cim">General CiM<a class="headerlink" href="#general-cim" title="Permanent link">&para;</a></h3>
<h4 id="specific-application-algorithm">Specific Application &amp; Algorithm<a class="headerlink" href="#specific-application-algorithm" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>ISVLSI</td>
<td>USC</td>
<td>Multi-Objective Neural Architecture Search for In-Memory Computing</td>
<td>neural architecture search methodology; integration of Hyperopt, PyTorch and MNSIM</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>Intel</td>
<td>CiMNet: Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware</td>
<td>framework that jointly searches for optimal sub-networks and hardware configurations for CiM architectures; multi-objective evolutionary search method</td>
<td>4</td>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>2025</td>
<td>AICAS</td>
<td>UVA</td>
<td>Optimizing and Exploring System Performance in Compact Processing-in-Memory-based Chips</td>
<td>Pipeline Method for Compact PIM Designs; Dynamic Duplication Method (DDM); Maximum NN Size Estimation &amp; Deployment in Compact PIM Design</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="modeling-simulation">Modeling &amp; Simulation<a class="headerlink" href="#modeling-simulation" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2018</td>
<td>TCAD</td>
<td>ASU</td>
<td>NeuroSim: A Circuit-Level Macro Model for Benchmarking Neuro-Inspired Architectures in Online Learning</td>
<td>estimate the circuit-level performance of neuro-inspired architectures; estimates the area, latency, dynamic energy, and leakage power; Support both SRAM and eNVM; tested on 2-layer MLP NN, MNIST</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2019</td>
<td>IEDM</td>
<td>Georgia Tech</td>
<td>DNN+NeuroSim: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators with Versatile Device Technologies</td>
<td>a python wrapper to interface NeuroSim; for inference only</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2020</td>
<td>TCAD</td>
<td>ZJU</td>
<td>Eva-CiM: A System-Level Performance and Energy Evaluation Framework for Computing-in-Memory Architectures</td>
<td>models for capturing memory access and dependency-aware ISA traces; models for quantifying interactions between the host CPU and the CiM module</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022</td>
<td>ICCAD</td>
<td>Purdue</td>
<td>Design Space and Memory Technology Co-Exploration for In-Memory Computing Based Machine Learning Accelerators</td>
<td>simulation framework to evaluate the systemlevel performance of IMC architecture; area-aware weight mapping strategy</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>ISPASS</td>
<td>MIT</td>
<td>CiMLoop: A Flexible, Accurate, and Fast Compute-In-Memory Modeling Tool</td>
<td>flexible specification to describe CiM systems; accurate model/fast statistical model of data-value-dependent component energy</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>ASPDAC</td>
<td>HKUST</td>
<td>MICSim: A Modular Simulator for Mixed-signal Compute-in-Memory based AI Accelerator</td>
<td>modulared Neurosim; data statistic-based average-mode instead of trace-based mode</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="compiler">Compiler<a class="headerlink" href="#compiler" title="Permanent link">&para;</a></h4>
<p>Challenge: Compiler for CIM is not well studied. Existing compilers are either for specific architecture or not efficient.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>TACO</td>
<td>HUST</td>
<td>A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures</td>
<td>compilation tool to migrate legacy programs to CPU/CIM heterogeneous architectures; a model to quantify the performance gain</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>DAC</td>
<td>CAS</td>
<td>PIMCOMP: A Universal Compilation Framework for Crossbar-based PIM DNN Accelerators</td>
<td>compiler based on Crossbar/IMA/Tile/Chip hierarchy; low latency and high throughput mode; genetic algorithm to optimize weight replication and core mapping; scheduling algorithms for complex DNN</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>ASPLOS</td>
<td>CAS</td>
<td>CIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators</td>
<td>compilation stack for various CIM accelerators; multi-level DNN scheduling approach</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>DATE</td>
<td>RWTH Aachen University</td>
<td>CLSA-CIM: A Cross-Layer Scheduling Approach for Computing-in-Memory Architectures</td>
<td>algorithm to decide which parts of NN are duplicated to reduce inference latency; cross layer scheduling on tiled CIM architectures</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>TCAD</td>
<td>NJU</td>
<td>A Compilation Framework for SRAM Computing-in-Memory Systems With Optimized Weight Mapping and Error</td>
<td>input/output side parallelism (IOSP); partition-based MAQE(duplicating MSB storage)</td>
<td>4</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="cim-dram">CIM: DRAM<a class="headerlink" href="#cim-dram" title="Permanent link">&para;</a></h3>
<p>Solution: Rather than placing logic units into DRAM; modify the physical structure of DRAM/eDRAM to enable in-memory computing.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021</td>
<td>ICCD</td>
<td>ASU</td>
<td>CIDAN: Computing in DRAM with Artificial Neurons</td>
<td>Threshold Logic Processing Element (TLPE) for in-memory computation; Four-bank activation window; Configurable threshold functions; Energy-efficient bitwise operations; Integration with DRAM architecture</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022</td>
<td>HPCA</td>
<td>UCSD</td>
<td>TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer</td>
<td>token-based dataflow for general Transformer-based models; ring-based data broadcast in modified HBM</td>
<td>4</td>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>2024</td>
<td>A-SSCC</td>
<td>UNIST</td>
<td>A 273.48 TOPS/W and 1.58 Mb/mm2 Analog-Digital Hybrid CIM Processor with Transpose Ternary-eDRAM Bitcell</td>
<td>analog DRAM CIM for partial sum and digital adder</td>
<td>1</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>KAIST</td>
<td>RED: Energy Optimization Framework for eDRAM-based PIM with Reconfigurable Voltage Swing and Retention-aware Scheduling</td>
<td>RED framework for energy optimization; reconfigurable eDRAM design; retention-aware scheduling; trade-off analysis between RBL voltage swing, sense amplifier power, and retention time; refresh skipping and sense amplifier power gating</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UTokyo</td>
<td>MVDRAM: Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM Acceleration</td>
<td>GeMV operations for end-to-end low-bit LLM inference using unmodified DRAM; processor-DRAM co-design; on-the-fly vector encoding; horizontal matrix layout</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Purdue</td>
<td>HALO: Memory-Centric Heterogeneous Accelerator with 2.5D Integration for Low-Batch LLM Inference</td>
<td>heterogeneous CiD/CiM accelerator; phase-aware mapping strategy</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="cim-sram">CIM: SRAM<a class="headerlink" href="#cim-sram" title="Permanent link">&para;</a></h3>
<p>Challenge: Memory wall causing high latency of data transfer between CPU and memory; DIMM-based NDP causing high energy consumption; area overhead and low performance efficiency.</p>
<p>Solution: Generally modify the physical structure of SRAM to enable in-memory computing; rather than placing logic units into SRAM.</p>
<h4 id="sram-cim-general-architecture">SRAM CIM: General Architecture<a class="headerlink" href="#sram-cim-general-architecture" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>ISCAS</td>
<td>NYCU</td>
<td>CIMR-V: An End-to-End SRAM-based CIM Accelerator with RISC-V for AI Edge Device</td>
<td>incorporates CIM layer fusion, convolution/max pooling pipeline, and weight fusion; weight fusion: pipelining the CIM convolution and weight loading</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2018</td>
<td>JSSC</td>
<td>MIT</td>
<td>CONV-SRAM: An Energy-Efficient SRAM With In-Memory Dot-Product Computation for Low-Power Convolutional Neural Networks</td>
<td>SRAM-embedded convolution (dot-product) computation architecture for BNN; support multi-bit input-output</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>ESSCIRC</td>
<td>THU</td>
<td>A 65nm 8b-Activation 8b-Weight SRAM-Based Charge-Domain Computing-in-Memory Macro Using A Fully-Parallel Analog Adder Network and A Single-ADC Interface</td>
<td>SRAM-based CD-CiM architecture; charge-domain analog adder tree; ReLU-optimized ADC</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2021</td>
<td>ISSCC</td>
<td>TSMC</td>
<td>An 89TOPS/W and 16.3TOPS/mm2 All-Digital SRAM-Based Full-Precision Compute-In Memory Macro in 22nm for Machine-Learning Edge Applications</td>
<td>programmable bit-widths for both input and weights; SRAM and CIM mode</td>
<td>2</td>
<td>5</td>
<td>1</td>
</tr>
<tr>
<td>2021</td>
<td>JSSC</td>
<td>KAIST</td>
<td>Z-PIM: A Sparsity-Aware Processing-in-Memory Architecture With Fully Variable Weight Bit-Precision for Energy-Efficient Deep Neural Networks</td>
<td>bit-serial operation to support variable weight bit-precision; data mapping and computation flow for sparsity handling</td>
<td>3</td>
<td>4</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="sram-cim-reconfigurable-macro">SRAM CIM: Reconfigurable Macro<a class="headerlink" href="#sram-cim-reconfigurable-macro" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021</td>
<td>JSSC</td>
<td>UCSB</td>
<td>Colonnade: A Reconfigurable SRAM-Based Digital Bit-Serial Compute-In-Memory Macro for Processing Neural Networks</td>
<td>reconfigurable column MAC; sparse pipelining</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2023</td>
<td>TCSI</td>
<td>UCSB</td>
<td>A 1-16b Reconfigurable 80Kb 7T SRAM-Based Digital Near-Memory Computing Macro for Processing Neural Networks</td>
<td>radix-4 booth encoding; bit-serial reconfigurable logic</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>TCSI</td>
<td>CAS</td>
<td>A 1-8b Reconfigurable Digital SRAM Compute-in-Memory Macro for Processing Neural Networks</td>
<td>Decompose-Accumulate-and-Shift(DAS); Reconfigurable Digital Arithmetic Unit (RDAU); input-sparsity driven clock gating</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="sram-cim-specific-use-or-application">SRAM CIM: Specific Use or Application<a class="headerlink" href="#sram-cim-specific-use-or-application" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>TCAS-I</td>
<td>UIC</td>
<td>MC-CIM: Compute-in-Memory With Monte-Carlo Dropouts for Bayesian Edge Intelligence</td>
<td>SRAM-based CIM macros to accelerate Monte-Carlo dropout; compute reuse between consecutive iterations</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>DAC</td>
<td>GWU</td>
<td>Addition is Most You Need: Efficient Floating-Point SRAM Compute-in-Memory by Harnessing Mantissa Addition</td>
<td>decomposing FP mantissa multiplication into sub-ADD and sub-MUL; hybrid-domain SRAM CIM architecture</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>A-SSCC</td>
<td>Georgia Tech</td>
<td>A 28nm 1.80Mb/mm2 Digital/Analog Hybrid SRAM-CIM Macro Using 2D-Weighted Capacitor Array for Complex Number Mac Operations</td>
<td>Hybrid DCIM/ACIM SRAM; lightweight correction schemes; complex CIM-SRAM units</td>
<td>2</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>GWU</td>
<td>Unicorn-CIM: Uncovering the Vulnerability and Improving the Resilience of High-Precision Compute-in-Memory</td>
<td>SRAM-CIM for FP DNNs; a fault-injection framework for FP DNNs; a ECC scheme for FP DNNs</td>
<td>3</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>ISCAS</td>
<td>KAUST</td>
<td>Reconfigurable Precision INT4-8/FP8 Digital Compute-in-Memory Macro for AI Acceleration</td>
<td>parallel-input approach; mantissa parallel-alignment technique</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="sram-cim-hardware-software-co-design">SRAM CIM: Hardware-Software Co-Design<a class="headerlink" href="#sram-cim-hardware-software-co-design" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022</td>
<td>TCAD</td>
<td>NTHU</td>
<td>MARS: Multi-macro Architecture SRAM CIM-Based Accelerator with Co-designed Compressed Neural Networks</td>
<td>sparsity algorithm designed for SRAM CiM; quantization algorithm with BN fusion</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2023</td>
<td>TCAD</td>
<td>UCSB</td>
<td>SDP: Co-Designing Algorithm, Dataflow, and Architecture for In-SRAM Sparse NN Acceleration</td>
<td>double-broadcast hybridgrained pruning method; bit-serial booth inSRAM (BBS) multiplication dataflow</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>TCAD</td>
<td>BUAA</td>
<td>DDC-PIM: Efficient Algorithm/Architecture Co-Design for Doubling Data Capacity of SRAM-Based Processing-in-Memory</td>
<td>doubling the equivalent data capacity of SRAM-based PIM; FCC algorithm to obtain bitwise complementary filters</td>
<td>4</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>TCASAI</td>
<td>Purdue</td>
<td>Algorithm Hardware Co-Design for ADC-Less Compute In-Memory Accelerator</td>
<td>reduce ADC overhead in analog CiM architectures; Quantization-Aware Training; Partial Sum Quantization; ADC-Less hybrid analog-digital CiM hardware architecture HCiM</td>
<td>3</td>
<td>3</td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>TCAD</td>
<td>BUAA</td>
<td>Efficient SRAM-PIM Co-design by Joint Exploration of Value-Level and Bit-Level Sparsity</td>
<td>hybrid-grained pruning algorithm; customized Dyadic Block PIM (DB-PIM) architecture</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="sram-cim-simulator-modeling">SRAM CIM: Simulator &amp; Modeling<a class="headerlink" href="#sram-cim-simulator-modeling" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2020</td>
<td>ISCAS</td>
<td>JCU</td>
<td>MemTorch: A Simulation Framework for Deep Memristive Cross-Bar Architectures</td>
<td>supports both GPUs and CPUs; integrates directly with PyTorch; simulate non-idealities of memristive devices within cross-bar, tested on VGG-16, CIFAR-10</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2021</td>
<td>TCAD</td>
<td>Geogia Tech</td>
<td>DNN+NeuroSim V2.0: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators for On-Chip Training</td>
<td>non-ideal device properties of NVMS' effect for on-chip training</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>DAC</td>
<td>BUAA</td>
<td>CIMFlow: An Integrated Framework for Systematic Design and Evaluation of Digital CIM Architectures</td>
<td>workflow for implementing and evaluating DNN workloads on digital CIM architectures; CIM-specific ISA design; compilation flow built on the MLIR infrastructure</td>
<td>4</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="sram-cim-transformer-accelerator">SRAM CIM: Transformer Accelerator<a class="headerlink" href="#sram-cim-transformer-accelerator" title="Permanent link">&para;</a></h4>
<p>Challenge: Transformer architecture is widely used in NLP and CV tasks. Existing SRAM CIM architectures are not suitable for transformer acceleration.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>DATE</td>
<td>PKU</td>
<td>Leveraging Compute-in-Memory for Efficient Generative Model Inference in TPUs</td>
<td>architecture model and simulator for CIM-based TPUs; designed for LLM inference</td>
<td>4</td>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>2023</td>
<td>arXiv</td>
<td>Keio</td>
<td>An 818-TOPS/W CSNR-31dB SQNR-45dB 10-bit Capacitor-Reconfiguring Computing-in-Memory Macro with Software-Analog Co-Design for Transformers</td>
<td>Capacitor-Reconfiguring analog CIM architecture</td>
<td>1</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>Purdue</td>
<td>Hardware-Software Co-Design for Accelerating Transformer Inference Leveraging Compute-in-Memory</td>
<td>SRAM based softmax-friendly CIM architecture for transformer; finer-granularity pipelining strategy</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>PKU</td>
<td>Leveraging Compute-in-Memory for Efficient Generative Model Inference in TPUs</td>
<td>Energy-efficient CIM core integration in TPUs (replace the original MXU); CIM-MXU with systolic data path; Array dimension scaling for CIM-MXU; Area-efficient CIM macro design; Mapping engine for generative model inference</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>JSSC</td>
<td>THU</td>
<td>MulTCIM: Digital Computing-in-Memory-Based Multimodal Transformer Accelerator With Attention-Token-Bit Hybrid Sparsity</td>
<td>long reuse elimination scheduler (LRES) to dynamically reshape the attention matrix; runtime token pruner (RTP) to remove insignificant tokens; modal-adaptive CIM network (MACN) to dynamically divide CIM cores into Pipeline; effective-bits-balanced CIM (EBBCIM) macro architecture</td>
<td>5</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="cim-rram">CIM: RRAM<a class="headerlink" href="#cim-rram" title="Permanent link">&para;</a></h3>
<p>Challenge: RRAM devices are non-volatile and have high density; suitable for CIM applications. However; RRAM devices have non-ideal effects that can cause significant performance degradation.</p>
<h4 id="rram-cim-simulator">RRAM CiM: Simulator<a class="headerlink" href="#rram-cim-simulator" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2018</td>
<td>TCAD</td>
<td>THU</td>
<td>MNSIM: Simulation Platform for Memristor-Based Neuromorphic Computing System</td>
<td>reference design for largescale neuromorphic accelerator and can also be customized; behavior-level computing accuracy model</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>TCAD</td>
<td>THU</td>
<td>MNSIM 2.0: A Behavior-Level Modeling Tool for Processing-In-Memory Architectures</td>
<td>integrated PIM-oriented NN model training and quantization flow; unified PIM memory array model; support for mixed-precision NN operations</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>DATE</td>
<td>UCAS</td>
<td>PIMSIM-NN: An ISA-based Simulation Framework for Processing-in-Memory Accelerators</td>
<td>event-driven simulation approach; can evaluate the optimizations of software and hardware independently</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-architecture">RRAM CiM: Architecture<a class="headerlink" href="#rram-cim-architecture" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2019</td>
<td>ASPLOS</td>
<td>Purdue &amp; HP</td>
<td>PUMA: A Programmable Ultra-efficient Memristor-based Accelerator for Machine Learning Inference</td>
<td>Programmable and general-purpose ReRAM based ML Accelerator; Supports an instruction set; Has potential for DNN training; Provides simulator that accepts model</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2018</td>
<td>ICRC</td>
<td>Purdue &amp; HP</td>
<td>Hardware-Software Co-Design for an Analog-Digital Accelerator for Machine Learning</td>
<td>compiler to translate model to ISA; ONNX interpreter to support models in common DL frame work; simulator to evaluate performance</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>NANOARCH</td>
<td>HUST</td>
<td>Heterogeneous Instruction Set Architecture for RRAM-enabled In-memory Computing</td>
<td>General ISA for RRAM CiM &amp; digital heterogeneous architecture; a tile-processing unit-array three-level architecture</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>VLSI-SoC</td>
<td>RWTH Aachen University</td>
<td>Architecture-Compiler Co-design for ReRAM-Based Multi-core CIM Architectures</td>
<td>inference latency predictions and analysis of the crossbar utilization for CNN</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>CAS</td>
<td>A Fully Hardware Implemented Accelerator Design in ReRAM Analog Computing without ADCs</td>
<td>Based on Stochastic Binary Neural Networks; Winner-Take-All (WTA) strategy; Hardware implemented sigmoid and softmax</td>
<td>4</td>
<td>3</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-architecture-optimization">RRAM CiM: Architecture optimization<a class="headerlink" href="#rram-cim-architecture-optimization" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>ISCA</td>
<td>MIT</td>
<td>RAELLA: Reforming the Arithmetic for Efficient, Low-Resolution, and Low-Loss Analog PIM: No Retraining Required!</td>
<td>per-layer optimized weight slicing; center+offset weight encoding</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2023</td>
<td>TETCI</td>
<td>TU Delft</td>
<td>Accurate and Energy-Efficient Bit-Slicing for RRAM-Based Neural Networks</td>
<td>unbalanced bit-slicing scheme for higher accuracy; holistic solution using 2's compliment</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>MICRO</td>
<td>HUST</td>
<td>DRCTL: A Disorder-Resistant Computation  Translation Layer Enhancing the Lifetime and  Performance of Memristive CIM Architecture</td>
<td>address conversion method for dynamic scheduling; hierarchical wear-leveling (HWL) strategy for reliability improvement; data layout-aware selective remapping (LASR) to improve communication locality and reduce latency</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>TC</td>
<td>SJTU</td>
<td>ERA-BS: Boosting the Efficiency of ReRAM-Based  PIM Accelerator With Fine-Grained  Bit-Level Sparsity</td>
<td>bit-level sparsity in both weights and activations; bit-flip scheme; dynamic activation sparsity exploitation scheme</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>Science</td>
<td>USC</td>
<td>Programming memristor arrays with arbitrarily high precision for analog computing</td>
<td>represent high-precision numbers using multiple relatively low-precision analog devices;using RRAM CIM to solve PDEs</td>
<td>5</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-design-space-exploration">RRAM CiM: Design Space Exploration<a class="headerlink" href="#rram-cim-design-space-exploration" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>RWTH Aachen</td>
<td>Optimizing Binary and Ternary Neural Network Inference on RRAM Crossbars using CIM-Explorer</td>
<td>Tensor Virtual Machine (TVM)-based compiler; implementation of different mapping techniques; DSE flow to analyze the impact of parameters</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-modeling">RRAM CiM: Modeling<a class="headerlink" href="#rram-cim-modeling" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>AICAS</td>
<td>RWTH Aachen University</td>
<td>A Calibratable Model for Fast Energy Estimation of MVM Operations on RRAM Crossbars</td>
<td>system energy model for MVM on ReRAM crossbars; methodology to study the effect of the selection transistor and wire parasitics in 1T1R crossbar arrays</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>arXiv</td>
<td>MIT</td>
<td>Modeling Analog-Digital-Converter Energy and Area for Compute-In-Memory Accelerator Design</td>
<td>architecture-level model that estimates ADC energy and area</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-training-optimization">RRAM CiM: Training optimization<a class="headerlink" href="#rram-cim-training-optimization" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021</td>
<td>TCAD</td>
<td>SJTU</td>
<td>ITT-RNA: Imperfection Tolerable Training for RRAM-Crossbar-Based Deep Neural-Network Accelerator</td>
<td>prevent the large-weight synapses from being mapped to the imperfect memristor cells; off-device training algorithm to alleviate the accumulation of errors across multiple layers; bit-wise mechanism to compensate the resistance variations</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2023</td>
<td>arXiv</td>
<td>UND</td>
<td>U-SWIM: Universal Selective Write-Verify for Computing-in-Memory Neural Accelerators</td>
<td>only do write-verify for important weights; based on weight second derivatives as a guide</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2023</td>
<td>Adv. Mater.</td>
<td>UMich</td>
<td>BulkSwitching MemristorBased ComputeInMemory Module for Deep Neural Network Training</td>
<td>Bulk-ReRAM based digital-CIM hybrid architecture for training; CIM for forward, digital for backward</td>
<td>4</td>
<td>4</td>
<td>1</td>
</tr>
<tr>
<td>2024</td>
<td>APIN</td>
<td>SWU</td>
<td>Multi-optimization scheme for in-situ training of memristor neural network based on contrastive learning</td>
<td>optimizations to the deployment method, loss function and gradient calculation; compensation measures for non-ideal effects</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>TNNLS</td>
<td>SNU</td>
<td>Efficient Hybrid Training Method for Neuromorphic Hardware Using Analog Nonvolatile Memory</td>
<td>Hybrid offline-online training method</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-float-point-processing">RRAM CiM: Float-Point processing<a class="headerlink" href="#rram-cim-float-point-processing" title="Permanent link">&para;</a></h4>
<p>Challenge: Raw RRAM devices are not suitable for floating-point operations; while floating point data is common in DNNs (e.g. FP32).</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>SC</td>
<td>UCLA</td>
<td>ReFloat: Low-Cost Floating-Point Processing in ReRAM for Accelerating Iterative Linear Solvers</td>
<td>data format and accelerator architecture</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>DATE</td>
<td>UESTC</td>
<td>AFPR-CIM: An Analog-Domain Floating-Point RRAM -based Compute- In- Memory Architecture with Dynamic Range Adaptive FP-ADC</td>
<td>all-analog domain CIM architecture for FP8 calculations; adaptive dynamic range FP-ADC &amp; FP-DAC</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>GWU</td>
<td>A Hybrid-Domain Floating-Point Compute-in-Memory Architecture for Efficient Acceleration of High-Precision Deep Neural Networks</td>
<td>SRAM based hybrid-domain FP CIM architecture; detailed circuit schematics and physical layouts</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-convolutional-layer">RRAM CiM: Convolutional Layer<a class="headerlink" href="#rram-cim-convolutional-layer" title="Permanent link">&para;</a></h4>
<p>Challenge: Convolutional layer is the most compute-intensive layer in CNNs. RRAM CIM architecture is quite suitable for convolutional layer operations but face challenges related to non-ideal effects and performance degradation.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2020</td>
<td>Nature</td>
<td>THU</td>
<td>Fully hardware-implemented memristor convolutional neural network</td>
<td>fabrication of high-yield, high-performance and uniform memristor crossbar arrays; hybrid-training method; replication of multiple identical kernels for processing different inputs in parallel</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2019</td>
<td>TED</td>
<td>PKU</td>
<td>Convolutional Neural Networks Based on RRAM Devices for Image Recognition and Online Learning Tasks</td>
<td>RRAM-based hardware implementation of CNN; expand kernel to the size of image</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>TVLSI</td>
<td>NBU</td>
<td>A 578-TOPS/W RRAM-Based Binary Convolutional Neural Network Macro for Tiny AI Edge Devices</td>
<td>ReRAM XNOR cell; BCNN CIM macro with FPGA as the control core</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h5 id="rram-cim-mapping-for-cnn">RRAM CiM: Mapping for CNN<a class="headerlink" href="#rram-cim-mapping-for-cnn" title="Permanent link">&para;</a></h5>
<p>Challenge: Efficient mapping of CNN layers onto RRAM CIM architecture is crucial for performance.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2020</td>
<td>TCAS-I</td>
<td>Georgia Tech</td>
<td>Optimizing Weight Mapping and Data Flow for Convolutional Neural Networks on Processing-in-Memory Architectures</td>
<td>weight mapping to avoid multiple access to input; pipeline architecture for conv layer calculation</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2021</td>
<td>TCAD</td>
<td>SJTU</td>
<td>Efficient and Robust RRAM-Based Convolutional Weight Mapping With Shifted and Duplicated Kernel</td>
<td>shift and duplicate kernel (SDK) convolutional weight mapping architecture; parallel-window size allocation algorithm; kernel synchronization method</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>VLSI-SoC</td>
<td>Aachen</td>
<td>Mapping of CNNs on multi-core RRAM-based CIM architectures</td>
<td>architecture optimized for communication; compiler algorithms for conv2D layer; cycle-accurate simulator</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>TODAES</td>
<td>UCAS</td>
<td>Mathematical Framework for Optimizing Crossbar Allocation for ReRAM-based CNN Accelerators</td>
<td>formulate a crossbar allocation problem for ReRAM-based CNN accelerators; dynamic programming based solver; models the performance considering allocation problem</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>IEEE Access</td>
<td>UTehran</td>
<td>SCiMA: A Systolic CiM-Based Accelerator With a New Weight Mapping for CNNsA Virtual Framework Approach</td>
<td>kernel-major inter-crossbar weight mapping (KM-InterCWM) for convolution layers; structured pruning techniques; system-level virtual framework</td>
<td>4</td>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-transformer-accelerator">RRAM CIM: Transformer Accelerator<a class="headerlink" href="#rram-cim-transformer-accelerator" title="Permanent link">&para;</a></h4>
<p>Challenge: RRAM's cross-bar architecture is suitable for matrix operations.</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>VLSI</td>
<td>Purdue</td>
<td>X-Former: In-Memory Acceleration of Transformers</td>
<td>in-memory accelerate attention layers; intralayer sequence blocking dataflow; provides a simulator</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>TODAES</td>
<td>HUST</td>
<td>A Cascaded ReRAM-based Crossbar Architecture for Transformer Neural Network Acceleration</td>
<td>cascaded crossbar arrays that uses transimpedance amplifiers; data mapping scheme to store signed operands; ADC virtualization scheme</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>VLSI</td>
<td>HUST</td>
<td>An RRAM-Based Computing-in-Memory Architecture and Its Application in Accelerating Transformer Inference</td>
<td>RRAM-based in-memory floating-point computation architecture (RIME); pipelined implementations of MatMul and softmax</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2020</td>
<td>ICCAD</td>
<td>Duke</td>
<td>ReTransformer: ReRAM-based processing-in-memory architecture for transformer acceleration</td>
<td>MatMul does matrix decomposition in scaled dot-product attention; in-memory logic techniques for softmax; sub-matrix pipeline</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2022</td>
<td>TCAD</td>
<td>KAIST</td>
<td>A Framework for Accelerating Transformer-Based Language Model on ReRAM-Based Architecture</td>
<td>window self-attention and window-size search algorithm; ReRAM hardware design optimized for this algorithm</td>
<td>4</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>2020</td>
<td>ICCD</td>
<td>LSU</td>
<td>ATT: A Fault-Tolerant ReRAM Accelerator for Attention-based Neural Networks</td>
<td>ReRAM-based accelerator with pipeline for AttNNs; heuristic redundancy algorithm</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>ISCA</td>
<td>UCSD</td>
<td>Hybrid SLC-MLC RRAM Mixed-Signal Processing-in-Memory Architecture for Transformer Acceleration via Gradient Redistribution</td>
<td>architectural and circuit-level hardware designs supporting importance-based data flow with hybrid SLC-MLC ReRAM; gradient redistribution technique</td>
<td>3</td>
<td>2</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-special-usage">RRAM CiM: Special Usage<a class="headerlink" href="#rram-cim-special-usage" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>GLSVLSI</td>
<td>Yale</td>
<td>Examining the Role and Limits of Batchnorm Optimization to Mitigate Diverse Hardware-noise in In-memory Computing</td>
<td>non-idealities; circuit-level parasitic resistances and device-level non-idealities; crossbar-aware fine-tuning of batchnorm parameters</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2019</td>
<td>ASPDAC</td>
<td>POSTECH</td>
<td>In-memory batch-normalization for resistive memory based binary neural network hardware</td>
<td>in-memory batchnormalization schemes; integrate BN layers on crossbar</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>TRETS</td>
<td>UFRGS</td>
<td>Reprogrammable Non-Linear Circuits Using ReRAM for NN Accelerators</td>
<td>perform typical non-linear operations using ReRAM</td>
<td>4</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2019</td>
<td>Adv. Funct. Mater.</td>
<td>HUST</td>
<td>Functional Demonstration of a Memristive Arithmetic Logic Unit (MemALU) for InMemory Computing</td>
<td>non-volatile Boolean logic using RRAM crossbar;reconfigurable boolean logic gates</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="rram-cim-matrix-equation-solver">RRAM CiM: Matrix Equation Solver<a class="headerlink" href="#rram-cim-matrix-equation-solver" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>DATE</td>
<td>PKU</td>
<td>BlockAMC: Scalable In-Memory Analog Matrix Computing for Solving Linear Systems</td>
<td>Novel scalable algorithm for matrix equation solving; reconfigurable BlockAMC macros design</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>Sci.Adv.</td>
<td>HUST</td>
<td>Fully analog iteration for solving matrix equations with in-memory computing</td>
<td>Analog Iteration with Digital Refinement solver</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>2025</td>
<td>Nat.Elec.</td>
<td>PKU</td>
<td>Precise and scalable analogue matrix equation solving using resistive random-access memory chips</td>
<td>Mixed-Precision Iterative Algorithm for High-Precision Analogue Computing; Scalable Hardware Implementation with BlockAMC algorithm</td>
<td>3</td>
<td>5</td>
<td>4</td>
</tr>
</tbody>
</table>
<h3 id="cim-hybrid-architecture">CIM: Hybrid Architecture<a class="headerlink" href="#cim-hybrid-architecture" title="Permanent link">&para;</a></h3>
<p>Solution: Use hybrid architecture (like SRAM + RRAM) to overcome the limitations of single device (e.g. RRAM's non-ideal effects).</p>
<h4 id="hybrid-cim-sram-general-logic">Hybrid CIM: SRAM + General Logic<a class="headerlink" href="#hybrid-cim-sram-general-logic" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>GLSVLSI</td>
<td>USC</td>
<td>Heterogeneous Integration of In-Memory Analog Computing Architectures with Tensor Processing Units</td>
<td>hybrid TPU-IMAC architecture; TPU for conv, CIM for fc</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>ASPLOS</td>
<td>CAS</td>
<td>PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System</td>
<td>dynamic parallelism-aware task scheduling for llm decoding; online kernel characterization for heterogeneous architectures; hybrid PIM units for compute-bound and memory-bound kernels</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="hybrid-cim-sram-rram">Hybrid CIM: SRAM + RRAM<a class="headerlink" href="#hybrid-cim-sram-rram" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2024</td>
<td>Science</td>
<td>NTHU</td>
<td>Fusion of memristor and digital compute-in-memory processing for energy-efficient edge computing</td>
<td>Fusion of ReRAM and SRAM CiM; ReRAM SLC &amp; MLC Hybrid; Current quantization; Weight shifting with compensation</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>IPDPS</td>
<td>Georgia Tech</td>
<td>Harmonica: Hybrid Accelerator to Overcome Imperfections of Mixed-signal DNN Accelerators</td>
<td>select and transfer imperfectionsensitive weights to digital accelerator; hybrid quantization(weights on analog part is more quantized)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>ICCAD</td>
<td>SJTU</td>
<td>TL-nvSRAM-CIM: Ultra-High-Density Three-Level ReRAM-Assisted Computing-in-nvSRAM with DC-Power Free Restore and Ternary MAC Operations</td>
<td>DCpower-free weight-restore from ReRAM; ternary SRAM-CIM mechanism with differential computing scheme</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="hybrid-cim-memristormram-sram">Hybrid CIM: Memristor/MRAM + SRAM<a class="headerlink" href="#hybrid-cim-memristormram-sram" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>Nature</td>
<td>TSMC</td>
<td>A mixed-precision memristor and SRAM compute-in-memory AI processor</td>
<td>layer based INT-FP hybrid architure; kernel-based mix-CIM (SRAM/ReRAM/digital hybrid architecture)</td>
<td>5</td>
<td>5</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>DAC</td>
<td>Chung-Ang Univ.</td>
<td>HH-PIM: Dynamic Optimization of Power and Performance with Heterogeneous-Hybrid PIM for Edge AI Devices</td>
<td>heterogeneous-hybrid PIM with HP/LP modules and MRAM/SRAM; dynamic data placement algorithm for energy optimization; dual PIM controller design</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>AaltoU</td>
<td>Acore-CIM: build accurate and reliable mixed-signal CIM cores with RISC-V controlled self-calibration</td>
<td>reliability-focused MAC cell; proof-of-concept SoC composed of a CIM core and a RISC-V control processor; automated Built-In Self-Calibration (BISC) routine</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
</tbody>
</table>
<h4 id="hybrid-cim-analog-digital">Hybrid CIM: Analog + Digital<a class="headerlink" href="#hybrid-cim-analog-digital" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>arXiv</td>
<td>HP</td>
<td>RACE-IT: A Reconfigurable Analog CAM-Crossbar Engine for In-Memory Transformer Acceleration</td>
<td>Compute Analog Content Addressable Memory (Compute-ACAM) structure; accelerator based on crossbars and Compute-ACAMs; encoding-based optimization</td>
<td>3</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2024</td>
<td>VLSI</td>
<td>FDU</td>
<td>HARDSEA: Hybrid Analog-ReRAM Clustering and Digital-SRAM In-Memory Computing Accelerator for Dynamic Sparse Self-Attention in Transformer</td>
<td>product-quantization-based sparse self-attention algorithm; ADC-free ReRAM-CIM macro; ReRAM-CIM for front-end attention sparsification, SRAM-CIM for back-end sparse attention</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2024</td>
<td>ASP-DAC</td>
<td>Keio</td>
<td>OSA-HCIM: On-The-Fly Saliency-Aware Hybrid SRAM CIM with Dynamic Precision Configuration</td>
<td>On-the-fly Saliency-Aware precision configuration scheme; Hybrid CIM Array for DCIM and ACIM using split-port SRAM</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>South Carolina</td>
<td>PIM-LLM: A High-Throughput Hybrid PIM Architecture for 1-bit LLMs</td>
<td>hybrid PIM-Digital architecture; analog PIM for low-precision MatMul; digital systolic array for high-precision matMul</td>
<td>4</td>
<td>3</td>
<td>1</td>
</tr>
<tr>
<td>2024</td>
<td>ESSERC</td>
<td>UCSD</td>
<td>An Analog and Digital Hybrid Attention Accelerator for Transformers with Charge-based In-memory Computing</td>
<td>analog CIM for low-score tokens, digital processor for high</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="cim-quantization">CIM: Quantization<a class="headerlink" href="#cim-quantization" title="Permanent link">&para;</a></h3>
<p>Challenge: Limited by the precision &amp; area &amp; power trade-off of the ADC; certain CIM devices like RRAM are not suitable for high-precision computation (e.g. FP32). Quantization is needed to reduce the precision of the data.</p>
<h4 id="cim-quantization-partial-sum-quantization">CIM: Quantization: Partial Sum Quantization<a class="headerlink" href="#cim-quantization-partial-sum-quantization" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>ISLPED</td>
<td>Purdue</td>
<td>Partial-Sum Quantization for Near ADC-Less Compute-In-Memory Accelerators</td>
<td>ADC-Less and near ADC-Less CiM accelerators; CiM hardware aware DNN quantization methodology</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>DATE</td>
<td>SKKU</td>
<td>Column-wise Quantization of Weights and Partial Sums for Accurate and Efficient Compute-In-Memory Accelerators</td>
<td>granularity alignment logic; learned step-size quantization(LSQ)</td>
<td>3</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="cim-quantization-for-analog-cim">CIM Quantization: For Analog CIM<a class="headerlink" href="#cim-quantization-for-analog-cim" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022</td>
<td>TCAS-I</td>
<td>Georgia Tech</td>
<td>BitS-Net: Bit-Sparse Deep Neural Network for Energy-Efficient RRAM-Based Compute-In-Memory</td>
<td>bit-sparsity quantization; bias-shifted MVM; hardware-aware loss function</td>
<td>3</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>2023</td>
<td>AICAS</td>
<td>TU Delft</td>
<td>Mapping-aware Biased Training for Accurate Memristor-based Neural Networks</td>
<td>favorability constraint analysis to find important weight values; mapping-aware biased training to restrict weight values to low variance RRAM states</td>
<td>3</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>TCAD</td>
<td>BUAA</td>
<td>CIMQ: A Hardware-Efficient Quantization Framework for Computing-In-Memory-Based Neural Network Accelerators</td>
<td>bit-level sparsity induced activation quantization; quantizing partial sums to decrease required resolution of ADCs; arraywise quantization granularity</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>TCAD</td>
<td>BUAA</td>
<td>CIMPQ: An Arraywise and Hardware-Friendly Mixed Precision Quantization Method for Analog Computing-In-Memory</td>
<td>mixed precision quantization method based on evolutionary algorithm; arraywise quantization granularity; evaluation method to obtain the performance of strategy on the CIM</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2024</td>
<td>ICCAD</td>
<td>TU Delft</td>
<td>Hardware-Aware Quantization for Accurate Memristor-Based Neural Networks</td>
<td>analysis of fixed-point quantization impact on conductance variation; weight quantization tuning technique; approach to reduce the residual error</td>
<td>3</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<h4 id="cim-quantization-for-all-cim">CIM Quantization: For all CIM<a class="headerlink" href="#cim-quantization-for-all-cim" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2018</td>
<td>CVPR</td>
<td>Google</td>
<td>Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</td>
<td>integer-only inference arithmetic; quantizes both weights and activations as 8-bit integers, bias 32-bit; provides both quantized inference framework and training frame work</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>ICCD</td>
<td>SJTU</td>
<td>PSQ: An Automatic Search Framework for Data-Free Quantization on PIM-based Architecture</td>
<td>post-training quantization framework without retraining; hardware-aware block reassembly</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>arXiv</td>
<td>UHK</td>
<td>Binary Weight Multi-Bit Activation Quantization for Compute-in-Memory CNN Accelerators</td>
<td>a quantization framework that considers CIM's mixed-signal constraints; closed-form layer-specific weight binarization method; differentiable function for uniform multi-bit quantization</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="cim-digital-cim">CIM: Digital CIM<a class="headerlink" href="#cim-digital-cim" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025</td>
<td>ISCAS</td>
<td>CAS</td>
<td>StreamDCIM: A Tile-based Streaming Digital CIM Accelerator with Mixed-stationary Cross-forwarding Dataflow for Multimodal Transformer</td>
<td>tile-based reconfigurable CIM macro microarchitecture; mixed-stationary cross-forwarding dataflow; ping-pong-like finegrained compute-rewriting pipeline</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="nvm">NVM<a class="headerlink" href="#nvm" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2020</td>
<td>GLSVLSI</td>
<td>UND</td>
<td>Benchmarking Computing-in-Memory for Design Space Exploration</td>
<td>uniform benchmarking of CiM designs based on different memory technologies</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2024</td>
<td>ISCAS</td>
<td>UMCP</td>
<td>On-Chip Adaptation for Reducing Mismatch in Analog Non-Volatile Device Based Neural Networks</td>
<td>float-gate transistors based; hot-electron injection to address the issue of mismatch and variation</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2023</td>
<td>DATE</td>
<td>UniBo</td>
<td>End-to-End DNN Inference on a Massively Parallel Analog In Memory Computing Architecture</td>
<td>many-core heterogeneous architecture; general-purpose system based on RISC-V cores and nvAIMC cores; based on Phase-Change Memory(PCM);</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2025</td>
<td>DAC</td>
<td>ZJU</td>
<td>VQT-CiM: Accelerating Vector Quantization Enhanced Transformer with Ferroelectric Compute-in-Memory</td>
<td>static-dynamic VMM Conversion; look-up table accelerated RVQ(Residual Vector Quantization); product vector quantization (PVQ)</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="prefetching">Prefetching<a class="headerlink" href="#prefetching" title="Permanent link">&para;</a></h3>
<p>Challenge: Speculative prefetch requests can cause undesirable effects on the system (e.g., increased memory bandwidth consumption, cache pollution, memory access interference).</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Venue</th>
<th>Authors</th>
<th>Title</th>
<th>Tags</th>
<th>P</th>
<th>E</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021</td>
<td>MICRO</td>
<td>ETHZ</td>
<td>Pythia: A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning</td>
<td>formulating prefetching as a reinforcement learning problem; holistic learning from multiple program features and system feedback; customizable prefetching objective via configuration registers</td>
<td>3</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>2025</td>
<td>MICRO</td>
<td>NUDT</td>
<td>Elevating Temporal Prefetching Through Instruction Correlation</td>
<td>critical instruction detection based on miss contribution; coverage-based classification for metadata utility; adaptive metadata cache partitioning via controller</td>
<td>3</td>
<td>4</td>
<td>4</td>
</tr>
</tbody>
</table>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="January 21, 2026 12:55:22 UTC">January 21, 2026</span>
  </span>

    
    
    
    
  </aside>


  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.instant", "navigation.instant.progress", "navigation.tracking", "navigation.sections", "navigation.expand", "navigation.indexes", "toc.follow", "navigation.top", "content.action.edit", "content.action.view", "search.highlight", "search.share", "search.suggest"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
    
  </body>
</html>