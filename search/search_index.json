{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lemonade Review Guidelines","text":""},{"location":"#about","title":"About","text":"<p>The New Lemonade Review is a paper review list maintained by prospective and current PhD students @pku-lemonade/phds at LEMONADE, Peking University. It is intended to provide a comprehensive overview of the quintessential research ideas in the field of computer architecture and systems.</p>"},{"location":"#how-to-find-the-papers","title":"How to find the papers?","text":"<p>Important</p> <p>Choose one of the first three strategies as your focus each week, depending on your current research needs.</p> <p>Important</p> <p>Check arXiv every day to make it a regular habit!</p>"},{"location":"#broad-search","title":"Broad Search","text":"<ul> <li>Goal: Get a broad overview of the topic.</li> <li>Method: Search in Google Scholar or read survey papers.</li> <li>Pace: 5+ papers/week (mostly skimming).</li> </ul>"},{"location":"#author-focused-search","title":"Author-Focused Search","text":"<ul> <li>Goal: Understand a key researcher's approach (methods, presentation, evaluation).</li> <li>Method: Search the author's papers in DBLP.</li> <li>Pace: 2-5 papers/week (skim first, then carefully read relevant papers).</li> </ul>"},{"location":"#citation-chasing","title":"Citation Chasing","text":"<ul> <li>Goal: Deep dive into a core paper (e.g., your intended main baseline).</li> <li>Method: Follow its main baselines/references (backward look) and check papers that cite it (forward look).</li> <li>Pace: 2-3 papers/week (requires careful reading).</li> <li>[!NOTE] Aim for at least 2 papers/week as you typically need to read the core paper alongside its main baseline.</li> </ul>"},{"location":"#arxiv-monitoring","title":"arXiv Monitoring","text":"<ul> <li>Goal: Stay updated with the latest research developments</li> <li>Method: Try AI immersive reading with Cool papers in the following categories:</li> <li>Distributed, Parallel, and Cluster Computing</li> <li>Emerging Technologies</li> <li>Hardware Architecture</li> <li>Performance</li> <li>Pace: 0+ papers/week (mostly skimming).</li> </ul>"},{"location":"#what-papers-should-be-included","title":"What papers should be included?","text":"<ul> <li>Peer-reviewed full papers ONLY.</li> <li>NO workshop papers, posters, or abstracts.</li> <li>Preprints are okay only for LLM-related topics.</li> </ul> <p>Important</p> <p>Ensure that at least half the papers are either published in CCF Rank A venues, or have a relavance score at least 3.</p>"},{"location":"#how-to-organize-the-papers","title":"How to organize the papers?","text":""},{"location":"#format","title":"Format","text":"<ul> <li>Sort papers by year in each subsection.</li> <li>CSV: Use quotes (\" \") around titles and tags containing commas \",\"</li> <li>PR Title/Description: Include search strategy, keywords, and note any papers read carefully (as opposed to skimmed).</li> </ul>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Use the affiliation of the corresponding author (or last author).</li> <li>Use standard, globally recognized abbreviations for affiliations.</li> </ul>"},{"location":"#subsections","title":"Subsections","text":"<ul> <li>Put papers only in Level 3 or Level 4 subsections.</li> <li>Group papers by a common challenge in each subsection. Briefly state that challenge at the beginning of the subsection.</li> <li>Min 2 papers per subsection. If only one fits, find a partner paper, or connect this paper to a related subsection.</li> <li>Max 5 papers per subsection. If more fit,  split the subsection.</li> </ul>"},{"location":"#tags","title":"Tags","text":"<ul> <li>Use specific tags for key techniques or contributions, such as \"xxx algorithm\", \"yyy model\", \"zzz architecture\", etc.</li> <li>Don't use broad area tags (\"performance\" or \"architecture\").</li> <li>Don't use vague feature tags (\"expresiveness\", \"scalability\", \"efficiency\")unless combined with a specific technique (e.g., \"xxx algorithm for scalability\").</li> <li>Explain potentially unclear acronyms (\"XYZ framework\") in tags.</li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>Link related subsections (e.g., link hardware papers to related software papers sharing an idea, link compiler papers to related system papers sharing a technique).</li> </ul>"},{"location":"#how-to-read-the-papers-review-scores","title":"How to read the papers? (Review Scores)","text":""},{"location":"#presentation","title":"Presentation","text":"<p>Tip</p> <p>Skimming: Title, Abstract, Introduction, Figures.</p> <ul> <li>5: Definitely stealing some ideas for how they explained things or made their figures. I wish my papers looked this good.</li> <li>4: I will take note a figure or explanation and use in my next paper.</li> <li>3: I can write as good as it.</li> <li>2: Kind of a pain to read, or the figures are confusing/ugly. Hard to tell what's going on easily.</li> </ul>"},{"location":"#evaluation","title":"Evaluation","text":"<p>Tip</p> <p>Skimming: Experiments, Results, Reproducibility.</p> <ul> <li>5: Solid real-world tests. NVDA could actually use or bet on.</li> <li>4: Solid tests with real hardware. But the setup is not real-wrold ready.</li> <li>3: Okay tests with open source simulators. The results aren't obviously wrong.</li> <li>2: Okay tests with close source simulators. The results aren't obviously wrong.</li> <li>1: The results just don't make sense.</li> </ul>"},{"location":"#novelty","title":"Novelty","text":"<p>Note</p> <p>Requires careful reading: Assessing novelty means comparing the paper critically to related work. If you only skimmed the paper, either skip this score or assign at most 2.</p> <ul> <li>5: Totally new idea; groundbreaking.</li> <li>4: New take on existing ideas, or combines them smartly.</li> <li>3: An existing idea applied to a new area/topic.</li> <li>2: An existing idea applied to the same problem with standard incremental contributions.</li> <li>1: An existing idea applied to the same problem with minor variations.</li> </ul>"},{"location":"#relevance","title":"Relevance","text":"<p>Note</p> <p>Don't put the score in the review record because this score should be personal.</p> <ul> <li>5: Everyone need to read this paper</li> <li>4: I will put this paper on my side when I write my next paper</li> <li>3: I will cite this paper in my next paper</li> <li>2: I will cite this paper in my next survey paper</li> </ul>"},{"location":"hardware/","title":"Hardware","text":"<p>For each paper, identify its primary contribution and assign it to the single most appropriate category below.</p>"},{"location":"hardware/#processor-microarchitecture","title":"Processor Microarchitecture","text":"<p>Focuses on a single processing core and its components. Includes:</p> <ul> <li>Instruction set design</li> <li>Branch prediction</li> <li>Other core-level components and techniques</li> </ul>"},{"location":"hardware/#parallel-and-multi-processor-architecture","title":"Parallel and Multi-Processor Architecture","text":"<p>Covers systems with multiple processing units and their interactions. Includes:</p> <ul> <li>Multi-core processor architecture</li> <li>Many-core processor architecture</li> <li>GPU architecture</li> <li>Cache coherence protocols</li> <li>Memory consistency models</li> <li>System-level integration techniques such as:<ul> <li>Chiplets</li> <li>3D stacking</li> </ul> </li> </ul>"},{"location":"hardware/#memory-architecture","title":"Memory Architecture","text":"<p>Concerns memory subsystems and their interactions. Includes:</p> <ul> <li>Memory hierarchy design</li> <li>\u5b58\u7b97<ul> <li>Processing-in-Memory (PIM)</li> <li>Processing-Near-Memory (PNM)</li> <li>Computation-in-Memory (CIM)</li> </ul> </li> </ul>"},{"location":"hardware/#interconnection-networks","title":"Interconnection Networks","text":"<p>Addresses the communication fabric. Includes:</p> <ul> <li>Network-on-Chip (NoC) - topology, routing, flow control</li> <li>Note: Focus should be on the network itself, not primarily on using it to build a parallel architecture. Most CXL papers likely belong in other categories unless the primary contribution is the CXL protocol/architecture itself.</li> </ul>"},{"location":"hardware/#domain-specific-accelerators","title":"Domain-Specific Accelerators","text":"<p>Concerns accelerators tailored for specific applications. Includes:</p> <ul> <li>Reconfigurable architectures like FPGA/CGRA</li> <li>Accelerators for domains such as AI, graph processing, bioinformatics, etc.</li> </ul>"},{"location":"hardware/#security-and-reliability","title":"Security and Reliability","text":"<p>Covers hardware mechanisms for trust and resilience. Includes:</p> <ul> <li>Side-channel countermeasures (hardware aspects)</li> <li>Fault detection/mitigation hardware</li> </ul>"},{"location":"hardware/#emerging-technologies","title":"Emerging Technologies","text":"<p>Explores architectures based on novel technologies or paradigms. Includes:</p> <ul> <li>Quantum computing architectures</li> <li>Photonic computing architectures</li> <li>Note: CIM should be grouped under Memory Architecture.</li> </ul> <p>Error processing data file 'data/hardware/index.csv' referenced implicitly by page 'hardware/index.md'. Check build logs.</p>"},{"location":"hardware/accelerators/","title":"Domain-Specific Accelerators","text":""},{"location":"hardware/accelerators/#graph-accelerators","title":"Graph Accelerators","text":"<p>Challenge: Massive memory requirement, Non-ordered memory access</p> Year Venue Authors Title Tags P E N 2016 MICRO Princeton Graphicionado: A High-Performance and Energy-Efficient Accelerator for Graph Analytics vertex-programming based pipeline; on-chip scratchpad optimization; source/destination-oriented parallel streams 4 4 4 2019 FPL NUS On-The-Fly Parallel Data Shuffling for Graph Processing on OpenCL-Based FPGAs on-the-fly parallel data shuffling; OpenCL-based data dispatcher; runtime data dependency resolution; decoder-filter shuffling architecture 3 4 2 2020 MICRO UCR GraphPulse: An Event-Driven Hardware Accelerator for Asynchronous Graph Processing asynchronous event-driven model; in-place event coalescing; delta-based accumulative processing 3 3 4 2024 TRETS HUST ScalaBFS2: A High-performance BFS Accelerator on an HBM-enhanced FPGA Chip HBM-enhanced BFS accelerator; independent HBM Reader; hybrid-mode PE; multi-layer crossbar 3 4 3"},{"location":"hardware/accelerators/#hypergraph-accelerators","title":"Hypergraph Accelerators","text":"<p>Solution: Realize the shared parts in hyperedges</p> Year Venue Authors Title Tags P E N 2022 MICRO HUST A Data-Centric Accelerator for High-Performance Hypergraph Processing Data-Centric; Load-Trigger-Reduce (LTR); Adaptive Data Loading 4 4 3 2025 HPCA HUST MeHyper: Accelerating Hypergraph Neural Networks by Exploring Implicit Dataflows Microedge; Microedge-Centric Dataflow; RePAG Execution Model 4 3 4"},{"location":"hardware/accelerators/#dynamic-graph-accelerators","title":"Dynamic Graph Accelerators","text":"<p>Challenge: Edge update, Graph store data structure design</p> Year Venue Authors Title Tags P E N 2021 MICRO UCR JetStream: Graph Analytics on Streaming Data with Event-Driven Hardware Accelerator first streaming graph accelerator; asynchronous incremental algorithms; asynchronous edge deletion handling (VAP; DAP) 3 3 4 2022 ISCA HUST TDGraph: A Topology-Driven Accelerator for High-Performance Streaming Graph Processing topology-driven incremental execution; streaming graph processing; regularized state propagation; vertex states coalescing 4 4 3 2023 MICRO UCR MEGA Evolving Graph Accelerator first evolving graph accelerator; Batch-Oriented Execution (BOE); deletion-free based on CommonGraph; Batch Pipelining 3 3 4 2024 TRETS UoV Dynamic-ACTS - A Dynamic Graph Analytics Accelerator For HBM-Enabled FPGAs a novel edge packing format (ACTPACK); hashed edge updates; low-overhead online partitioning 4 4 3"},{"location":"hardware/accelerators/#graph-mining-accelerators","title":"Graph Mining Accelerators","text":"<p>Challenge: Complex graph algorithms, Irregular access patterns</p> Year Venue Authors Title Tags P E N 2021 ISCA MIT FlexMiner: A Pattern-Aware Accelerator for Graph Pattern Mining pattern-aware GPM accelerator; software/hardware co-design; pattern-specific execution plan; connectivity map (c-map) 4 4 3"},{"location":"hardware/accelerators/#dnn-accelerators","title":"DNN Accelerators","text":""},{"location":"hardware/accelerators/#layer-fusion-accelerators","title":"Layer Fusion Accelerators","text":"<p>Solution: Use layer fusion to combine multiple layers of a neural network into a single layer. This can help reduce the number of computations and memory accesses required during inference; leading to faster execution times and lower power consumption.</p> Year Venue Authors Title Tags P E N 2016 MICRO SBU Fused-Layer CNN Accelerators fuse the processing of multiple CNN layers by modifying the order in which the input data are brought on chip 2025 TC KU Leuven Stream: Design Space Exploration of Layer-Fused DNNs on Heterogeneous Dataflow Accelerators fine-grain mapping paradigm; mapping of layer-fused DNNs on heterogeneous dataflow accelerator architectures; memory- and communication-aware latency analysis; constraint optimization 2024 SOCC IIT Hyderabad Hardware-Aware Network Adaptation using Width and Depth Shrinking including Convolutional and Fully Connected Layer Merging Width Shrinking: reduces the number of feature maps in CNN layers; Depth Shrinking: Merge of conv layer and fc layer 2024 ICSAI MIT LoopTree: Exploring the Fused-Layer Dataflow  Accelerator Design Space design space that supports set of tiling, recomputation, retention choices, and their combinations; model that validates design space"},{"location":"hardware/accelerators/#llm-accelerators","title":"LLM Accelerators","text":"<p>Challenge: LLM accelerators face challenges in terms of memory bandwidth; power consumption; and the need for efficient data movement.</p> Year Venue Authors Title Tags P E N 2024 DATE NTU ViTA: A Highly Efficient Dataflow and Architecture for Vision Transformers highly efficient memory-centric dataflow; fused special function module for non-linear functions; A comprehensive DSE of ViTA Kernels and VMUs 2025 arXiv SJTU ROMA: A Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM hybrid ROM-SRAM architecture for on-device LLM; B-ROM design for area-efficient ROM; fused cell integration of ROM and compute unit; QLoRA rank adaptation for task-specific tuning; on-chip storage optimization for quantized models 2025 ISCA Duke Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-Aware Cache Compression entropy-aware cache compression for LLMs; group-wise non-uniform quantization; shared k-means patterns; parallel Huffman hardware decoder 4 3 3"},{"location":"hardware/accelerators/#quantized-dnn-accelerators","title":"Quantized DNN Accelerators","text":"<p>Solution: Quantized DNN accelerators are designed to efficiently execute quantized neural networks, which use lower precision representations for weights and activations.</p> Year Venue Authors Title Tags P E N 2018 ISCA SNU Energy-Efficient Neural Network Accelerator Based on Outlier-Aware Low-Precision Computation accelerator architecture for outlier-aware quantized models; outlier-aware low-precision computation; separate outlier MAC unit 4 3 2 2018 ISCA Georgia Tech Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Network accelerator for layer-aware quantized DNN; bit-flexible computation unit; block-structured instruction set architecture 4 3 3 2023 HPCA KAIST Sibia: Signed Bit-slice Architecture for Dense DNN Acceleration with Slice-level Sparsity Exploitation signed bit-slice representation;flexible zero skipping processing element 3 3 4 2024 DAC ASU Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference composite data type Logarithmic Posits (LP); automated post training LP Quantization (LPQ) Framework based on genetic algorithms; mixed-precision LP Accelerator (LPA) 3 3 2 2025 HPCA POSTECH Panacea: Novel DNN Accelerator using Accuracy-Preserving Asymmetric Quantization and Energy-Saving Bit-Slice Sparsity Asymmetrically-Quantized bit-Slice GEMM; Zero-Point Manipulation and Distribution-based Bit-Slicing to increase sparsity 3 3 4"},{"location":"hardware/accelerators/#reconfigurable-accelerators","title":"Reconfigurable Accelerators","text":"<p>Solution: Reconfigurable accelerators not only break the trade-off of flexibility and performance, but also enable hardware to adapt to algorithm changes as quickly as software while maintaining high energy efficiency.</p> Year Venue Authors Title Tags P E N 2018 ASPLOS Georgia Tech MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects augmented reduction tree(ART) for link conflict; chubby distribution tree for bandiwdth optimization; ART based virtual neuron construction 4 3 2 2019 JETCAS MIT Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices hierarchical mesh NoC for multiple transmission modes; sparse PE architecture 5 4 2 2020 HPCA Georgia Tech &amp; Intel SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training flexible dot product engine; forwarding adder network 4 3 2 2023 ASPLOS UM &amp; Georgia Tech Flexagon: A Multi-dataflow Sparse-Sparse Matrix Multiplication Accelerator for Efficient DNN Processing merger-reduction network for area efficiency; compression format conversion without hardware module; dedicated L1 memory architecture for different access pattern 4 3 2"},{"location":"hardware/accelerators/#application-specific-accelerators","title":"Application-Specific Accelerators","text":"<p>Solution: Application-specific accelerators are designed to efficiently execute specific types of neural networks or machine learning workloads, providing optimized performance and energy efficiency for targeted applications.</p> Year Venue Authors Title Tags P E N 2022 PLDI University of Edinburgh Bind the Gap: Compiling Real Software to Hardware FFT Accelerators IO-based synthesis; adapter generation; multi-level mismatch resolution; neural code classification; generate-and-test verification 4 4 3"},{"location":"hardware/accelerators/#benchmarks","title":"Benchmarks","text":"Year Venue Authors Title Tags P E N 2025 arXiv Cambridge Benchmarking Ultra-Low-Power \u00b5NPUs Comparative \u00b5NPU Benchmarking (\u00b5NPU: microcontroller-scale Neural Processing Unit); open-source model compilation framework; \u00b5NPU memory I/O bottleneck identification 4 4 2"},{"location":"hardware/accelerators/#dataflow-architecture","title":"Dataflow Architecture","text":"<p>Solution: Dataflow architecture allows the execution of instructions based on the availability of data rather than a predetermined sequence; leading to more efficient use of resources and better performance in parallel processing and real-time systems.</p> Year Venue Authors Title Tags P E N 2019 ASPLOS THU Tangram: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators buffer sharing dataflow(BSD); alternate layer loop ordering (ALLO) dataflow; heuristics spatial layer mapping algorithm 2024 MICRO CMU The TYR Dataflow Architecture: Improving Locality by Taming Parallelism local tag spaces technique; space tag managing instruction set; CT based concurrent-block communication 2024 MICRO UCR Sparsepipe: Sparse Inter-operator Dataflow Architecture with Cross-Iteration Reuse producer-consumer reuse; cross-iteration reuse; sub-tensor dependency; OEI dataflow; sparsepipe architecture 2025 arXiv UCSB FETTA: Flexible and Efficient Hardware Accelerator for Tensorized Neural Network Training contraction sequence search engine; tensor contraction unit; distribution/reduction network 3 4 3 2025 ISCA PKU H2-LLM: Hardware-Dataflow Co-Exploration for Heterogeneous Hybrid-Bonding-based Low-Batch LLM Inference operator-channel binding; computation-andwidth trade-off; dataflow-based DSE 4 3 3"},{"location":"hardware/accelerators/#data-mapping","title":"Data Mapping","text":"<p>Solution: Assign data to specific locations in memory or storage to optimize performance; reduce latency; and improve resource utilization.</p>"},{"location":"hardware/accelerators/#survey","title":"Survey","text":"Year Venue Authors Title Tags P E N 2013 DAC NUS Mapping on Multi/Many-core Systems: Survey of Current and Emerging Trends dense/run-time mapping; centralized/distributred management; hybrid mapping"},{"location":"hardware/accelerators/#heuristic-algorithm","title":"Heuristic Algorithm","text":"Year Venue Authors Title Tags P E N 2021 HPCA Georgia Tech MAGMA: An Optimization Framework for Mapping Multiple DNNs on Multiple Accelerator Cores sub-accelerator selection; fine-grained job prioritization; MANGA crossover genetic operators 2023 ISCA THU MapZero: Mapping for Coarse-grained Reconfigurable Architectures with Reinforcement Learning and Monte-Carlo Tree Search GAT based DFG and CGRA embedding; routing penalty based reinforcement learning; Monte-Carlo tree search space exploration 2023 VLSI IIT Kharagpur Application Mapping Onto Manycore Processor Architectures Using Active Search Framework RNN based active search framework; IP-Core Numbering Scheme; active search with/without pretraining"},{"location":"hardware/accelerators/#optimization-modeling","title":"Optimization Modeling","text":"Year Venue Authors Title Tags P E N 2020 FPGA ETH Zurich Flexible Communication Avoiding Matrix Multiplication on FPGA with High-Level Synthesis computation and I/O decomposition model for matrix multiplication; 1D array collapse mapping method; internal double buffering 2021 HPCA Georgia Tech Heterogeneous Dataflow Accelerators for Multi-DNN Workloads heterogeneous dataflow accelerators (HDAs) for DNN; dataflow flexibility; high utilization across the sub-accelerators 2023 MICRO Alibaba; CUHK ArchExplorer: Microarchitecture Exploration Via Bottleneck Analysis dynamic event-dependence graph(EDG); induced DEG based critical path construction; bottleneck-removal-driven DSE 2023 ISCA THU Inter-layer Scheduling Space Definition and Exploration for Tiled Accelerators inter-layer encoding method; temperal cut; spatial cut; RA tree analysis"},{"location":"hardware/accelerators/#fault-tolerant-mapping","title":"Fault Tolerant Mapping","text":"Year Venue Authors Title Tags P E N 2017 SC NIT High-performance and energy-efficient fault-tolerance core mapping in NoC weighted communication energy; placing unmapped vertices region; application core graph; spare core placement algorithm 2019 IVLSI UESTC Optimized mapping algorithm to extend lifetime of both NoC and cores in many-core system lifetime budget metric; LBC-LBL mapping algorithm; electro-migration fault model"},{"location":"hardware/accelerators/#reliability-management","title":"Reliability Management","text":"Year Venue Authors Title Tags P E N 2020 DATE Turku Thermal-Cycling-aware Dynamic Reliability Management in -Core System-on-Chip Coffin-Mason equation based reliability model; reliability-aware mapping/scheduling; dynamic power management 2024 arXiv WUSTL A Two-Level Thermal Cycling-Aware Task Mapping Technique for Reliability Management in Manycore Systems temperature based bin packing; task-to-bin assignment; thermal cycling-aware based task-to-core mapping 2024 arXiv WUSTL A Reinforcement Learning-Based Task Mapping Method to Improve the Reliability of Clustered Manycores mean time to failure; density-based spatial clustering of applications with noise algorithm"},{"location":"hardware/accelerators/#task-scheduling","title":"Task Scheduling","text":"Year Venue Authors Title Tags P E N 2023 ICCAD PKU Memory-aware Scheduling for Complex Wired Networks with Iterative Graph Optimization topology-aware pruning algorithm; integer linear programming scheduling method; sub-graph fusion algorithm ; memory-aware graph partitioning 2023 MICRO Duke Si-Kintsugi: Towards Recovering Golden-Like Performance of Defective Many-Core Spatial Architectures for AI graph alignment algorithm for dataflow graph and platform pe grap; producer-consumer pattern dataflow generation algorithm"},{"location":"hardware/accelerators/#many-core-architecture","title":"Many-core Architecture","text":"<p>Challenge: Many-core architectures are designed to handle a large number of cores; but they face challenges in terms of power consumption; performance; and resource allocation.</p>"},{"location":"hardware/accelerators/#resource-management","title":"Resource Management","text":"<p>Challenge: Cores share resources with each other, how to achieve high performance by coordinating access among cores to prevent conflicts and ensure data consistency is a problem.</p> Year Venue Authors Title Tags P E N 2015 HPCA Cornel Increasing Multicore System Efficiency through Intelligent Bandwidth Shifting online bandwidth shifting mechanism; prefetch usefulness (PU) level 2015 HPCA IBM XChange: A Market-based Approach to Scalable Dynamic Multi-resource Allocation in Multicore Architectures CMP multiresource allocation mechanism XChange; market framework based modeling 2018 MICRO SNU RpStacks-MT: A High-throughput Design Evaluation Methodology for Multi-core Processors graph-based multi-core performance model; distance-based memory system model; dynamic scheduling reconstruction method 2023 MICRO Yonsei McCore: A Holistic Management of High-Performance Heterogeneous Multicores cluster partitioning via index hash function; partitions balancing method; hardware support for RL based scheduling"},{"location":"hardware/accelerators/#hardware-design","title":"Hardware Design","text":"<p>Solution: Hardware implementation for many-core architecture to achieve massive parallelism.</p> Year Venue Authors Title Tags P E N 2016 SCIS THU&amp;BNU&amp;CAS The Sunway TaihuLight supercomputer: system and applications Sunway TaihuLight's composition; scientific computing applications on TaihuLight 3 4 2 2017 IPDPSW SJTU&amp;Tokyo Tech Benchmarking SW26010 Many-core Processor hand-coded assembly benchmark for SW26010; CPE pipeline&amp;memory hierarchy&amp;RLC mechanism benchmarking 3 4 2 2023 MICRO THU MAICC: A Lightweight Many-core Architecture with In-Cache Computing for Multi-DNN Parallel Inference slice improved and hardware-implemented reduction CIM; ISA extension for CIM; CNN layer segmentation and mapping algorithm"},{"location":"hardware/accelerators/#application-optimization","title":"Application Optimization","text":"Year Venue Authors Title Tags P E N 2023 SC NUDT Optimizing Direct Convolutions on ARM Multi-Cores direct convolution algorithm NDirect; loop ordering algorithm; micro convolution kernal for computing &amp; packeting 2023 SC NUDT Optimizing MPI Collectives on Shared Memory Multi-Cores intra-node reduction algorithm for redundant data movements; fine grained non-temporal store based adaptive collectives 2024 PPoPP NUDT Towards Scalable Unstructured Mesh Computations on Shared Memory Many-Cores task dependency tree(TDT); tree traversal based parallel algorithm for CPU/GPU"},{"location":"hardware/accelerators/#heterogeneous-many-core-system","title":"Heterogeneous Many-core System","text":"Year Venue Authors Title Tags P E N 2018 ICCAD WSU Hybrid On-Chip Communication Architectures for Heterogeneous Manycore Systems many-to-few communication patterns; long range shortcut based wireless NoC ; 3D-TSV based heterogeneous NoC 2018 IEEE TC WSU On-Chip Communication Network for Efficient Training of Deep Convolutional Networks on Heterogeneous Manycore Systems wireless-enabled heterogeneous NoC; archived multi-objective simulated annealing for network connectivity"},{"location":"hardware/accelerators/#architecture-dse","title":"Architecture DSE","text":"<p>Challenge: It's crucial to find the optimal hardware configurations that meet performance; power; and area constraints for specific applications.</p>"},{"location":"hardware/accelerators/#mapping-co-exploration-dse","title":"Mapping &amp; Co-Exploration DSE","text":"<p>Challenge: Efficiently co-optimize DNN mapping and hardware architecture under complex constraints.</p> Year Venue Authors Title Tags P E N 2020 ICCAD UIUC DNNExplorer: A Framework for Modeling and Exploring a Novel Paradigm of FPGA-based DNN Accelerator two-level (global and local) automatic DSE engine; dynamic design space exploration framework; high-dimensional design space support 4 4 4 2024 HPCA THU Gemini: Mapping and Architecture Co-exploration for Large-scale DNN Chiplet Accelerators layer-centric encoding method; DP-based graph partition algorithm; SA based D2D link communication optimization 2024 ASPLOS THU Cocco: Hardware-Mapping Co-Exploration towards Memory Capacity-Communication Optimization consumption-centric flow based subgraph execution scheme; main/side region based memory management 2024 ASPDAC CUHK SoC-Tuner: An Importance-guided Exploration Framework for DNN-targeting SoC Design intercluster distance algorithm; importance-based pruning and initialization 3 2 2 2024 Arxiv Georgia Tech PIPEORGAN: Efficient Inter-operation Pipelining with Flexible Spatial Org spatial organization strategy pipeorgan for inter-operator pipelining; augmented mesh for pipelining(AMP) topology 4 2 2"},{"location":"hardware/accelerators/#microarchitecture-cross-architecture-dse","title":"Microarchitecture &amp; Cross-Architecture DSE","text":"<p>Challenge: Efficiently explore and optimize design spaces across microarchitectures and heterogeneous hardware.</p> Year Venue Authors Title Tags P E N 2025 arXiv THU &amp; Macau MLDSE: Scaling Design Space Exploration Infrastructure for Multi-Level Hardware IR and builder based hardware modeling; cross-architecture DSE; spatial-level DSE 3 3 2 2025 arXiv PKU DiffuSE: Cross-Layer Design Space Exploration of DNN Accelerator via Diffusion-Driven Optimization diffusion-based design generation; conditional sampling 3 4 3"},{"location":"hardware/accelerators/#data-access-accelerators","title":"Data Access Accelerators","text":"<p>Challenge: Indirect and sparse memory access patterns; low memory bandwidth utilization; core structural limitations</p> Year Venue Authors Title Tags P E N 2025 ISCA UMich DX100: Programmable Data Access Accelerator for Indirection indirect memory access accelerator; bulk memory access reordering; DRAM row-buffer hit rate optimization; programmable data access ISA 4 3 2"},{"location":"hardware/accelerators/#gemm-accelerators","title":"GEMM Accelerators","text":"<p>Challenge: High computational intensity; memory bandwidth bottlenecks; energy efficiency</p> Year Venue Authors Title Tags P E N 2020 HPCA Georgia Tech SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training flexible dot product engine; forwarding adder network 4 2 3"},{"location":"hardware/eda/","title":"Electronic Design Automation","text":""},{"location":"hardware/eda/#rtl-code-generation","title":"RTL Code Generation","text":"<p>Challenge: The need for automated RTL code generation tools to reduce the time and effort required for hardware design.</p> <p>Solution: Use advanced techniques such as LLM; graph-based approaches; and domain-specific languages to automate and optimize the RTL code generation process and integrate into existing design tools.</p> Year Venue Authors Title Tags P E N 2013 DAC Columbia A Method to Abstract RTL IP Blocks into C++ Code and Enable High-Level Synthesis process communication graph; I/O port loop unrolling; HLS design space expansion 2021 ASPLOS Cornell A compiler infrastructure for accelerator generators a split representation combining a high-level control flow language with a hardware-like structural language; pass-based compiler; systolic array generator; live-range-based register-sharing 4 3 3 2024 ISEDA UESTC GraphRTL: an Agile Design Framework of RTL Code from Data Flow Graphs graph error detection kernel; DFS based graph equivalent reconstruction; template/scala based DFG and CFG merging 2024 arXiv UCSD MAGE: A Multi-Agent Engine for Automated RTL Code Generation multi-agent; high-temperature sampling and ranking; verilog-state checkpoint debugging 2024 ICCAD PKU OriGen: Enhancing RTL Code Generation with Code-to-Code Augmentation and Self-Reflection self-reflection mechanism; dataset augmentation methodology; VerilogFixEval benchmark 2 4 2"},{"location":"hardware/eda/#higher-level-rtl-code-generation","title":"Higher-Level RTL Code Generation","text":"<p>Challenge: raise the level of abstraction to specifically target the construction of applications</p> Year Venue Authors Title Tags P E N 2022 PLDI Cornell University PDL: A High-Level Hardware Design Language for Pipelined Processors one-instruction-at-a-time semantics; hardware description language; hazard locks; speculation API; guaranteed-correct pipelining 3 3 3"},{"location":"hardware/eda/#rtl-code-generation-benchmarks","title":"RTL Code Generation Benchmarks","text":"<p>Challenge: The lack of standardized benchmarks for evaluating RTL code generation tools.</p> Year Venue Authors Title Tags P E N 2023 DATE NYU Benchmarking Large Language Models for Automated Verilog RTL Code Generation verilog code training corpus; multi-level verilog coding problems for analysis"},{"location":"hardware/emerging/","title":"Emerging Technologies","text":""},{"location":"hardware/emerging/#photonic-computing","title":"Photonic computing","text":"<p>Solution: Uses light (photons) instead of electricity (electrons) to perform calculations and process information. It leverages the unique properties of light; such as its speed and parallelism; to achieve high-performance computing.</p> Year Venue Authors Title Tags P E N 2021 Nature Swinburne 11 TOPS photonic convolutional accelerator for optical neural networks universal optical convolutional accelerator for vector processing 2025 arXiv ASU H3PIMAP: A Heterogeneity-Aware Multi-Objective DNN Mapping Framework on Electronic-Photonic Processing-in-Memory Architectures Electronic-Photonic-PIM Accelerator; coresponding mapping framework and evaluation infrastructure"},{"location":"hardware/fail/","title":"Security and Reliability","text":""},{"location":"hardware/fail/#error-pattern","title":"Error Pattern","text":""},{"location":"hardware/fail/#manycore-architecture","title":"Manycore Architecture","text":"Year Venue Authors Title Tags P E N 2009 MICRO UIUC mSWAT: Low-Cost Hardware Fault Detection and Diagnosis for Multicore Systems selective Triple Modular Redundant(TMR) replay method; symptom based fault detection; permanent/transient fault 2015 IEEE TSM NTU Wafer Map Failure Pattern Recognition and Similarity Ranking for Large-Scale Data Sets wafer map failure pattern; wafer map similarity ranking; radon/geometry-based feature extraction; WM-811K wafer map dataset"},{"location":"hardware/fail/#system-level","title":"System Level","text":"Year Venue Authors Title Tags P E N 2017 SC Argonne National Lab Run-to-run Variability on Xeon Phi based Cray XC Systems OS noise based core-level variability; tile-level varibility; memory mode varibility 2018 FAST UChicago Fail-Slow at Scale: Evidence of Hardware Performance Faults in Large Production Systems conversion among fail-stop/slow/trasient; permanent/transient/partial slowdown; internal/external root causes"},{"location":"hardware/fail/#hardware-fault","title":"Hardware Fault","text":"Year Venue Authors Title Tags P E N 2014 DTIS LIRMM A Survey on Simulation-Based Fault Injection Tools for Complex Systems runtime fault injection; compile-time fault injection 2021 ASPLOS UIUC BayesPerf: Minimizing Performance Monitoring Errors using Bayesian Statistics microarchitectural relationship incorporation; measurement uncertainty quantification; high-frequency sampling reduction 3 4 3 2024 arXiv GWU Algorithmic Strategies for Sustainable Reuse of Neural Network Accelerators with Permanent Faults stack-at-0/1 faults; weight register fault; invertible scaling and shifting technique; elementary tile operations for mantissa fault 2025 arXiv NUDT FlexStep: Enabling Flexible Error Detection in Multi/Many-core Real-time Systems register checkpoints based error detection; memory access log unit; data buffering and channelling unit 2025 DAC SEU MEEK: Re-thinking Heterogeneous Parallel Error Detection Architecture for Real-World OoO Superscalar Processors data extraction unit; bespoke forwarding fabric; little core upgrade 3 4 3"},{"location":"hardware/fail/#noc-fault","title":"NoC Fault","text":"Year Venue Authors Title Tags P E N 2006 IOLTS UBC &amp; WSU On-line Fault Detection and Location for NoC Interconnects code-disjoint based error detection algorithm; code-disjoint switch design 2 2 2 2011 ASPDAC NTHU On the Design and Analysis of Fault Tolerant NoC Architecture Using Spare Routers shift-and-replace allocation algorithm; defect-awareness-path allocation algorithm 3 2 2 2013 TVLSI NUDT Addressing Transient and Permanent Faults in NoC With Efficient Fault-Tolerant Deflection Router link-level error control scheme; on-line fault diagnosis mechanism;RL based fault-tolerant deflection routing 4 2 2 2017 TECS NTUA SoftRM: Self-Organized Fault-Tolerant Resource Management for Failure Detection and Recovery in NoC Based Many-Cores permanent fault; tweaked perfect failure detector; paxos algorithm to recover fault 2 4 2 2017 DDECS TTU From Online Fault Detection to Fault Management in Network-on-Chips: A Ground-Up Approach data-path fault detection; control part fault detection; assertion vector based fault localization 3 1 2"},{"location":"hardware/fail/#fail-slow","title":"Fail-Slow","text":"<p>Challenge: Fail-slow faults can cause performance degradation without complete failure; making them difficult to detect and diagnose than the fail-stop failure.</p> Year Venue Authors Title Tags P E N 2019 ATC UChicago IASO: A Fail-Slow Detection and Mitigation Framework for Distributed Storage Services slowdown detection based on peer score; sub-root causes for five kinds of root causes 2022 ATC SJTU &amp; Alibaba NVMe SSD Failures in the Field: the Fail-Stop and the Fail-Slow hardware infant mortality; write amplification factor; intra-node/rock failure 3 4 2 2023 FAST SJTU &amp; Alibaba PERSEUS: A Fail-Slow Detection Framework for Cloud Storage Systems outlier data detection; regression model for detection threshold; risk evaluating algorithm 4 4 3 2024 MICRO MIT DelayAVF: Calculating Architectural Vulnerability Factors for Delay Faults architectural vulnerability factors; tractable two-step derivation for AVF computation 3 3 2 2025 ASPDAC Xiamen University A Fail-Slow Detection Framework for HBM Devices outlier data detection; regression model for detection threshold; risk evaluating algorithm 2 4 2"},{"location":"hardware/fail/#physical-effects","title":"Physical Effects","text":""},{"location":"hardware/fail/#rram","title":"RRAM","text":"<p>Challenge: Non-ideal effects of RRAM devices (e.g. device-to-device variation; cycle-to-cycle variation; etc.) can cause significant performance degradation.</p> <p>Solution: Data types; training algorithm; SRAM for compensation.</p> Year Venue Authors Title Tags P E N 2019 DAC UCF Noise Injection Adaption: End-to-End ReRAM Crossbar Non-ideal Effect Adaption for Neural Network Mapping stuck-at-fault; crossbar wire resistance based IR drop; thermal noise model; shot noise; random telegraph noise 2019 DATE Georgia Tech Design of Reliable DNN Accelerator with Un-reliable ReRAM dynamical fixed point data representation format; device variation aware training methodology 2020 DAC ASU Accurate Inference with Inaccurate RRAM Devices: Statistical Data, Model Transfer, and On-line Adaptation introduce statistical variations in knowledge distillation; On-line sparse adaptation with a small SRAM array 2020 DATE SJTU Go Unary: A Novel Synapse Coding and Mapping Scheme for Reliable ReRAM-based Neuromorphic Computing unary coding; priority mapping* 2022 TCAD ASU Hybrid RRAM/SRAM in-Memory Computing for Robust DNN Acceleration integrates an RRAM-based IMC macro with a digital SRAM macro using a programmable shifter to compensate for RRAM variations; ensemble learning 2023 ISCAS TAMU Memristor-based Offset Cancellation Technique in Analog Crossbars peripheral circuitry to remove the systematic offset of crossbar 2024 LATS AMU Analysis of Conductance Variability in RRAM for Accurate Neuromorphic Computing analyzation and quantification of conductance variability in RRAMs; analysis of conductance variation over multiple cycles 2025 arXiv AMU Energy-Efficient RRAM-Based Neuromorphic Computing with Adaptive Voltage and Frequency Scaling energy-efficient RRAM-based neuromorphic computing; adaptive voltage and frequency scaling; energy-efficient RRAM-based neuromorphic computing 2 4 3"},{"location":"hardware/fail/#dram","title":"DRAM","text":"<p>Challenge: DRAM devices are sensitive to temperature and voltage variations; which can lead to performance degradation and reliability issues.</p> Year Venue Authors Title Tags P E N 2015 RACS NTU Thermal/Performance Characterization of CMPs with 3D-stacked DRAMs under Synergistic Voltage-Frequency Control of Cores and DRAMs coordinate dynamic voltage and frequency scaling; thermal efficiency quantification 3 2 2 2017 IEEE Access Yuan Ze University Thermal- and Performance-Aware Address Mapping for the Multi-Channel Three-Dimensional DRAM Systems inter-channel bank swapping; inter-channel bank reordering 3 3 2 2020 TCAD BUAA Temperature-Aware DRAM Cache Management\u2014Relaxing Thermal Constraints in 3-D Systems temperature-safe cache operation; exploration on cache remapping; write-back optimization 4 3 2 2024 TCAD IIT 3D-TemPo: Optimizing 3-D DRAM Performance Under Temperature and Power Constraints reward-based dynamic power budgeting; adjacency awareness; DRAM low-power-based DTM 3 3 2"},{"location":"hardware/fail/#3dic","title":"3DIC","text":"Year Venue Authors Title Tags P E N 2004 ICCAD UCLA A thermal-driven floorplanning algorithm for 3D ICs combined bucket and 2D array; tile stack based model; horizontal and vertical heat flow analysis 2016 IJHMT UCR Analysis of critical thermal issues in 3D integrated circuits thermal hotspots; impact of thermal interface materials; power distribution; processor pitch and area"},{"location":"hardware/fail/#fault-tolerant-cache","title":"Fault-Tolerant Cache","text":"Year Venue Authors Title Tags P E N 2009 ICCD NUS The Salvage Cache: A fault-tolerant cache architecture for next-generation memory technologies fault-bit protection for divisions; victim map based division replacement 2011 CASES UCSD FFT-Cache: A Flexible Fault-Tolerant Cache Architecture for Ultra Low Voltage Operation flexible defect map for faulty block; FDM configuration algorithm; non-functional lines minimization"},{"location":"hardware/memory/","title":"Memory Architecture","text":""},{"location":"hardware/memory/#ndp-flash","title":"NDP: Flash","text":""},{"location":"hardware/memory/#general-application-specific-optimization","title":"General Application-Specific Optimization","text":"<p>Solution: Intergrate the compute unit into the SSD controller to process the capacity-sensitive applications.</p> Year Venue Authors Title Tags P E N 2024 HPCA UCLA BeaconGNN: Large-Scale GNN Acceleration with Out-of-Order Streaming In-Storage Computing DirectGraph format for out-of-order sampling; die-level processing units; channel-level command router 4 2 3 2025 ISCA ETHZ REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing In-Storage processing 2 4 3 2025 ISCA UCSD In-Storage Acceleration of Retrieval Augmented Generation as a Service metamorphic in-storage accelerator; Metadata Navigation Unit for dynamic data access 4 3 2 2025 arxiv ETHZ MARS: Processing-In-Memory Acceleration of Raw Signal Genome Analysis Inside the Storage Subsystem PIM module inside the SSD controller; early signal quantization; read filtering 3 3 2 2024 arXiv ICT Cambricon-LLM: A Chiplet-Based Hybrid Architecture for On-Device Inference of 70B LLM chiplet-based NPU &amp; NAND flash hybrid architecture; Hardware-aware tiling for NPU-flash workload distribution 4 3 2"},{"location":"hardware/memory/#llm-specific-optimization","title":"LLM-Specific Optimization","text":"<p>Solution: Store weights in flash memory as read-only to prevent failures caused by write operations.</p> Year Venue Authors Title Tags P E N 2025 ISCA Seoul National AiF: Accelerating On-Device LLM Inference Using In-Flash Processing in-flash GEMV computation; charge-recycling read to skip precharge/discharge steps in flash memory 3 3 4 2025 HPCA THU Lincoln: Real-Time 50~100B LLM Inference on Consumer Devices with LPDDR-Interfaced, Compute-Enabled Flash Memory flash-on-LPDDR-interface for prefill phase; hybrid-bonding-based near-Flash computing for generation phase 3 4 3 2025 HPCA PKU InstAttention: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference offloading decoding-phase attention computation to computational SSDs; SparF Attention flash-aware sparse algorithm 4 2 2"},{"location":"hardware/memory/#ndp-dimm","title":"NDP: DIMM","text":"<p>Challenge: Memory wall causing high latency of data transfer between CPU and memory.</p> <p>Solution: Put the compute unit in the memory or near the memory to reduce the data transfer overhead.</p>"},{"location":"hardware/memory/#general-application-specific-optimization_1","title":"General Application-Specific Optimization","text":"<p>Challenge: Existing NDP architecture are designed for general-purpose computing; not efficient for specific tasks like graph processing.</p> Year Venue Authors Title Tags P E N 2022 ISCA Micron To PIM or Not for Emerging General Purpose Processing in DDR Memory Systems vector engine inside NDP bank; intelligent code offload decision 2 3 2 2024 ISCA Samsung pSyncPIM: Partially Synchronous Execution of Sparse Matrix Operations for All-Bank PIM Architectures partially synchronous PIM control; predicated execution; sparse matrix distribution &amp; compaction 3 3 3 2025 ATC RUC Turbocharge ANNS on Real Processing-in-Memory by Enabling Fine-Grained Per-PIM-Core Scheduling per-PU scheduling; persistent PIM kernel; per-PU dispatching with selective replication 3 4 4 2025 HPCA UC Davis NOVA: A Novel Vertex Management Architecture for Scalable Graph Processing message-driven processors capable of executing algorithms; a direct-mapped cache with a write-back policy; support both asynchronous and bulk synchronous parallel execution models 3 3 3"},{"location":"hardware/memory/#dnn-specific-optimization","title":"DNN-Specific Optimization","text":"Year Venue Authors Title Tags P E N 2021 HPCA Seoul National GradPIM: A Practical Processing-in-DRAM Architecture for Gradient Descent fixed-function PIM architecture for DNN gradient descent; non-invasive PIM operations using reserved DDR commands 3 3 2 2022 PACT PKU GNNear: Accelerating Full-Batch Training of Graph Neural Networks with Near-Memory Processing splitting reduce operations to NDP units; narrow-shard strategy for data reuse; hybrid graph partition strategy for load balancing 4 3 3 2024 ASPLOS PKU PIM-DL: Expanding the Applicability of Commodity DRAM-PIMs for Deep Learning via Algorithm-System Co-Optimization algorithm for DNN to look-up-table conversion; auto-tuner for optimizing LUT-NN mapping on DRAM-PIMs 3 4 3"},{"location":"hardware/memory/#llm-specific-optimization_1","title":"LLM-Specific Optimization","text":"<p>Challenge: LLM inference is fundamentally bottlenecked by memory bandwidth; HBM is expensive and not scalable.</p> Year Venue Authors Title Tags P E N 2024 npj Unconv. Comput. UMich PIM-GPT: a hybrid process in memory accelerator for autoregressive transformers hybrid system to accelerate GPT inference; mapping scheme for data locality and workload distribution 3 2 2 2024 DAC Seoul National MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models activation movement strategy to replace costly parameter movement; dynamic GPU-MoNDE load balancing for hot/cold experts 4 4 2 2024 DAC Hunan Univ. A Real-time Execution System of Multimodal Transformer through PIM-GPU Collaboration dynamic strategy for PIM-GPU task offloading; variable-length-aware PIM allocation optimizer; extended TVM backend for PIM-GPU command generation 3 3 3 2025 MICRO KAIST PIMBA: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving Unified PIM acceleration for both transformer and post-transformer LLMs; access interleaving technique for shared State-update Processing Unit 4 2 3 2025 MICRO Samsung Duplex: A Device for Large Language Models with Mixture of Experts, Grouped Query Attention, and Continuous Batching replace the GPU HBM memory die with HBM-PIM die; expert and attention co-processing for dynamic workload splitting within MoE/attn layers 4 4 4"},{"location":"hardware/memory/#memory-address-space","title":"Memory Address Space","text":"<p>Challenge: Host pages need to enable interleaving to improve concurrent throughput, while PIM pages need to disable it to maintain better locality, creating a conflict.</p> Year Venue Authors Title Tags P E N 2023 DAC Georgia Tech vPIM: Efficient Virtual Address Translation for Scalable Processing-in-Memory Architectures network-contention-aware hashing to minimize cross-stack page table walks; pre-translation using repurposed PIM cores to move page table walks off the critical path 4 4 3 2024 ISCA SJTU UM-PIM: DRAM-based PIM with Uniform &amp; Shared Memory Space Uniform shared CPU-PIM memory; dual-track memory management; zero-copy data re-layout 3 3 4"},{"location":"hardware/memory/#memory-allocation-management","title":"Memory Allocation &amp; Management","text":"<p>Challenge: Existing NDP architecture has numerous independent memory spaces; lacks unified management; and features inefficient memory allocation.</p> Year Venue Authors Title Tags P E N 2024 ISCA KAIST PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for Processing-In-Memory (PIM) Architectures PIM-specific memory allocator; hierarchical memory allocation scheme; hardware metadata cache 4 2 3 2024 arXiv ETHZ PUMA: Efficient and Low-Cost Memory Allocation and Alignment Support for Processing-Using-Memory Architectures aligned memory allocator for PUM; DRAM-aware memory allocation 2 3 2 2024 MICRO KAIST PIM-MMU: A Memory Management Unit for Accelerating Data Transfers in Commercial PIM Systems data copy engine for host-PIM transfers; PIM-aware memory scheduler for MLP maximization; memory remapping unit for dual address mapping 2 4 3 2025 arXiv Amazon DL-PIM: Improving Data Locality in Processing-in-Memory Systems subscription-based architecture to proactively move data; distributed address-indirection hardware lookup table 3 2 3"},{"location":"hardware/memory/#pim-compiler-isa-extension","title":"PIM Compiler &amp; ISA Extension","text":"<p>Challenge: Existing compilers are not optimized for locality-aware PIM architectures and require specialized programming models to fully utilize PIM capabilities.</p> Year Venue Authors Title Tags P E N 2015 ISCA Seoul National PIM-Enabled Instructions: A Low-Overhead; Locality-Aware Processing-in-Memory Architecture PIM-Enabled Instructions for ISA extension; PIM directory for atomicity and coherence; single-cache-block restriction 3 4 4 2020 ISCA UCSB iPIM: Programmable In-Memory Image Processing Accelerator Using Near-Bank Architecture Single-Instruction-Multiple-Bank ISA; register allocation; instruction reordering 4 4 2 2025 ISCA POSTECH ATIM: Autotuning Tensor Programs for Processing-in-DRAM autotuning framework for DRAM PIM; search-based optimizing tensor compiler; balanced evolutionary search algorithm 3 3 4"},{"location":"hardware/memory/#evaluation-simulators","title":"Evaluation &amp; Simulators","text":"Year Venue Authors Title Tags P E N 2025 HPCA THU UniNDP: A Unified Compilation and Simulation Tool for Near DRAM Processing Architectures unified NDP hardware abstraction; NDP compiler optimization; instruction-driven NDP simulator 3 5 2 2025 arXiv ETHZ EasyDRAM: An FPGA-based Infrastructure for Fast and Accurate End-to-End Evaluation of Emerging DRAM Techniques FPGA-based DRAM evaluation framework; C++ high-level language for description; time scaling for accurate modeling 3 4 3"},{"location":"hardware/memory/#intra-dimm-communication","title":"Intra-DIMM Communication","text":"<p>Challenge: High latency of intra-DIMM (cross-bank) communication via host CPU forwarding.</p> Year Venue Authors Title Tags P E N 2024 ISCA THU NDPBridge: Enabling Cross-Bank Coordination in Near-DRAM-Bank Processing Architectures gather &amp; scatter messages via buffer chip; task-based message-passing model; hierarchical, data-transfer-aware load balancing 2025 HPCA Samsung Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory Scatter-Gather In-DRAM fine-grained scatter-gather via data bus offsets; fine-grained cache architecture using fg-tags; Standard DDR command interpretation for FIM control; Combined graph tiling with fine-grained memory access 3 3 4 2025 arXiv ETHZ PIMDAL: Mitigating the Memory Bottleneck in Data Analytics using a Real Processing-in-Memory System PIMDAL library for DB operators; quicksort/mergesort/hashing on UPMEM PIM; scatter/gather/async transfers for PIM communication 4 4 2 2024 arXiv Seoul National PID-Comm: A Fast and Flexible Collective Communication Framework for Commodity Processing-in-DIMM Devices Virtual hypercube PIM model; PE-assisted data reordering; in-register and cross-domain data modulation 3 4 3 2025 ISCA KAIST PIMnet: A Domain-Specific Network for Efficient Collective Communication in Scalable PIM domain-specific PIM interconnect; hierarchical network for PIM packaging; PIM-controlled deterministic scheduling 2 4 3"},{"location":"hardware/memory/#inter-dimm-communication","title":"Inter-DIMM Communication","text":"<p>Challenge: High latency of inter-DIMM (cross-DIMM) communication via host CPU forwarding.</p> Year Venue Authors Title Tags P E N 2017 MEMSYS UCLA AIM: Accelerating Computational Genomics through Scalable and Noninvasive Accelerator-Interposed Memory placing FPGA chip between DIMM and the conventional memory network; multi-drop bus for inter-accelerator communication 1 2 2 2023 ASPLOS THU ABNDP: Co-optimizing Data Access and Load Balance in Near-Data Processing Traveller Cache; hybrid task scheduling; hybrid scheduling leveraging distributed cache 4 3 4 2023 HPCA PKU DIMM-Link: Enabling Efficient Inter-DIMM Communication for Near-Memory Processing high-speed hardware link bridges between DIMMs; direct intra-group P2P communication &amp; broadcast; hybrid routing mechanism for inter-group communication 2025 HPCA SJTU AsyncDIMM: Achieving Asynchronous Execution in DIMM-Based Near-Memory Processing Offload-Schedule-Return mechanism; switch-recovery scheduling; explicit/implicit synchronization 2 4 3 2018 MICRO UIUC Application-Transparent Near-Memory Processing Architecture with Memory Channel Network integrates a processor on a buffered DIMM; application-transparent near-memory processing; leverages memory channels for high-bandwidth/low-latency inter-processor communication 3 4 4"},{"location":"hardware/memory/#concurrent-host-and-pim-operations","title":"Concurrent Host and PIM operations","text":"<p>Challenge: High latency of concurrent host CPU/GPU and PIM operations via host CPU forwarding.</p> Year Venue Authors Title Tags P E N 2024 IEEE CA KAIST Analysis of Data Transfer Bottlenecks in Commercial PIM Systems: A Study With UPMEM-PIM runtime data transposition causing high CPU overhead; PIM-integrated system memory mapping impact 2 2 2 2024 ASPLOS KAIST NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing dual row buffer architecture; sub-batch interleaving; greedy min-load bin packing algorithm 3 4 3 2025 HPCA ICT Make LLM Inference Affordable to Everyone: Augmenting GPU Memory with NDP-DIMM activation sparsity-based hot(GPU)/cold(NDP) neuron partitioning; offline ILP + online predictor for neuron partition; window-based online remapping for GPU-NDP &amp; NDP-NDP load balance 2 3 4 2025 ISCA Univ. of Virginia Membrane: Accelerating Database Analytics with Bank-Level DRAM-PIM Filtering bank-level DRAM-PIM filtering; CPU-PIM cooperative query execution; denormalization for PIM-amenable filtering 3 3 2"},{"location":"hardware/memory/#optimizations-on-upmem-pim","title":"Optimizations on UPMEM-PIM","text":"<p>Challenge: The original UMPEM API library is not well-suited for all workloads especially for those with cross-bank communication.</p> Year Venue Authors Title Tags P E N 2023 arXiv ETHZ A Framework for High-throughput Sequence Alignment using Real Processing-in-Memory Systems Alignment-in-Memory framework; hybrid WRAM-MRAM sketch data management for PIM 2 3 4 2025 arXiv ETHZ PIMDAL: Mitigating the Memory Bottleneck in Data Analytics using a Real Processing-in-Memory System PIMDAL library on UPMEM PIM system for data analytics; scatter/gather-aware transfers for inter-PIM communication; Apache Arrow for host memory management 3 3 3"},{"location":"hardware/memory/#pim-in-cache-computing","title":"PIM: In-Cache-Computing","text":"Year Venue Authors Title Tags P E N 2025 arXiv Torino ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions ARCANE in-cache NMC coprocessor architecture; software-defined matrix ISA for NMC abstraction; cache-integrated control runtime for NMC management 3 4 4"},{"location":"hardware/memory/#pim-ndp-benchmarks","title":"PIM &amp; NDP: Benchmarks","text":"<p>Challenge: Conventional parallel computing benchmarks are not suitable for PIM/NDP.</p>"},{"location":"hardware/memory/#benchmarks-for-conventional-computing","title":"Benchmarks for Conventional Computing","text":"Year Venue Authors Title Tags P E N 2021 ATC UBC A Case Study of Processing-in-Memory in off-the-Shelf Systems benchmark 2022 IEEE Access ETH Benchmarking a New Paradigm: Experimental Analysis and Characterization of a Real Processing-in-Memory System benchmark suite PrIM 2024 CAL KAIST Analysis of Data Transfer Bottlenecks in Commercial PIM Systems: A Study With UPMEM-PIM low MLP; manual data placement; unbalanced thread allocation and scheduling 2024 IEEE Access Lisbon NDPmulator: Enabling Full-System Simulation for Near-Data Accelerators From Caches to DRAM simulator PiMulator based on Ramulator &amp; gem5; full system support; multiple ISA support 2024 HPCA KAIST Pathfinding Future PIM Architectures by Demystifying a Commercial PIM Technology simulator uPIMulator"},{"location":"hardware/memory/#benchmarks-for-quantum-computing","title":"Benchmarks for Quantum Computing","text":"Year Venue Authors Title Tags P E N 2025 ASPDAC NUS PIMutation: Exploring the Potential of PIM Architecture for Quantum Circuit Simulation PIMutation framework for quantum circuit simulation; gate merging optimization; row swapping instead of matrix multiplication; vector partitioning for separable states; leveraging UPMEM PIM architecture"},{"location":"hardware/memory/#ndp-cxl","title":"NDP: CXL","text":"<p>Challenge: No direct physical connectivity between the banks in the DIMM-based NDP architecture. Limited number of DDR channels causing poor scalability.</p> <p>Solution: Introduce CXL-based interconnects to enable direct communication between memory banks; Use CXL memory pools and CXL switches to enable scalable NDP architecture.</p> Year Venue Authors Title Tags P E N 2022 MICRO UCSB BEACON: Scalable Near-Data-Processing Accelerators for Genome Analysis near Memory Pool with the CXL Support scalable hardware accelerator inside CXL switch or bank; lossless memory expansion for CXL memory pools 2024 ICS Samsung CLAY: CXL-based Scalable NDP Architecture Accelerating Embedding Layers direct interconnect between DRAM clusters; dedicated memory address mapping scheme; Multi-CLAY system support through customized CXL switch 2024 MICRO SK Hyrix Low-overhead General-purpose Near-Data Processing in CXL Memory Expanders CXL.mem protocol instead of CXL.io (DMA) for low-latency; lightweight threads to reduce address calculation overhead 2025 ISCA Seoul National COSMOS: A CXL-Based Full In-Memory System for Approximate Nearest Neighbor Search CXL core-based ANNS task offload; rank-level parallel distance computation; adjacency-aware data placement algorithm 2 2 2 2025 ASPLOS UMich PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference hierarchical CXL PIM-PNM compute architecture; use die-shot to estimate area cost; multiple LLM parallelism policies 2 3 3"},{"location":"hardware/memory/#ndp-3d-stacked-dram","title":"NDP: 3D-stacked DRAM","text":"<p>Challenge: No direct physical connectivity between the banks in the DIMM-based NDP architecture.</p> <p>Solution: Use TSVs to provide TB/s level bandwidth in inter-bank communication &amp; band-to-logic layer communication.</p> Year Venue Authors Title Tags P E N 2013 PACT KAIST Memory-centric System Interconnect Design with Hybrid Memory Cubes memory-centric network; distributor-based topology for reduced latency; non-minimal routing for higher throughput 2024 DAC SNU MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models NDP for MoE; activation movement; GPU-MoNDE load-balancing scheme 2024 ASPLOS PKU SpecPIM: Accelerating Speculative Inference on PIM-Enabled System via Architecture-Dataflow Co-Exploration algorithmic and architectural heterogeneity; PIM resource allocation; multi-model collaboration workflow"},{"location":"hardware/memory/#ndp-hbm","title":"NDP: HBM","text":"Year Venue Authors Title Tags P E N 2021 ISCA Samsung Hardware Architecture and Software Stack for PIM Based on Commercial DRAM Technology Industrial Product drop-in replacement for standard HBM2; bank-level parallelism using standard DRAM commands; address aligned mode to tolerate host-side command reordering 3 5 3 2022 Hot Chips Samsung Aquabolt-XL HBM2-PIM, LPDDR5-PIM With In-Memory Processing, and AXDIMM With Acceleration Buffer HBM2-PIM with bank-level SIMD programmable computing units; Acceleration DIMM with acceleration buffers for rank-level parallelism 2 5 3"},{"location":"hardware/memory/#benchmark","title":"Benchmark","text":"Year Venue Authors Title Tags P E N 2019 DAC ETHZ NAPEL: Near-Memory Computing Application Performance Prediction via Ensemble Learning simulator Ramulator-PIM; tracefile from Ramulator &amp; run on zsim 2021 CAL UVA MultiPIM: A Detailed and Configurable Multi-Stack Processing-In-Memory Simulator simulator MultiPIM; multi-stack &amp; virtual memory support; parallel offloading"},{"location":"hardware/memory/#ndp-heterogeneous-architecture","title":"NDP: Heterogeneous Architecture","text":"<p>Challenge: Different PIM architectures have different characteristics and performance trade-offs; communicating between different PIM architectures is challenging.</p> Year Venue Authors Title Tags P E N 2025 ISCA HUST HeterRAG: Heterogeneous Processing-in-Memory Acceleration for Retrieval-augmented Generation combine DIMM-PIM and HBM-PIM for acceleration; locality-aware retrieval and generation; fine-grained parallel pipelining 2 3 3 2025 arXiv NUS LEAP: LLM Inference on Scalable PIM-NoC Architecture with Balanced Dataflow and Fine-Grained Parallelism data dynamicity-aware task assignment to PIM or NoC; fine-grained model partitioning and heuristically optimized spatial mapping strategy 3 4 3 2025 arXiv THU CompAir: Synergizing Complementary PIMs and In-Transit NoC Computation for Efficient LLM Acceleration heterogeneous DRAM-PIM and SRAM-PIM architecture with hybrid bonding; in-transit NoC computation with Curry ALU; hierarchical ISA for hybrid PIM systems 3 4 2"},{"location":"hardware/memory/#general-cim","title":"General CiM","text":""},{"location":"hardware/memory/#specific-application-algorithm","title":"Specific Application &amp; Algorithm","text":"Year Venue Authors Title Tags P E N 2024 ISVLSI USC Multi-Objective Neural Architecture Search for In-Memory Computing neural architecture search methodology; integration of Hyperopt, PyTorch and MNSIM 2024 arXiv Intel CiMNet: Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware framework that jointly searches for optimal sub-networks and hardware configurations for CiM architectures; multi-objective evolutionary search method 4 2 4 2025 AICAS UVA Optimizing and Exploring System Performance in Compact Processing-in-Memory-based Chips Pipeline Method for Compact PIM Designs; Dynamic Duplication Method (DDM); Maximum NN Size Estimation &amp; Deployment in Compact PIM Design"},{"location":"hardware/memory/#modeling-simulation","title":"Modeling &amp; Simulation","text":"Year Venue Authors Title Tags P E N 2018 TCAD ASU NeuroSim: A Circuit-Level Macro Model for Benchmarking Neuro-Inspired Architectures in Online Learning estimate the circuit-level performance of neuro-inspired architectures; estimates the area, latency, dynamic energy, and leakage power; Support both SRAM and eNVM; tested on 2-layer MLP NN, MNIST 2019 IEDM Georgia Tech DNN+NeuroSim: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators with Versatile Device Technologies a python wrapper to interface NeuroSim; for inference only 2020 TCAD ZJU Eva-CiM: A System-Level Performance and Energy Evaluation Framework for Computing-in-Memory Architectures models for capturing memory access and dependency-aware ISA traces; models for quantifying interactions between the host CPU and the CiM module 2022 ICCAD Purdue Design Space and Memory Technology Co-Exploration for In-Memory Computing Based Machine Learning Accelerators simulation framework to evaluate the systemlevel performance of IMC architecture; area-aware weight mapping strategy 4 3 2 2024 ISPASS MIT CiMLoop: A Flexible, Accurate, and Fast Compute-In-Memory Modeling Tool flexible specification to describe CiM systems; accurate model/fast statistical model of data-value-dependent component energy 2025 ASPDAC HKUST MICSim: A Modular Simulator for Mixed-signal Compute-in-Memory based AI Accelerator modulared Neurosim; data statistic-based average-mode instead of trace-based mode 4 3 2"},{"location":"hardware/memory/#cim-dram","title":"CIM: DRAM","text":"<p>Solution: Rather than placing logic units into DRAM; modify the physical structure of DRAM/eDRAM to enable in-memory computing.</p> Year Venue Authors Title Tags P E N 2021 ICCD ASU CIDAN: Computing in DRAM with Artificial Neurons Threshold Logic Processing Element (TLPE) for in-memory computation; Four-bank activation window; Configurable threshold functions; Energy-efficient bitwise operations; Integration with DRAM architecture 2022 HPCA UCSD TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer token-based dataflow for general Transformer-based models; ring-based data broadcast in modified HBM 4 2 4 2024 A-SSCC UNIST A 273.48 TOPS/W and 1.58 Mb/mm2 Analog-Digital Hybrid CIM Processor with Transpose Ternary-eDRAM Bitcell analog DRAM CIM for partial sum and digital adder 1 4 2 2025 arXiv KAIST RED: Energy Optimization Framework for eDRAM-based PIM with Reconfigurable Voltage Swing and Retention-aware Scheduling RED framework for energy optimization; reconfigurable eDRAM design; retention-aware scheduling; trade-off analysis between RBL voltage swing, sense amplifier power, and retention time; refresh skipping and sense amplifier power gating 2025 arXiv UTokyo MVDRAM: Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM Acceleration GeMV operations for end-to-end low-bit LLM inference using unmodified DRAM; processor-DRAM co-design; on-the-fly vector encoding; horizontal matrix layout 4 4 3 2025 arXiv Purdue HALO: Memory-Centric Heterogeneous Accelerator with 2.5D Integration for Low-Batch LLM Inference heterogeneous CiD/CiM accelerator; phase-aware mapping strategy 3 2 2"},{"location":"hardware/memory/#cim-sram","title":"CIM: SRAM","text":"<p>Challenge: Memory wall causing high latency of data transfer between CPU and memory; DIMM-based NDP causing high energy consumption; area overhead and low performance efficiency.</p> <p>Solution: Generally modify the physical structure of SRAM to enable in-memory computing; rather than placing logic units into SRAM.</p>"},{"location":"hardware/memory/#sram-cim-general-architecture","title":"SRAM CIM: General Architecture","text":"Year Venue Authors Title Tags P E N 2024 ISCAS NYCU CIMR-V: An End-to-End SRAM-based CIM Accelerator with RISC-V for AI Edge Device incorporates CIM layer fusion, convolution/max pooling pipeline, and weight fusion; weight fusion: pipelining the CIM convolution and weight loading 2018 JSSC MIT CONV-SRAM: An Energy-Efficient SRAM With In-Memory Dot-Product Computation for Low-Power Convolutional Neural Networks SRAM-embedded convolution (dot-product) computation architecture for BNN; support multi-bit input-output 2024 ESSCIRC THU A 65nm 8b-Activation 8b-Weight SRAM-Based Charge-Domain Computing-in-Memory Macro Using A Fully-Parallel Analog Adder Network and A Single-ADC Interface SRAM-based CD-CiM architecture; charge-domain analog adder tree; ReLU-optimized ADC 4 4 4 2021 ISSCC TSMC An 89TOPS/W and 16.3TOPS/mm2 All-Digital SRAM-Based Full-Precision Compute-In Memory Macro in 22nm for Machine-Learning Edge Applications programmable bit-widths for both input and weights; SRAM and CIM mode 2 5 1 2021 JSSC KAIST Z-PIM: A Sparsity-Aware Processing-in-Memory Architecture With Fully Variable Weight Bit-Precision for Energy-Efficient Deep Neural Networks bit-serial operation to support variable weight bit-precision; data mapping and computation flow for sparsity handling 3 4 4"},{"location":"hardware/memory/#sram-cim-specific-use-or-application","title":"SRAM CIM: Specific Use or Application","text":"Year Venue Authors Title Tags P E N 2023 TCAS-I UIC MC-CIM: Compute-in-Memory With Monte-Carlo Dropouts for Bayesian Edge Intelligence SRAM-based CIM macros to accelerate Monte-Carlo dropout; compute reuse between consecutive iterations 2024 DAC GWU Addition is Most You Need: Efficient Floating-Point SRAM Compute-in-Memory by Harnessing Mantissa Addition decomposing FP mantissa multiplication into sub-ADD and sub-MUL; hybrid-domain SRAM CIM architecture 3 3 2 2025 A-SSCC Georgia Tech A 28nm 1.80Mb/mm2 Digital/Analog Hybrid SRAM-CIM Macro Using 2D-Weighted Capacitor Array for Complex Number Mac Operations Hybrid DCIM/ACIM SRAM; lightweight correction schemes; complex CIM-SRAM units 2 4 2 2025 arXiv GWU Unicorn-CIM: Uncovering the Vulnerability and Improving the Resilience of High-Precision Compute-in-Memory SRAM-CIM for FP DNNs; a fault-injection framework for FP DNNs; a ECC scheme for FP DNNs 3 2 3 2025 ISCAS KAUST Reconfigurable Precision INT4-8/FP8 Digital Compute-in-Memory Macro for AI Acceleration parallel-input approach; mantissa parallel-alignment technique 3 2 2"},{"location":"hardware/memory/#sram-cim-hardware-software-co-design","title":"SRAM CIM: Hardware-Software Co-Design","text":"Year Venue Authors Title Tags P E N 2022 TCAD NTHU MARS: Multi-macro Architecture SRAM CIM-Based Accelerator with Co-designed Compressed Neural Networks sparsity algorithm designed for SRAM CiM; quantization algorithm with BN fusion 3 3 2 2023 TCAD UCSB SDP: Co-Designing Algorithm, Dataflow, and Architecture for In-SRAM Sparse NN Acceleration double-broadcast hybridgrained pruning method; bit-serial booth inSRAM (BBS) multiplication dataflow 3 3 2 2024 TCAD BUAA DDC-PIM: Efficient Algorithm/Architecture Co-Design for Doubling Data Capacity of SRAM-Based Processing-in-Memory doubling the equivalent data capacity of SRAM-based PIM; FCC algorithm to obtain bitwise complementary filters 4 4 2 2024 TCASAI Purdue Algorithm Hardware Co-Design for ADC-Less Compute In-Memory Accelerator reduce ADC overhead in analog CiM architectures; Quantization-Aware Training; Partial Sum Quantization; ADC-Less hybrid analog-digital CiM hardware architecture HCiM 3 3 2025 TCAD BUAA Efficient SRAM-PIM Co-design by Joint Exploration of Value-Level and Bit-Level Sparsity hybrid-grained pruning algorithm; customized Dyadic Block PIM (DB-PIM) architecture 4 3 2"},{"location":"hardware/memory/#sram-cim-simulator-modeling","title":"SRAM CIM: Simulator &amp; Modeling","text":"Year Venue Authors Title Tags P E N 2020 ISCAS JCU MemTorch: A Simulation Framework for Deep Memristive Cross-Bar Architectures supports both GPUs and CPUs; integrates directly with PyTorch; simulate non-idealities of memristive devices within cross-bar, tested on VGG-16, CIFAR-10 2021 TCAD Geogia Tech DNN+NeuroSim V2.0: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators for On-Chip Training non-ideal device properties of NVMS' effect for on-chip training 3 3 2 2025 DAC BUAA CIMFlow: An Integrated Framework for Systematic Design and Evaluation of Digital CIM Architectures workflow for implementing and evaluating DNN workloads on digital CIM architectures; CIM-specific ISA design; compilation flow built on the MLIR infrastructure 4 2 3"},{"location":"hardware/memory/#sram-cim-transformer-accelerator","title":"SRAM CIM: Transformer Accelerator","text":"<p>Challenge: Transformer architecture is widely used in NLP and CV tasks. Existing SRAM CIM architectures are not suitable for transformer acceleration.</p> Year Venue Authors Title Tags P E N 2025 DATE PKU Leveraging Compute-in-Memory for Efficient Generative Model Inference in TPUs architecture model and simulator for CIM-based TPUs; designed for LLM inference 4 2 4 2023 arXiv Keio An 818-TOPS/W CSNR-31dB SQNR-45dB 10-bit Capacitor-Reconfiguring Computing-in-Memory Macro with Software-Analog Co-Design for Transformers Capacitor-Reconfiguring analog CIM architecture 1 4 3 2025 arXiv Purdue Hardware-Software Co-Design for Accelerating Transformer Inference Leveraging Compute-in-Memory SRAM based softmax-friendly CIM architecture for transformer; finer-granularity pipelining strategy 4 3 2 2025 arXiv PKU Leveraging Compute-in-Memory for Efficient Generative Model Inference in TPUs Energy-efficient CIM core integration in TPUs (replace the original MXU); CIM-MXU with systolic data path; Array dimension scaling for CIM-MXU; Area-efficient CIM macro design; Mapping engine for generative model inference 2024 JSSC THU MulTCIM: Digital Computing-in-Memory-Based Multimodal Transformer Accelerator With Attention-Token-Bit Hybrid Sparsity long reuse elimination scheduler (LRES) to dynamically reshape the attention matrix; runtime token pruner (RTP) to remove insignificant tokens; modal-adaptive CIM network (MACN) to dynamically divide CIM cores into Pipeline; effective-bits-balanced CIM (EBBCIM) macro architecture 5 4 3"},{"location":"hardware/memory/#cim-rram","title":"CIM: RRAM","text":"<p>Challenge: RRAM devices are non-volatile and have high density; suitable for CIM applications. However; RRAM devices have non-ideal effects that can cause significant performance degradation.</p>"},{"location":"hardware/memory/#rram-cim-simulator","title":"RRAM CiM: Simulator","text":"Year Venue Authors Title Tags P E N 2018 TCAD THU MNSIM: Simulation Platform for Memristor-Based Neuromorphic Computing System reference design for largescale neuromorphic accelerator and can also be customized; behavior-level computing accuracy model 2023 TCAD THU MNSIM 2.0: A Behavior-Level Modeling Tool for Processing-In-Memory Architectures integrated PIM-oriented NN model training and quantization flow; unified PIM memory array model; support for mixed-precision NN operations 2024 DATE UCAS PIMSIM-NN: An ISA-based Simulation Framework for Processing-in-Memory Accelerators event-driven simulation approach; can evaluate the optimizations of software and hardware independently"},{"location":"hardware/memory/#rram-cim-architecture","title":"RRAM CiM: Architecture","text":"Year Venue Authors Title Tags P E N 2019 ASPLOS Purdue &amp; HP PUMA: A Programmable Ultra-efficient Memristor-based Accelerator for Machine Learning Inference Programmable and general-purpose ReRAM based ML Accelerator; Supports an instruction set; Has potential for DNN training; Provides simulator that accepts model 2018 ICRC Purdue &amp; HP Hardware-Software Co-Design for an Analog-Digital Accelerator for Machine Learning compiler to translate model to ISA; ONNX interpreter to support models in common DL frame work; simulator to evaluate performance 2023 NANOARCH HUST Heterogeneous Instruction Set Architecture for RRAM-enabled In-memory Computing General ISA for RRAM CiM &amp; digital heterogeneous architecture; a tile-processing unit-array three-level architecture 2024 VLSI-SoC RWTH Aachen University Architecture-Compiler Co-design for ReRAM-Based Multi-core CIM Architectures inference latency predictions and analysis of the crossbar utilization for CNN 2024 arXiv CAS A Fully Hardware Implemented Accelerator Design in ReRAM Analog Computing without ADCs Based on Stochastic Binary Neural Networks; Winner-Take-All (WTA) strategy; Hardware implemented sigmoid and softmax 4 3 4"},{"location":"hardware/memory/#rram-cim-architecture-optimization","title":"RRAM CiM: Architecture optimization","text":"Year Venue Authors Title Tags P E N 2024 MICRO HUST DRCTL: A Disorder-Resistant Computation  Translation Layer Enhancing the Lifetime and  Performance of Memristive CIM Architecture address conversion method for dynamic scheduling; hierarchical wear-leveling (HWL) strategy for reliability improvement; data layout-aware selective remapping (LASR) to improve communication locality and reduce latency 2024 DATE RWTH Aachen University CLSA-CIM: A Cross-Layer Scheduling Approach for Computing-in-Memory Architectures algorithm to decide which parts of NN are duplicated to reduce inference latency; cross layer scheduling on tiled CIM architectures 2024 TC SJTU ERA-BS: Boosting the Efficiency of ReRAM-Based  PIM Accelerator With Fine-Grained  Bit-Level Sparsity bit-level sparsity in both weights and activations; bit-flip scheme; dynamic activation sparsity exploitation scheme 2023 TETCI TU Delft Accurate and Energy-Efficient Bit-Slicing for RRAM-Based Neural Networks unbalanced bit-slicing scheme for higher accuracy; holistic solution using 2's compliment 2024 Science USC Programming memristor arrays with arbitrarily high precision for analog computing represent high-precision numbers using multiple relatively low-precision analog devices;using RRAM CIM to solve PDEs 5 4 3"},{"location":"hardware/memory/#rram-cim-design-space-exploration","title":"RRAM CiM: Design Space Exploration","text":"Year Venue Authors Title Tags P E N 2025 arXiv RWTH Aachen Optimizing Binary and Ternary Neural Network Inference on RRAM Crossbars using CIM-Explorer Tensor Virtual Machine (TVM)-based compiler; implementation of different mapping techniques; DSE flow to analyze the impact of parameters 3 3 3"},{"location":"hardware/memory/#rram-cim-modeling","title":"RRAM CiM: Modeling","text":"Year Venue Authors Title Tags P E N 2024 AICAS RWTH Aachen University A Calibratable Model for Fast Energy Estimation of MVM Operations on RRAM Crossbars system energy model for MVM on ReRAM crossbars; methodology to study the effect of the selection transistor and wire parasitics in 1T1R crossbar arrays 2024 arXiv MIT Modeling Analog-Digital-Converter Energy and Area for Compute-In-Memory Accelerator Design architecture-level model that estimates ADC energy and area 4 3 3"},{"location":"hardware/memory/#rram-cim-training-optimization","title":"RRAM CiM: Training optimization","text":"Year Venue Authors Title Tags P E N 2021 TCAD SJTU ITT-RNA: Imperfection Tolerable Training for RRAM-Crossbar-Based Deep Neural-Network Accelerator prevent the large-weight synapses from being mapped to the imperfect memristor cells; off-device training algorithm to alleviate the accumulation of errors across multiple layers; bit-wise mechanism to compensate the resistance variations 3 3 2 2023 arXiv UND U-SWIM: Universal Selective Write-Verify for Computing-in-Memory Neural Accelerators only do write-verify for important weights; based on weight second derivatives as a guide 3 3 3 2023 Adv. Mater. UMich Bulk\u2010Switching Memristor\u2010Based Compute\u2010In\u2010Memory Module for Deep Neural Network Training Bulk-ReRAM based digital-CIM hybrid architecture for training; CIM for forward, digital for backward 4 4 1 2024 APIN SWU Multi-optimization scheme for in-situ training of memristor neural network based on contrastive learning optimizations to the deployment method, loss function and gradient calculation; compensation measures for non-ideal effects 2025 TNNLS SNU Efficient Hybrid Training Method for Neuromorphic Hardware Using Analog Nonvolatile Memory Hybrid offline-online training method"},{"location":"hardware/memory/#rram-cim-compiler","title":"RRAM CiM: Compiler","text":"<p>Challenge: Compiler for RRAM CIM is not well studied. Existing compilers are either for specific architecture or not efficient.</p> Year Venue Authors Title Tags P E N 2023 TACO HUST A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures compilation tool to migrate legacy programs to CPU/CIM heterogeneous architectures; a model to quantify the performance gain 2023 DAC CAS PIMCOMP: A Universal Compilation Framework for Crossbar-based PIM DNN Accelerators compiler based on Crossbar/IMA/Tile/Chip hierarchy; low latency and high throughput mode; genetic algorithm to optimize weight replication and core mapping; scheduling algorithms for complex DNN 2024 ASPLOS CAS CIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators compilation stack for various CIM accelerators; multi-level DNN scheduling approach"},{"location":"hardware/memory/#rram-cim-float-point-processing","title":"RRAM CiM: Float-Point processing","text":"<p>Challenge: Raw RRAM devices are not suitable for floating-point operations; while floating point data is common in DNNs (e.g. FP32).</p> Year Venue Authors Title Tags P E N 2023 SC UCLA ReFloat: Low-Cost Floating-Point Processing in ReRAM for Accelerating Iterative Linear Solvers data format and accelerator architecture 2024 DATE UESTC AFPR-CIM: An Analog-Domain Floating-Point RRAM -based Compute- In- Memory Architecture with Dynamic Range Adaptive FP-ADC all-analog domain CIM architecture for FP8 calculations; adaptive dynamic range FP-ADC &amp; FP-DAC 2025 arXiv GWU A Hybrid-Domain Floating-Point Compute-in-Memory Architecture for Efficient Acceleration of High-Precision Deep Neural Networks SRAM based hybrid-domain FP CIM architecture; detailed circuit schematics and physical layouts"},{"location":"hardware/memory/#rram-cim-convolutional-layer","title":"RRAM CiM: Convolutional Layer","text":"<p>Challenge: Convolutional layer is the most compute-intensive layer in CNNs. RRAM CIM architecture is quite suitable for convolutional layer operations but face challenges related to non-ideal effects and performance degradation.</p> Year Venue Authors Title Tags P E N 2020 Nature THU Fully hardware-implemented memristor convolutional neural network fabrication of high-yield, high-performance and uniform memristor crossbar arrays; hybrid-training method; replication of multiple identical kernels for processing different inputs in parallel 2019 TED PKU Convolutional Neural Networks Based on RRAM Devices for Image Recognition and Online Learning Tasks RRAM-based hardware implementation of CNN; expand kernel to the size of image 2025 TVLSI NBU A 578-TOPS/W RRAM-Based Binary Convolutional Neural Network Macro for Tiny AI Edge Devices ReRAM XNOR cell; BCNN CIM macro with FPGA as the control core 4 4 3"},{"location":"hardware/memory/#rram-cim-mapping-for-cnn","title":"RRAM CiM: Mapping for CNN","text":"<p>Challenge: Efficient mapping of CNN layers onto RRAM CIM architecture is crucial for performance.</p> Year Venue Authors Title Tags P E N 2020 TCAS-I Georgia Tech Optimizing Weight Mapping and Data Flow for Convolutional Neural Networks on Processing-in-Memory Architectures weight mapping to avoid multiple access to input; pipeline architecture for conv layer calculation 2021 TCAD SJTU Efficient and Robust RRAM-Based Convolutional Weight Mapping With Shifted and Duplicated Kernel shift and duplicate kernel (SDK) convolutional weight mapping architecture; parallel-window size allocation algorithm; kernel synchronization method 2023 VLSI-SoC Aachen Mapping of CNNs on multi-core RRAM-based CIM architectures architecture optimized for communication; compiler algorithms for conv2D layer; cycle-accurate simulator 2023 TODAES UCAS Mathematical Framework for Optimizing Crossbar Allocation for ReRAM-based CNN Accelerators formulate a crossbar allocation problem for ReRAM-based CNN accelerators; dynamic programming based solver; models the performance considering allocation problem 2025 IEEE Access UTehran SCiMA: A Systolic CiM-Based Accelerator With a New Weight Mapping for CNNs\u2014A Virtual Framework Approach kernel-major inter-crossbar weight mapping (KM-InterCWM) for convolution layers; structured pruning techniques; system-level virtual framework 4 2 2"},{"location":"hardware/memory/#rram-cim-transformer-accelerator","title":"RRAM CIM: Transformer Accelerator","text":"<p>Challenge: RRAM's cross-bar architecture is suitable for matrix operations.</p> Year Venue Authors Title Tags P E N 2023 VLSI Purdue X-Former: In-Memory Acceleration of Transformers in-memory accelerate attention layers; intralayer sequence blocking dataflow; provides a simulator 2024 TODAES HUST A Cascaded ReRAM-based Crossbar Architecture for Transformer Neural Network Acceleration cascaded crossbar arrays that uses transimpedance amplifiers; data mapping scheme to store signed operands; ADC virtualization scheme 2023 VLSI HUST An RRAM-Based Computing-in-Memory Architecture and Its Application in Accelerating Transformer Inference RRAM-based in-memory floating-point computation architecture (RIME); pipelined implementations of MatMul and softmax 3 3 4 2020 ICCAD Duke ReTransformer: ReRAM-based processing-in-memory architecture for transformer acceleration MatMul does matrix decomposition in scaled dot-product attention; in-memory logic techniques for softmax; sub-matrix pipeline 4 3 3 2022 TCAD KAIST A Framework for Accelerating Transformer-Based Language Model on ReRAM-Based Architecture window self-attention and window-size search algorithm; ReRAM hardware design optimized for this algorithm 4 2 3 2020 ICCD LSU ATT: A Fault-Tolerant ReRAM Accelerator for Attention-based Neural Networks ReRAM-based accelerator with pipeline for AttNNs; heuristic redundancy algorithm 3 2 2 2025 ISCA UCSD Hybrid SLC-MLC RRAM Mixed-Signal Processing-in-Memory Architecture for Transformer Acceleration via Gradient Redistribution architectural and circuit-level hardware designs supporting importance-based data flow with hybrid SLC-MLC ReRAM; gradient redistribution technique 3 2 4"},{"location":"hardware/memory/#rram-cim-special-usage","title":"RRAM CiM: Special Usage","text":"Year Venue Authors Title Tags P E N 2023 GLSVLSI Yale Examining the Role and Limits of Batchnorm Optimization to Mitigate Diverse Hardware-noise in In-memory Computing non-idealities; circuit-level parasitic resistances and device-level non-idealities; crossbar-aware fine-tuning of batchnorm parameters 2019 ASPDAC POSTECH In-memory batch-normalization for resistive memory based binary neural network hardware in-memory batchnormalization schemes; integrate BN layers on crossbar 2024 TRETS UFRGS Reprogrammable Non-Linear Circuits Using ReRAM for NN Accelerators perform typical non-linear operations using ReRAM 4 3 4 2019 Adv. Funct. Mater. HUST Functional Demonstration of a Memristive Arithmetic Logic Unit (MemALU) for In\u2010Memory Computing non-volatile Boolean logic using RRAM crossbar;reconfigurable boolean logic gates 3 4 3"},{"location":"hardware/memory/#rram-cim-matrix-equation-solver","title":"RRAM CiM: Matrix Equation Solver","text":"Year Venue Authors Title Tags P E N 2024 DATE PKU BlockAMC: Scalable In-Memory Analog Matrix Computing for Solving Linear Systems Novel scalable algorithm for matrix equation solving; reconfigurable BlockAMC macros design 3 3 3 2025 Sci.Adv. HUST Fully analog iteration for solving matrix equations with in-memory computing Analog Iteration with Digital Refinement solver 4 4 3 2025 Nat.Elec. PKU Precise and scalable analogue matrix equation solving using resistive random-access memory chips Mixed-Precision Iterative Algorithm for High-Precision Analogue Computing; Scalable Hardware Implementation with BlockAMC algorithm 3 5 4"},{"location":"hardware/memory/#cim-hybrid-architecture","title":"CIM: Hybrid Architecture","text":"<p>Solution: Use hybrid architecture (like SRAM + RRAM) to overcome the limitations of single device (e.g. RRAM's non-ideal effects).</p>"},{"location":"hardware/memory/#hybrid-cim-sram-general-logic","title":"Hybrid CIM: SRAM + General Logic","text":"Year Venue Authors Title Tags P E N 2023 GLSVLSI USC Heterogeneous Integration of In-Memory Analog Computing Architectures with Tensor Processing Units hybrid TPU-IMAC architecture; TPU for conv, CIM for fc 2025 ASPLOS CAS PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System dynamic parallelism-aware task scheduling for llm decoding; online kernel characterization for heterogeneous architectures; hybrid PIM units for compute-bound and memory-bound kernels"},{"location":"hardware/memory/#hybrid-cim-sram-rram","title":"Hybrid CIM: SRAM + RRAM","text":"Year Venue Authors Title Tags P E N 2024 Science NTHU Fusion of memristor and digital compute-in-memory processing for energy-efficient edge computing Fusion of ReRAM and SRAM CiM; ReRAM SLC &amp; MLC Hybrid; Current quantization; Weight shifting with compensation 2024 IPDPS Georgia Tech Harmonica: Hybrid Accelerator to Overcome Imperfections of Mixed-signal DNN Accelerators select and transfer imperfectionsensitive weights to digital accelerator; hybrid quantization(weights on analog part is more quantized) 2023 ICCAD SJTU TL-nvSRAM-CIM: Ultra-High-Density Three-Level ReRAM-Assisted Computing-in-nvSRAM with DC-Power Free Restore and Ternary MAC Operations DCpower-free weight-restore from ReRAM; ternary SRAM-CIM mechanism with differential computing scheme"},{"location":"hardware/memory/#hybrid-cim-memristormram-sram","title":"Hybrid CIM: Memristor/MRAM + SRAM","text":"Year Venue Authors Title Tags P E N 2025 Nature TSMC A mixed-precision memristor and SRAM compute-in-memory AI processor layer based INT-FP hybrid architure; kernel-based mix-CIM (SRAM/ReRAM/digital hybrid architecture) 5 5 2 2025 DAC Chung-Ang Univ. HH-PIM: Dynamic Optimization of Power and Performance with Heterogeneous-Hybrid PIM for Edge AI Devices heterogeneous-hybrid PIM with HP/LP modules and MRAM/SRAM; dynamic data placement algorithm for energy optimization; dual PIM controller design 3 4 2 2025 arXiv AaltoU Acore-CIM: build accurate and reliable mixed-signal CIM cores with RISC-V controlled self-calibration reliability-focused MAC cell; proof-of-concept SoC composed of a CIM core and a RISC-V control processor; automated Built-In Self-Calibration (BISC) routine 3 3 4"},{"location":"hardware/memory/#hybrid-cim-analog-digital","title":"Hybrid CIM: Analog + Digital","text":"Year Venue Authors Title Tags P E N 2023 arXiv HP RACE-IT: A Reconfigurable Analog CAM-Crossbar Engine for In-Memory Transformer Acceleration Compute Analog Content Addressable Memory (Compute-ACAM) structure; accelerator based on crossbars and Compute-ACAMs; encoding-based optimization 3 3 4 2024 VLSI FDU HARDSEA: Hybrid Analog-ReRAM Clustering and Digital-SRAM In-Memory Computing Accelerator for Dynamic Sparse Self-Attention in Transformer product-quantization-based sparse self-attention algorithm; ADC-free ReRAM-CIM macro; ReRAM-CIM for front-end attention sparsification, SRAM-CIM for back-end sparse attention 4 3 3 2024 ASP-DAC Keio OSA-HCIM: On-The-Fly Saliency-Aware Hybrid SRAM CIM with Dynamic Precision Configuration On-the-fly Saliency-Aware precision configuration scheme; Hybrid CIM Array for DCIM and ACIM using split-port SRAM 2025 arXiv South Carolina PIM-LLM: A High-Throughput Hybrid PIM Architecture for 1-bit LLMs hybrid PIM-Digital architecture; analog PIM for low-precision MatMul; digital systolic array for high-precision matMul 4 3 1 2024 ESSERC UCSD An Analog and Digital Hybrid Attention Accelerator for Transformers with Charge-based In-memory Computing analog CIM for low-score tokens, digital processor for high 3 4 2"},{"location":"hardware/memory/#cim-quantization","title":"CIM: Quantization","text":"<p>Challenge: Limited by the precision &amp; area &amp; power trade-off of the ADC; certain CIM devices like RRAM are not suitable for high-precision computation (e.g. FP32). Quantization is needed to reduce the precision of the data.</p>"},{"location":"hardware/memory/#cim-quantization-for-analog-cim","title":"CIM Quantization: For Analog CIM","text":"Year Venue Authors Title Tags P E N 2023 ISLPED Purdue Partial-Sum Quantization for Near ADC-Less Compute-In-Memory Accelerators ADC-Less and near ADC-Less CiM accelerators; CiM hardware aware DNN quantization methodology 2023 AICAS TU Delft Mapping-aware Biased Training for Accurate Memristor-based Neural Networks favorability constraint analysis to find important weight values; mapping-aware biased training to restrict weight values to low variance RRAM states 3 4 2 2024 TCAD BUAA CIMQ: A Hardware-Efficient Quantization Framework for Computing-In-Memory-Based Neural Network Accelerators bit-level sparsity induced activation quantization; quantizing partial sums to decrease required resolution of ADCs; arraywise quantization granularity 2024 TCAD BUAA CIM\u00b2PQ: An Arraywise and Hardware-Friendly Mixed Precision Quantization Method for Analog Computing-In-Memory mixed precision quantization method based on evolutionary algorithm; arraywise quantization granularity; evaluation method to obtain the performance of strategy on the CIM 2024 ICCAD TU Delft Hardware-Aware Quantization for Accurate Memristor-Based Neural Networks analysis of fixed-point quantization impact on conductance variation; weight quantization tuning technique; approach to reduce the residual error 3 2 3"},{"location":"hardware/memory/#cim-quantization-for-all-cim","title":"CIM Quantization: For all CIM","text":"Year Venue Authors Title Tags P E N 2018 CVPR Google Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference integer-only inference arithmetic; quantizes both weights and activations as 8-bit integers, bias 32-bit; provides both quantized inference framework and training frame work 2023 ICCD SJTU PSQ: An Automatic Search Framework for Data-Free Quantization on PIM-based Architecture post-training quantization framework without retraining; hardware-aware block reassembly 2025 arXiv UHK Binary Weight Multi-Bit Activation Quantization for Compute-in-Memory CNN Accelerators a quantization framework that considers CIM's mixed-signal constraints; closed-form layer-specific weight binarization method; differentiable function for uniform multi-bit quantization 3 2 2"},{"location":"hardware/memory/#cim-digital-cim","title":"CIM: Digital CIM","text":"Year Venue Authors Title Tags P E N 2025 ISCAS CAS StreamDCIM: A Tile-based Streaming Digital CIM Accelerator with Mixed-stationary Cross-forwarding Dataflow for Multimodal Transformer tile-based reconfigurable CIM macro microarchitecture; mixed-stationary cross-forwarding dataflow; ping-pong-like finegrained compute-rewriting pipeline"},{"location":"hardware/memory/#nvm","title":"NVM","text":"Year Venue Authors Title Tags P E N 2020 GLSVLSI UND Benchmarking Computing-in-Memory for Design Space Exploration uniform benchmarking of CiM designs based on different memory technologies 3 3 2 2024 ISCAS UMCP On-Chip Adaptation for Reducing Mismatch in Analog Non-Volatile Device Based Neural Networks float-gate transistors based; hot-electron injection to address the issue of mismatch and variation 2023 DATE UniBo End-to-End DNN Inference on a Massively Parallel Analog In Memory Computing Architecture many-core heterogeneous architecture; general-purpose system based on RISC-V cores and nvAIMC cores; based on Phase-Change Memory(PCM);"},{"location":"hardware/network/","title":"Interconnection Networks","text":""},{"location":"hardware/network/#network-on-chip","title":"Network-on-Chip","text":"<p>Challenge: The bandwidth limitations and low communication efficiency faced by traditional bus architectures in on-chip multi-core &amp; many-core systems.</p> <p>Solution: NoC provides a flexible and high-bandwidth communication infrastructure for heterogeneous chiplets; enables efficient data movement and processing in heterogeneous many-core architectures.</p>"},{"location":"hardware/network/#wafer-scale","title":"Wafer-Scale","text":"Year Venue Authors Title Tags P E N 2024 SC THU Switch-Less Dragonfly on Wafers: A Scalable Interconnection Architecture based on Wafer-Scale Integration four-level topology structure; minimal routing algorithm on dragonfly for VC vumber reduction 2024 TCAS SYSU CINOC: Computing in Network-On-Chip With Tiled Many-Core Architectures for Large-Scale General Matrix Multiplications computable input buffers;  thread execution free from fine-grained instruction control; data-aware thread execution 2025 ISCA THU PD Constraint-aware Physical/Logical Topology Co-Design for Network on Wafer mesh-switch physical topology; dual-granularity logical topology 4 3 2 2025 ISCA THU WSC-LLM: Efficient LLM Service and Architecture Co-exploration for Wafer-scale Chips optimal resource partition algorithm; optimal KV cache placement algorithm 4 3 2 2025 ISCA Georgia Tech\u200c FRED: A Wafer-scale Fabric for 3D Parallel DNN Training microswitch based distributed on-wafer topology; conflict-free collective routing algorithm; congestion-aware device placement policy 3 3 2"},{"location":"hardware/network/#topology","title":"Topology","text":"<p>Challenge: Current NoC topologies are often rigid and not adaptable to the specific needs of heterogeneous chiplets.</p> Year Venue Authors Title Tags P E N 2021 HPCA GWU Adapt-NoC: A Flexible Network-on-Chip Design for Heterogeneous Manycore Architectures mux based adaptable router architecture; adaptable link design; reinforcement learning based subNoC optimization algorithm 2022 HPCA Huawei Application Defined On-chip Networks for Heterogeneous Chiplets: An Implementation Perspective bufferless multi-ring NoC design; application-architecture-physical co-design method; architecture expressiveness; deadlock resolution SWAP mechanism 2024 MICRO THU Ring Road: A Scalable Polar-Coordinate-based 2D Network-on-Chip Architecture Ring Road topology based on isolated cycles and trees; polar coordinate DOR(dimension-order-routing); inter/intra-chip decouple routing algorithm 2024 arXiv WSU Atleus: Accelerating Transformers on the Edge Enabled by 3D Heterogeneous Manycore Architectures heterogeneous 3D NoC; pipeline design across heterogeneous resources; crossbar-wise quantization 2024 ISLPED WSU HeTraX: Energy Efficient 3D Heterogeneous Manycore Architecture for Transformer Acceleration 3D integration;  distinct planar tiers where each tier is tailor-made for either MHA or the FF network; alleviate memory bottlenecks while preventing frequent rewrites on ReRAM crossbars"},{"location":"hardware/network/#interconnect","title":"Interconnect","text":"Year Venue Authors Title Tags P E N 2012 SIGCOMM CMU On-Chip Networks from a Networking Perspective: Congestion and Scalability in Many-Core Interconnects congestion control mechanism for bufferless NoC; interval-based congestion control algorithm; simple injection throttling algorithm 2021 IPDPSW FAU Fast HBM Access with FPGAs: Analysis, Architectures, and Applications HBM lateral routing contention analysis; Memory Access Optimizer (MAO) IP; hierarchical HBM interconnect; HBM address interleaving on FPGA 3 4 3 2023 ICCAD UCF ARIES: Accelerating Distributed Training in Chiplet-based Systems via Flexible Interconnects directional bypassing link; ARIES link with transistor; ARIES all-reduce optimization algorithm 2023 MICRO THU Heterogeneous Die-to-Die Interfaces: Enabling More Flexible Chiplet Interconnection Systems heterogeneous interface hetero-PHY and hetero-channel; hetero-channel routing algorithm; application-aware scheduling"},{"location":"hardware/network/#processing-on-noc","title":"Processing on NoC","text":"<p>Challenge: The need for efficient data processing and computation on-chip to reduce data movement and improve performance.</p> Year Venue Authors Title Tags P E N 2017 ISVLSI Ruhr-Universit\u00e4t Bochum Data Stream Processing in Network-on-Chip data stream processing unit(DSPU); operation mode based DSPU programming framework 2019 HPCA TAMU Active-Routing: Compute on the Way for Near-Data Processing active-routing tree; vector processing in cache block for regular access pattern; data prefetch for irregular access pattern 2020 HPCA Drexel University SnackNoC: Processing in the Communication Layer communication fabric quantification; central packet manager for instruction flit; router compute unit as dataflow pe"},{"location":"hardware/network/#traffic-control","title":"Traffic Control","text":"<p>Challenge: The need for efficient traffic control to manage network traffic and reduce congestion and power consumption.</p> Year Venue Authors Title Tags P E N 2017 ISCA TAMU APPROX-NoC: A Data Approximation Framework for Network-On-Chip Architectures value approximate technique VAXX; encoder/decoder module pair for data compression; approximate value compute logic 2017 ICCD HIT ABDTR: Approximation\u2013Based Dynamic Traffic Regulation for Networks\u2013on\u2013Chip Systems approximate computing based dynamic traffic regulation technique; lightweight design including controller, throttler and approximater 2019 DATE SCUT ACDC: An Accuracy- and Congestion-aware Dynamic Traffic Control Method for Networks-on-Chip quality loss and network congestion modeling; autoregressive model based flow prediction method 2025 arXiv NTU Learning Cache Coherence Traffic for NoC Routing Design cache coherence traffic analyzer; DRL based topology selection and routing design 2 3 2"},{"location":"hardware/network/#fault-tolerant-communication","title":"Fault-Tolerant Communication","text":"Year Venue Authors Title Tags P E N 2014 VLSI ICT ZoneDefense: A Fault-Tolerant Routing for 2-D Meshes Without Virtual Channels fault chains based faulty blocks construction; floor/ceiling rule based defense zone forming; L/F chain routing 2017 TPDS NTU Path-Diversity-Aware Fault-Tolerant Routing Algorithm for Network-on-Chip Systems path diversity analysis; fault-location-based path diversity; PDA-FTR algorithm 2019 DATE UMich SiPterposer: A Fault-Tolerant Substrate for Flexible System-in-Package Design blowing based customized topology; lightweight ECC module based defect tolerance 2022 DATE Colorado State University DeFT: A Deadlock-Free and Fault-Tolerant Routing Algorithm for 2.5D Chiplet Networks virtual network based deadlock freedom; congestion-aware vertical link selection"},{"location":"hardware/network/#nic","title":"NIC","text":"<p>Challenge: The peak bandwidth of NICs continues to increase, while the operating system and kernel layers are gradually lacking the capability to keep up with it.</p> Year Venue Authors Title Tags P E N 2025 HOTOS ETHZ The NIC should be part of the OS cache-coherent interconnect NIC control/data path; zero-overhead RPC dispatch; OS-NIC scheduling state sharing 3 3 3"},{"location":"hardware/network/#router","title":"Router","text":"Year Venue Authors Title Tags P E N 2016 HPCA KTH DVFS for NoCs in CMPs: A Thread Voting Approach thread voting based DVFS machenism; pre-defined region-based V/F adjustment algorithm 2022 HPCA Chalmers FastTrackNoC: A NoC with FastTrack Router Datapaths non-turning hops; direct FastTrack flit path; zero-load latency analysis 2022 HPCA UToronto Stay in your Lane: A NoC with Low-overhead Multi-packet Bypassing FastFlow flow controll method; time-division-multiplexed (TDM) based non-overlapping FastPass-lanes; FastPass for throughput enhancement 2023 HPCA THU A Scalable Methodology for Designing Efficient Interconnection Network of Chiplets interface grouping; hypercube construction algorithm; deadlock-free adaptive routing algorithm; safe/unsafe flow control; network interleaving method 2025 arXiv SJTU StreamGrid: Streaming Point Cloud Analytics via Compulsory Splitting and Deterministic Termination compulsory splitting for reducing on-chip buffer size; deterministic termination for regularizing non-deterministic operations; line buffer optimization for point cloud pipelines; ILP-based buffer size minimization"},{"location":"hardware/network/#remote-procedure-call","title":"Remote Procedure Call","text":"Year Venue Authors Title Tags P E N 2023 arXiv ICT CXL over Ethernet: A Novel FPGA-based Memory Disaggregation Design in Data Centers combining CXL and Ethernet for low-latency remote memory access; FPGA-based prototype with cache optimization; switch-independent congestion control algorithm; native memory semantics for transparent access 2024 arXiv UCSD Telepathic Datacenters: Fast RPCs using Shared CXL Memory pointer-passing RPC over CXL; MPK-based sandboxing for RPC safety; Seal/Release mechanism for RPC safety; RDMA fallback for RPC scalability; Lease/Quota shared memory management 4 3 3"},{"location":"hardware/network/#rdma","title":"RDMA","text":"Year Venue Authors Title Tags P E N 2024 arXiv UCR GPUVM: GPU-driven Unified Virtual Memory GPUVM architecture for on-demand paging; RDMA-capable NIC for GPU memory management; GPU thread-based memory management and page migration; reuse-oriented paged memory for efficient eviction; high-level programming abstraction for GPU memory extension"},{"location":"hardware/parallel/","title":"Parallel and Multi-Processor Architecture","text":""},{"location":"hardware/parallel/#heterogeneous-architecture","title":"Heterogeneous Architecture","text":"<p>Challenge: Classic Heterogeneous Architecture faces challenges in the data movement and memory access patterns; leading to performance bottlenecks.</p> Year Venue Authors Title Tags P E N 2017 TACO Intel HAShCache: Heterogeneity-Aware Shared DRAMCache for Integrated Heterogeneous Systems heterogeneity-aware DRAMCache scheduling PrIS; temporal bypass ByE; spatial occupancy control chaining 2018 ICS NC State ProfDP: A Lightweight Profiler to Guide Data Placement in Heterogeneous Memory Systems latency sensitivity; bandwidth sensitivity; moving factor based data placement 2023 HPCA THU Baryon: Efficient Hybrid Memory Management with Compression and Sub-Blocking stage area and selective commit for stable block; dual-format metadata scheme; cacheline-aligned compression and two-level replacements"},{"location":"hardware/parallel/#multiple-domain-specific-accelerator","title":"Multiple Domain Specific Accelerator","text":"Year Venue Authors Title Tags P E N 2024 HPCA UCSD &amp;&amp; Univ. of Kansas Data Motion Acceleration: Chaining Cross-Domain Multi Accelerators Proposes Data Motion Acceleration (DMX);Data Restructuring Accelerator (DRX) 3 4 3 2025 ISCA HyperAccel Hybe: GPU-NPU Hybrid System for Efficient LLM Inference with Million-Token Context Window GPU-NPU hybrid system for LLM; prefill-decode stage separation; fine-grained KV cache transmission; stage-wise pipelining 4 3 2"},{"location":"hardware/parallel/#cpu-gpu-system","title":"CPU-GPU System","text":"Year Venue Authors Title Tags P E N 2024 arXiv KTH Harnessing Integrated CPU-GPU System Memory for HPC: a first look into Grace Hopper Grace Hopper system memory characterization; integrated CPU-GPU page table analysis; first-touch policy impact study; system page size impact study; access-counter page migration evaluation 2 4 3 2025 ATC THU HYPERECA: Distributed Heterogeneous In-Memory Embedding Database for Training Recommender Models embedding database in host memory and GPU memory; 2-Fold Parallel strategy; contention-free ring schedule 2 3 3"},{"location":"hardware/parallel/#gpu-system","title":"GPU System","text":"Year Venue Authors Title Tags P E N 2022 Mlsys MIT TORCHSPARSE: EFFICIENT POINT CLOUD INFERENCE ENGINE 3D Sparse Convolution; optimize Gather-Matmul-Scatter dataflow; Adaptive Matmul Grouping; Quantized and Vectorized Memory access 4 3 4 2023 Mlsys THU&amp;&amp;SJTU EXPLOITING HARDWARE UTILIZATION AND ADAPTIVE DATAFLOW FOREFFICIENT SPARSE CONVOLUTION IN 3D POINT CLOUDS 3D Sparse Convolution; optimize Gather-Matmul-Scatter and fetch-on-demand dataflow; Dynamic dataflow changing; coded-CSR mapping; Parallel Processing of different workloads without padding; Pointer 4 3 3 2023 MICRO MIT TorchSparse++: Efficient Training and Inference Framework for Sparse Convolution on GPUs 3D Sparse Convolution; optimize Implicit Gather-Matmul-Scatter; Cuda Sparse Kernel; Sparse Autotuner by detailed workload 4 3 4"},{"location":"hardware/parallel/#disaggregated-memory","title":"Disaggregated Memory","text":"<p>Challenge: CXL and NVM offer higher speed &amp; bandwidth than storage devices with byte-level access. Memory disaggregation using DRAM (high-speed/BW + small capacity) and NVM (low-speed/BW + large capacity), faces latency, bandwidth, and consistency challenges.</p> Year Venue Authors Title Tags P E N 2025 ATC THU DSA-2LM: A CPU-Free Tiered Memory Architecture with Intel DSA CPU-free page migration in tiered memory via data streaming accelerator; adaptable migration algorithm for mixed 4KB/2MB pages; direct in-kernel DSA integration bypassing DMA 3 3 4 2025 ASPLOS Purdue EDM: An Ultra-Low Latency Ethernet Fabric for Memory Disaggregation Ethernet PHY network stack; PHY in-network scheduler; PHY intra-frame preemption 4 4 4 2025 HOTOS MSR Storage Class Memory is Dead, All Hail Managed-Retention Memory: Rethinking Memory for the AI Era Managed-Retention Memory class; relaxed retention non-volatile memory; dynamically Configurable Memory 3 3 2"},{"location":"hardware/parallel/#cxl-based-disaggregated-memory","title":"CXL-based Disaggregated Memory","text":"Year Venue Authors Title Tags P E N 2024 MICRO PKU NeoMem: Hardware-Software Co-Design for CXL-Native Memory Tiering device-side memory profiling unit; sketch-based hot page detector with error-bound estimation; dynamic hotness threshold adjustment based on statistics 2 2 3 2025 ASPLOS Yale PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated Memory iterator-based programming model; disaggregated accelerator architecture; in-network routing for distributed traversal 3 4 3 2025 arXiv Micron Architectural and System Implications of CXL-enabled Tiered Memory CXL parallelism bottleneck analysis; MIKU dynamic request control; ToR-based service time estimation 4 4 3 2025 arXiv PKU Enabling Efficient Transaction Processing on CXL-Based Memory Sharing hybrid coherence primitive for transactional data; hardware-assisted loose coherence 3 2 2 2025 ASPLOS PKU CTXNL: A Software-Hardware Co-designed Solution for Efficient CXL-Based Transaction Processing decouple coherence from memory access; software synchronization primitives at transaction commit 4 2 3"},{"location":"hardware/parallel/#survey","title":"Survey","text":"Year Venue Authors Title Tags P E N 2025 arXiv SJTU Survey of Disaggregated Memory: Cross-layer Technique Insights for Next-Generation Datacenters Cross-layer classification of DM techniques; hardware-level categories; architectural-level classifications; system and runtime-level groupings; application-level optimizations such as general-purpose and domain-specific approaches"},{"location":"hardware/parallel/#chiplets","title":"Chiplets","text":"<p>Challenge: Current chip designs are often monolithic and inflexible; leading to high costs and limited performance optimization opportunities.</p> <p>Solution: Use chiplets to enable more flexible and cost-effective system designs by allowing the integration of specialized dies manufactured using optimal processes; leading to improved performance and yield.</p>"},{"location":"hardware/parallel/#survey_1","title":"Survey","text":"Year Venue Authors Title Tags P E N 2020 Electronics NUDT Chiplet Heterogeneous Integration Technology\u2014Status and Challenges heterogeneous integration technology; interconnect interfaces and protocols; packaging technology 2022 CCF THPC ICT Survey on chiplets: interface, interconnect and integration methodology development history; interfaces and protocols; packaging technology; EDA tool; standardization of chiplet technology 2024 IEEE CASS THU Chiplet Heterogeneous Integration Technology\u2014Status and Challenges wafer-scale chip architecture; compiler tool chain; integration technology; wafer-scale system; fault tolerance"},{"location":"hardware/parallel/#multimodal-ai-chiplets","title":"Multimodal AI chiplets","text":"Year Venue Authors Title Tags P E N 2024 MICRO CA SCAR: Scheduling Multi-Model AI Workloads on Heterogeneous Multi-Chiplet Module Accelerators Hierarchical Scheduling Framework;Time Windowing;Chiplet-level Scheduling 3 3 3"},{"location":"hardware/parallel/#cost-analysis","title":"Cost Analysis","text":"Year Venue Authors Title Tags P E N 2025 arXiv ASU CATCH: a Cost Analysis Tool for Co-optimization of chiplet-based Heterogeneous systems heterogeneous chiplet system modeling; DSE on chiplets size,IO,connection"},{"location":"hardware/parallel/#3d-ic","title":"3D IC","text":"<p>Solution: 3DIC technology enables higher integration density; shorter interconnects; and improved performance by stacking multiple active layers in a single device.</p>"},{"location":"hardware/parallel/#general-3d-ic","title":"General 3D IC","text":"Year Venue Authors Title Tags P E N 2019 GLSVLSI Boston Univeristy An Overview of Thermal Challenges and Opportunities for Monolithic 3D ICs TSV-based 3D integration; Mono3D integration with nanoscale monolithic inter-tier vias; influence of lateral heat flow and inter-connection 2019 ECTC TSMC System on Integrated Chips (SoIC) for 3D Heterogeneous Integration system on integrated chips; SoIC package integration; reliability of SoIC bond,TSV and TDV 2020 DATE Georgia Tech Macro-3D: A Physical Design Methodology for Face-to-Face-Stacked Heterogeneous 3D ICs face-to-face stack; separate 2D floorplans generation; memory-on-logic projection 2022 IEEE Micro Cerebras Cerebras Architecture Deep Dive: First Look Inside the Hardware/Software Co-Design for Deep Learning fine-grained dataflow scheduling; high-bandwidth, low-latency fabric design; weight streaming"},{"location":"hardware/parallel/#interconnection","title":"Interconnection","text":"Year Venue Authors Title Tags P E N 2025 HPCA Fudan EIGEN: Enabling Efficient 3DIC Interconnect with Heterogeneous Dual-Layer Network-on-Active-Interposer Dual-layer interconnect architecture, Reinforcement learning routing, Switch-programmable interconnect 3 2 3"},{"location":"hardware/parallel/#design-space-exploration","title":"Design Space Exploration","text":"Year Venue Authors Title Tags P E N 2025 arXiv SJTU Cool-3D: An End-to-End Thermal-Aware Framework for Early-Phase Design Space Exploration of Microfluidic-Cooled 3DICs end-to-end thermal-aware framework; microfluidic cooling integration; Pre-RTL design space exploration; floorplan designer; microfluidic cooling strategy generator"},{"location":"hardware/parallel/#benchmarks","title":"Benchmarks","text":"Year Venue Authors Title Tags P E N 2025 arXiv NJU Open3DBench: Open-Source Benchmark for 3D-IC Backend Implementation and PPA Evaluation open-source 3D-IC benchmark; modular 3D partitioning and placement; Open3D-DMP algorithm for cross-die co-placement; comprehensive PPA evaluation with thermal simulation"},{"location":"hardware/perf/","title":"Performance Modeling and Analysis","text":""},{"location":"hardware/perf/#hardware-performance-counter","title":"Hardware Performance Counter","text":"<p>Challenge: Software performance analysis and optimization is often limited by the lack of accurate and detailed information about the underlying hardware behavior.</p> <p>Solution: Use hardware performance counters to gather data on CPU usage; memory access patterns; cache hits/misses; branch predictions; and other metrics that can help analyze the performance of software applications and hardware systems.</p>"},{"location":"hardware/perf/#survey","title":"Survey","text":"Year Venue Authors Title Tags P E N 2013 TODAES Crete A Survey and Taxonomy of On-Chip Monitoring of Multicore Systems-on-Chip debugging/performance/QoS monitor; physical parameter monitor; methodology based taxonomy 2 4 1 2016 CSUR Oak Ridge Lab Understanding GPU Power: A Survey of Profiling, Modeling, and Simulation Methods external/internal power measurement; HPC based power model; GPU power simulation 3 3 1 2019 SP UNC-Chapel Hill SoK: The Challenges, Pitfalls, and Perils of Using Hardware Performance Counters for Security non-determinism and overcounting effects; performance monitoring interrupt 3 4 1"},{"location":"hardware/perf/#specific-application","title":"Specific Application","text":"Year Venue Authors Title Tags P E N 2000 SC UT A Scalable Cross-Platform Infrastructure for Application Performance Tuning Using Hardware Counters portable and machine-dependent layers based architecture; eventset for group management; counter multiplexing 2 4 2 2004 SC UMD Using Hardware Counters to Automatically Improve Memory Performance two-phase dynamic page migration algorithm; sun fire link counter 3 4 3 2013 ISPASS UTAustin Non-determinism and overcount on modern hardware performance counter implementations nondeterministic hardware interrupts; float point unit related overcount; retired instruction overcount 2 4 2 2020 CONECCT IIIT Power, Performance And Thermal Management Using Hardware Performance Counters fine-grained dynamic voltage and frequency scaling; PMC-based power and temperature correlation model; thermal zone and partition-based management 2 4 2"},{"location":"hardware/perf/#architecture-design","title":"Architecture Design","text":"<p>Challenge: Existing hardware performance counters provide limited information; expansion is needed to support more hardware behavior data.</p> Year Venue Authors Title Tags P E N 2006 ASPLOS UW-Madison A Performance Counter Architecture for Computing Accurate CPI Components interval analysis based performance model; frontend miss table(FMT); shared FMT 3 3 2 2014 ISPASS Intel A Top-Down Method for Performance Analysis and Counters Architecture top-down bottleneck analysis method; frontend bound; bad speculation; retiring; backend bound; top-down performance events 3 5 3 2015 ISCA ANU Computer Performance Microscopy with SHIM double-time error correction; sample periods randomizing; CMP core sampling for low overhead 4 4 3"},{"location":"hardware/perf/#dataflow-architecture","title":"Dataflow Architecture","text":""},{"location":"hardware/perf/#modeling","title":"Modeling","text":"<p>Challenge: Accurately capturing the fine-grained data dependencies while keeping the model abstract enough for further analysis.</p> Year Venue Authors Title Tags P E N 2024 ISCA Stanford The Dataflow Abstract Machine Simulator Framework communicating sequential processes; event-queue free execution; context-channel based description; asynchronous distributed time 2024 ISCA SYSU Soter: Analytical Tensor-Architecture Modeling and Automatic Tensor Program Tuning for Spatial Accelerators analytical tensor-architecture model; transformer based optimizer; coordination between analytical modeling and automatic tuning 4 4 2"},{"location":"hardware/perf/#mapping","title":"Mapping","text":"<p>Challenge: Tiling the dataflow graph onto PEs and interconnects while minimizing buffering, balancing load, and meeting throughput/latency targets.</p> Year Venue Authors Title Tags P E N 2022 OSDI UCB Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning inter-operator parallelisms; intra-operator parallelisms; ILP and DP hierarchical optimization 2023 MICRO PKU TileFlow: A Framework for Modeling Fusion Dataflow via Tree-based Analysis 3D design space of fusion dataflow; tree-based description; tile-centric notation 2024 ICCD SYSU TileMap: Mapping Multi-Head Attention on Spatial Accelerators with Tile-based Analysis tile-based parallelism analysis; tensor life; RL-based searching algorithm for mapping DSE 3 3 2 2025 DATE SYSU Poros: One-Level Architecture-Mapping Co-Exploration for Tensor Algorithms gradient-based search in non-smooth space; policy sampling to address non-convexity; batch sampling for faster convergence 4 3 2"},{"location":"hardware/perf/#connection-architecture","title":"Connection Architecture","text":"Year Venue Authors Title Tags P E N 2014 JPDC Inria Versatile, scalable, and accurate simulation of distributed applications and platforms API based communication&amp;computation description; informed model of TCP for moderate size grids; file based modular network representation technique 2020 MICRO Georgia Tech; NVIDIA MAESTRO: A Data-Centric Approach to Understand Reuse, Performance, and Hardware Cost of DNN Mappings data-centric mapping; data reuse analysis; TemperalMap; SpatialMap; analytical cost model 2023 ISPASS Georgia Tech ASTRA-sim2.0: Modeling Hierarchical Networks and Disaggregated Systems for Large-model Training at Scale graph-based training-loop execution; multi-dimensional heterogeneous topology construction; analytical network backend 2024 ATC THU Evaluating Chiplet-based Large-Scale Interconnection Networks via Cycle-Accurate Packet-Parallel Simulation packet-centric simulation; critical resources recorading for process-order-induced deviations; unimportant stages elimination 2025 arXiv UCLM Understanding Intra-Node Communication in HPC Systems and Datacenters Intra-/inter-node communication interference; Packet-level simulation (OMNeT++); PCIe/NVLink modeling; LLM communication patterns (DP, TP, PP) impact"},{"location":"hardware/perf/#redundancy-detection","title":"Redundancy Detection","text":"<p>Challenge: Redundant zeros in data can lead to inefficiencies in software performance; making it important to detect and eliminate them.</p> Year Venue Authors Title Tags P E N 2020 SC NC State ZeroSpy: Exploring Software Inefficiency with Redundant Zeros code-centric analysis for instruction detection; data-centric analysis for data detection 2020 SC NC State GVPROF: A Value Profiler for GPU-Based Clusters temporal/spatial load/store redundancy; hierarchical sampling for reducing monitoring overhead; bidirectional search algorithm on dependency graph 2022 ASPLOS NC State ValueExpert: Exploring Value Patterns in GPU-accelerated Applications value-related inefficiencies data value pattern recoginition; value flow graph; parallel intervals merging algorithm 2022 SC NC State Graph Neural Networks Based Memory Inefficiency Detection Using Selective Sampling dead store; silent store; silent load; assembly-level procedural control-flow embedding; dynamic value semantic embedding; relative positional encoding for different compilation options"},{"location":"hardware/perf/#task-scheduling-for-accelerators","title":"Task Scheduling for Accelerators","text":"<p>Challenge: Efficiently scheduling tasks on heterogeneous accelerators while considering their unique characteristics and constraints.</p> Year Venue Authors Title Tags P E N 2023 ISCA Tsinghua University Shogun: A Task Scheduling Framework for Graph Mining Accelerators locality-aware out-of-order task scheduling; task tree for decoupling; task tree splitting; search tree merging 4 4 3"},{"location":"hardware/perf/#variation-impact","title":"Variation Impact","text":"<p>Solution: Characterize sources of variation (hardware; software; environment); develop models to predict variation impact; implement techniques to reduce variation (e.g., dynamic voltage and frequency scaling, adaptive scheduling).</p> Year Venue Authors Title Tags P E N 2009 HPCMP UCSD Measuring and Understanding Variation in Benchmark Performance MPI communication variation; distribution of performance variation 2016 SC UNM Understanding Performance Interference in Next-Generation HPC Systems extreme value theory; bulk-synchronous parallel based modeling; gang/earliest deadline first scheduling"},{"location":"hardware/perf/#stall-attribution","title":"Stall Attribution","text":"<p>Challenge: Stall can be caused by hardware or software; identifying the root cause of stalls and their impact on performance is crucial for performance optimization.</p> Year Venue Authors Title Tags P E N 2023 ICPE NC State University DrGPU: A Top-Down Profiler for GPU device memory stall; synchronization stall; instruction related stall; shared memory related stall 2024 MICRO NUDT HyFiSS: A Hybrid Fidelity Stall-Aware Simulator for GPGPUs memory/compute structual/data stall; synchronization stall; control stall; idle stall; cooperative thread array-sets based SM sampling algorithm"},{"location":"hardware/processor/","title":"Processor Architecture","text":""},{"location":"hardware/processor/#cache","title":"Cache","text":"<p>Challenge: Managing shared cache resources (e.g. LLC) efficiently in multi-core/multi-programmed environments.</p> Year Venue Authors Title Tags P E N 2011 ISCA Stanford Vantage: Scalable and Efficient Fine-Grain Cache Partitioning managed-unmanaged region division; churn-based management; feedback-based aperture control 2018 EuroSys PKU DCAPS: dynamic cache allocation with partial sharing dynamic fine-grained shared cache management; balance cache utilization and contention; online practical miss rate curve 2021 ISCA EPFL Large-Scale Graph Processing on FPGAs with Caches for Thousands of Simultaneous Misses miss-optimized memory system (MOMS); extreme non-blocking cache; two-level MOMS architecture; caches for irregular accesses 4 4 4 2024 MICRO Sungkyunkwan University CacheCraft: Enhancing GPU Performance under Memory Protection through Reconstructed Caching reconstructed caching; memory-oriented sector size; single-access in-band ECC; balanced memory layout for mixed-size sectors 3 3 2 2025 ISCA UW-Madison The XOR Cache: A Catalyst for Compression XOR compression; cache inclusion redundancy; inter-line &amp; intra-line synergy; map table for pairing 4 3 4"},{"location":"hardware/processor/#multi-level-cache","title":"Multi-Level Cache","text":"<p>Challenge: Optimizing the interaction and data movement between different levels of the cache hierarchy (e.g. L1 - L2 - L3) in Chip Multi-Processors is complex.</p> Year Venue Authors Title Tags P E N 2005 ISCA IBM Adaptive mechanisms and policies for managing cache hierarchies in chip multiprocessors limit unnecessary clean write backs; write back L2 to peer L2; second adaptive mechanism 2012 JIP Tokio Tech Autonomous L3 Cache Technolgy for High Responsiveness autonomous L3 cache; trio-configuration architecture; autonomous decentralized multi-layer cache"},{"location":"hardware/processor/#vector-unit","title":"Vector Unit","text":""},{"location":"hardware/processor/#memory-access","title":"Memory Access","text":"<p>Challenge: Vector memory access is not well supported &amp; optimized in the current microarchitecture like RISC-V.</p> Year Venue Authors Title Tags P E N 2025 CF Companion ETHZ AraOS: Analyzing the Impact of Virtual Memory Management on Vector Unit Performance virtual memory management for RVV; performance analysis of virtual memory overhead 2 2 1 2025 arXiv THU Efficient Architecture for RISC-V Vector Memory Access data reorganization module; load/store data organization; row/column-accessible vector register file 2 4 3"},{"location":"software/","title":"Software","text":"<p>The reviewed software papers are classified into the following groups:</p>"},{"location":"software/#algorithms-and-theory","title":"Algorithms and Theory","text":"<p>Focuses on the foundations of computation and problem-solving. Includes:</p> <ul> <li>Algorithm design and analysis (including ML/DL algorithms)</li> <li>Data structures</li> <li>Computational complexity</li> <li>Computability theory</li> </ul>"},{"location":"software/#programming-languages-and-compilers","title":"Programming Languages and Compilers","text":"<p>Concerns the design, implementation, and analysis of programming languages. Includes:</p> <ul> <li>Language design and semantics</li> <li>Compiler construction and optimization</li> <li>Program analysis</li> </ul>"},{"location":"software/#operating-systems","title":"Operating Systems","text":"<p>Focuses on managing computer hardware and providing system services. Includes:</p> <ul> <li>Kernel design and implementation</li> <li>Process and thread management</li> <li>Memory management</li> <li>File systems and storage</li> <li>Virtualization</li> <li>Synchronization</li> </ul>"},{"location":"software/#distributed-systems-and-networking","title":"Distributed Systems and Networking","text":"<p>Covers systems of multiple interacting components and communication. Includes:</p> <ul> <li>Distributed algorithms (e.g., consensus, replication)</li> <li>Cloud computing platforms and architectures</li> <li>Microservices</li> </ul>"},{"location":"software/#high-performance-computing-hpc","title":"High-Performance Computing (HPC)","text":"<ul> <li>Parallel programming models (e.g., MPI, OpenMP, CUDA)</li> <li>Scientific computing applications and libraries</li> <li>Parallel algorithms</li> </ul>"},{"location":"software/#performance-analysis","title":"Performance Analysis","text":"<ul> <li>Profilers (sampling, instrumentation)</li> <li>Simulators and emulators (for software/system analysis)</li> <li>Benchmarking methodologies and suites</li> <li>Workload characterization</li> </ul>"},{"location":"software/algorithm/","title":"Algorithms, Theory, and Formal Methods","text":""},{"location":"software/algorithm/#algorithm-design-and-analysis","title":"Algorithm design and analysis","text":"<p>Solution: an algorithm is a well-defined, finite sequence of steps that solves a specific problem or accomplishes a particular task. We focus on algorithms that can solving problems.</p>"},{"location":"software/algorithm/#dynamic-graph-algorithms","title":"Dynamic Graph Algorithms","text":"<p>Solution: Dynamic graph algorithms efficiently update solutions to graph problems as the graph evolves, addressing the challenge of frequent changes in structure and data.</p> Year Venue Authors Title Tags P E N 2023 ASPLOS UCR CommonGraph: Graph Analytics on Evolving Data convert deletions to additions; common graph concept; Triangular Grid (TG) for work sharing; mutation-free representation 3 4 4"},{"location":"software/algorithm/#ml-algorithms","title":"ML Algorithms","text":"<p>Solution: ML algorithms are fundamental tools that enable computers to learn from data and make predictions or decisions without being explicitly programmed.</p>"},{"location":"software/algorithm/#diffusion-models","title":"Diffusion Models","text":"<p>Solution: Diffusion models are generative models that learn to reverse a gradual noising process to generate data from noise.</p> Year Venue Authors Title Tags P E N 2023 arXiv UC Berkeley Scalable Diffusion Models with Transformers Diffusion Transformer (DiT) architecture; replace the original U-Net with transformer blocks; adaptive layer norm (adaLN-Zero) for conditioning 3 5 5"},{"location":"software/algorithm/#auto-regressive-models-for-image","title":"Auto Regressive Models for Image","text":"Year Venue Authors Title Tags P E N 2024 NeurIPS PKU Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction VAR modeling with next-scale prediction; multi-scale quantization for coarse-to-fine tokenization; power-law scaling laws for visual AR models 4 5 5"},{"location":"software/algorithm/#llm-algorithm","title":"LLM Algorithm","text":"<p>Solution: enable ai chat with human, some people think is the way to AGI.</p> Year Venue Authors Title Tags P E N 2020 arXiv OpenAI Scaling Laws for Neural Language Models fundamentals of LLM; increase model size and performance raise 4 5 5"},{"location":"software/algorithm/#llm-transformer","title":"LLM Transformer","text":"<p>Solution: Transformer is an old algorithm, which have many problems like square complexity. These problems raise new algorithms to fix the old architecture.</p> Year Venue Authors Title Tags P E N 2019 arXiv Google Fast Transformer Decoding: One Write-Head is All You Need MQA; share same KV cache for all heads; multi-query attention 1 4 3 2024 NeuroComputing ZhuiYi RoFormer: Enhanced Transformer with Rotary Position Embedding use rotary position embedding to fix the problem of long context; nter-word dependencies decay gradually with the increase of relative distance 3 4 3 2025 arXiv Qwen Parallel Scaling Law for Language Models enhance model's parallel ability to enhance the performance instead of increasing the model size; parallel multi output and conclude one output 4 4 4"},{"location":"software/algorithm/#diffusion-llms","title":"Diffusion LLMs","text":"<p>Challenge: diffusion generate result from noise, this is different from tradition's AR paradigm. Diffusion LLMs need to solve the problem of the order of text logic and the deneration of diffusion's random output index.</p> Year Venue Authors Title Tags P E N 2025 arXiv RUC Large Language Diffusion Models First using diffusion LLM model to generate; Diffusion model excels at reversal reasoning; inter-block AR while in-block diffusion 3 4 3 2025 arXiv HKU Dream 7B: Diffusion Large Language Models based on AR model's pre-train; inter-block diffusion 3 3 3 2025 arXiv THU Survey on Diffusion Language Models survey on training strategies, inference optimization, multimodal and applications of diffusion language models 4 2 2 2025 arXiv ByteDance Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference two-stage training with mask-based and edit-based noise; constrained-order training by filtering optimal generation paths; direct training to reduce generation steps 3 3 5 2025 arXiv RUC LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning visual instruction tuning for diffusion models; multi-stage training for multimodal reasoning; visual instruction tuning for diffusion models 2 3 4 2025 arXiv RUC UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models Diffusion-aware NTK extrapolation for RoPE; long-context post-training with adaptive attention masking 3 3 4"},{"location":"software/algorithm/#llm-alignment","title":"LLM Alignment","text":"<p>Solution: LLM alignment aims to make LLM outputs more consistent with user intent. Its challenges are ensuring safety, addressing multi-modal complexities, and balancing inference ability with alignment.</p> Year Venue Authors Title Tags P E N 2024 arXiv SJTU Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation social scene simulation; emulate realistic multiparty interactions and consequences; monopolylogue 2025 ICLR Princeton Safety Alignment Should Be Made More Than Just a Few Tokens Deep ai-savety centered alignment; enhance sacety on deeper tokens and data 3 3 3 2025 ACL PKU Language Models Resist Alignment: Evidence From Data Compression LLM have inner resistance to alignment; the larger scale in pre-train increase the resistance 4 3 4"},{"location":"software/algorithm/#llm-finetune","title":"LLM Finetune","text":"<p>Solution: finetune adapts a pre-trained model to a specific task or domain. By doing so, the model can better fit the specific task or domain.</p> Year Venue Authors Title Tags P E N 2021 ICLR Miscrosoft LoRA: Low-Rank Adaptation of Large Language Models split the weight matrix into two parts; reduce the number of parameters to finetune 2 4 4"},{"location":"software/algorithm/#coding-llm-finetune","title":"Coding LLM Finetune","text":"Year Venue Authors Title Tags P E N 2024 arXiv UMD HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages large synthetic parallel programming dataset; parallel code generation; HPC AI developer tools"},{"location":"software/algorithm/#llm-powered-ai-agent","title":"LLM-Powered AI Agent","text":"Year Venue Authors Title Tags P E N 2024 arXiv THU LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination hierarchical language agent; real-time human-AI coordination; slow mind &amp; fast mind 2025 arXiv THU Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents RAG-based LLM agent; use OpenAI api to run LLM; agent num is about 50 3 3 2 2025 arXiv Stanford Generative Agent Simulations of 1000 People LLM with memory system; OpenAI api to run LLM; agent num is about 1000 2 2 2 2025 arXiv AiLab OASIS: Open Agent Social Interaction Simulations with One Million Agents agent simulation with large scale; run LLM on local; optimization for large scale agent simulation; agent num is about 1 million 4 3 3"},{"location":"software/algorithm/#rl-algorithms","title":"RL Algorithms","text":"<p>Solution: RL learns from rewards or penalties received without labeled data. It takes actions that interact with the environment. It can learn optimal policies in super large config space.</p> Year Venue Authors Title Tags P E N 2015 Nature DeepMind Human-level control through deep reinforcement learning deep reinforcement learning; human-level control; playing Atari games 5 5 3 2025 arXiv DeepReinforce CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning contrastive RL-driven CUDA optimization without human priors; LLM-based CUDA kernel optimization; reward design for CUDA kernel 4 4 2"},{"location":"software/algorithm/#dnn-training-algorithms","title":"DNN Training Algorithms","text":"<p>Solution: DNN training algorithms are essential for optimizing deep neural networks, enabling them to learn from data and improve their performance on various tasks. They address challenges like convergence speed, generalization, and robustness.</p> Year Venue Authors Title Tags P E N 2017 ICLR Stanford DSD: Dense-Sparse-Dense Training for Deep Neural Networks 3 step dense-sparse-dense training 3 5 4 2020 NeurIPS MIT Differentiable Augmentation for Data-Efficient GAN Training Differentiable Augmentation to improve data efficiency in generative adversarial networks training 3 4 4 2020 CVPR NTHU Robust Processing-In-Memory Neural Networks via Noise-Aware Normalization noise-aware calibration in BatchNorm statistics 3 3 3 2025 ASPLOS Nvidia&amp;CMU&amp;MIT GraphPipe: Improving Performance and Scalability of DNN Training with Graph Pipeline Parallelism graph pipeline parallelism; topology-aware stage partitioning and scheduling algorithm 4 3 2 2025 arXiv ZhiCun Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware extension of STE for complex noise environments; STE-based gradient approximation strategy 3 3 3"},{"location":"software/algorithm/#multi-task-learning","title":"Multi-task Learning","text":"<p>Solution: Multi-task learning (MTL) is a machine learning paradigm where multiple related tasks are learned simultaneously, leveraging shared representations to improve performance across tasks.</p> Year Venue Authors Title Tags P E N 2018 NeurIPS Intel Multi-Task Learning as Multi-Objective Optimization Frank-Wolfe-based optimizer that scales to high-dimensional problems; provide an upper bound for the MGDA(multiple-gradient descent algorithm) optimization objective 3 4 4 2019 NeurIPS CUHK Pareto Multi-Task Learning method to decompose a MTL problem into multiple subproblems; scalable optimization algorithm to solve all constrained subproblems 3 4 4 2021 NeurIPS UTexas Conflict-Averse Gradient Descent for Multi-task learning Conflict-Averse Gradient descent (CAGrad); reduces the conflict among gradients while provably converges to minimum average loss 3 3 3"},{"location":"software/algorithm/#quantization","title":"Quantization","text":"<p>Solution: Quantization are focusing on tradeoffs of accuracy and computation/memory. The challenges are how to run models in high performance and low memory/computation cost.</p>"},{"location":"software/algorithm/#adaptive-datatype","title":"Adaptive Datatype","text":"<p>Solution: Adaptive datatypes aim to optimize numerical representation by dynamically adjusting to the precision and range requirements of data. The challenge lies in balancing computational efficiency, memory usage, and accuracy across diverse tasks and hardware constraints.</p>"},{"location":"software/algorithm/#for-llm","title":"For LLM","text":"Year Venue Authors Title Tags P E N 2023 ISCA SJTU OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization outlier-victim pair that sacrifices the colocated normal values to accommodate the outliers;OVP-based quantization framework and architectural implementation 4 4 2 2023 ICLR ETH Zurich GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers Arbitrary Order Insight; Lazy Batch-Updates\uff1b Cholesky Reformulation 4 4 3 2024 MLSys MIT AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration Preserving 1% Salient Weights;  Protecting Salient Weights by Activation-aware Scaling; searching to scale 4 4 4 2025 arXiv Rice 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float dynamic-length float; preserving bit-for-bit identical outputs; BFloat16 exponents carry significantly less information than their allocated bit width 4 4 4 2025 HPCA SJTU M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type group-wise quantization for both weight and KV cache; new encoding paradigm to improve information utilization in group-wise quantization; specific processing element for encoding paradigm 4 4 2 2025 HPCA Cornell BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration introduce additional asymmetry to FP by repurposing a redundant zero value with another special value; hardware accelerator design 3 3 3"},{"location":"software/algorithm/#for-non-llm","title":"For Non-LLM","text":"Year Venue Authors Title Tags P E N 2020 CVPR ByteDance Inc. AdaBits: Neural Network Quantization With Adaptive Bit-Widths joint-quantization method applied in training;Switchable Clipping Level (SCL) between layers 4 3 3 2022 ICLR Snap Inc. F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization variance-based fixed-point format selection for weights and activations; training algorithm for fixed-point models 3 3 2 2022 MICRO SJTU ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization fixed-length adaptive numerical data type; combines the advantages of float and int for adapting to the importance of different values within a tensor; adaptive framework that selects the best type for each tensor 2024 TCAD HKU DyBit: Dynamic Bit-Precision Numbers for Efficient Quantized Neural Network Inference adaptive data representation with variablelength encoding; hardware-aware quantization framework 2024 arXiv Harvard Nanoscaling Floating-Point (NxFP): NanoMantissa, Adaptive Microexponents, and Code Recycling for Direct-Cast Compression of Large Language Models Nanoscaling Floating-Point (NxFP); NanoMantissa; Adaptive Microexponents; Code Recycling 2025 ISCA SJTU FATE: Boosting the Performance of Hyper-Dimensional Computing Intelligence with Flexible Numerical DAta TypE dimensional fuzzing-distance importance measure; fine-grained compression framework 4 3 4"},{"location":"software/algorithm/#general-method","title":"General method","text":"<p>Solution: General quantization methods aim to optimize the trade-off between model accuracy and computational efficiency. Challenges include addressing layer-specific quantization errors, enhancing fault tolerance, and finding optimal bit-width configurations.</p>"},{"location":"software/algorithm/#for-general-llm","title":"For General LLM","text":"Year Venue Authors Title Tags P E N 2023 ICML MIT SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models offline migrates the quantization difficulty from activations to weights 4 5 3 2024 ISCA SNU Tender: Accelerating Large Language Models via Tensor Decomposition and Runtime Requantization \u201cpower of 2\u201d channel decomposition rule; Tender accelerator design 4 3 2 2025 arXiv PKU Bitnet.cpp: Efficient Edge Inference for Ternary LLMs ternary mpGEMM library; avoid intricate bit-level manipulations; achieving lossless inference for BitNet b1.58 2025 AAAI ByteDance ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models block-wise distribution correction and compensation scheme; bit balance strategy 4 3 2 2025 ICML Huawei,THU FlatQuant: Flatness Matters for LLM Quantization post-training quantization method to enhance the flatness of both weights and activations in LLMs 4 4 3"},{"location":"software/algorithm/#kv-cache-specialized","title":"KV Cache specialized","text":"Year Venue Authors Title Tags P E N 2025 arXiv UVa HACK: Homomorphic Acceleration via Compression of the Key-Value Cache for Disaggregated LLM Inference method without dequantization; homomorphic quantization method for matrix multiplication; requantization elimination 2 2 3 2025 arXiv SJTU MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV Product Quantization a non-uniform quantization algorithm based on product quantization; leverages sparse computation and asynchronous quantization; distributes quantization power unevenly across channels 3 4 2"},{"location":"software/algorithm/#for-non-llm_1","title":"For Non-LLM","text":"Year Venue Authors Title Tags P E N 2018 AAAI SUTD Adaptive Quantization for Deep Neural Network measurement to estimate the effect of parameter quantization errors in individual layers;optimization process for finding optimal quantization bit-width for each layer 3 3 4 2020 ISCA SJTU DRQ: Dynamic Region-based Quantization for Deep Neural Network Acceleration dynamic region-based quantization algorithm; sub-feature map quantization; accelerator architecture for proposing dynamic region-based quantization 4 3 2 2021 MLSys Nvidia VS-Quant: Per-vector Scaled Quantization for Accurate Low-Precision Neural Network Inference per-vector(\u224816-64 elements) scaled quantization technique; two-level scaling scheme and algorithm; modified MAC unit in accelerator 4 3 5 2021 ICML Intel Accurate Post Training Quantization With Small Calibration Sets layer-by-layer optimization method; integer programming; para-normalization 3 3 3 2023 ACML KOBE-U A Mixed-Precision Quantization Method without Accuracy Degradation Using Semilayers semilayers based on whether loss difference is positive or negative 3 2 2"},{"location":"software/algorithm/#fault-tolerance","title":"Fault Tolerance","text":"<p>Solution: Fault tolerance in quantization ensures that models remain robust and reliable despite errors or noise</p> Year Venue Authors Title Tags P E N 2019 DFT Xilinx Efficient Error-Tolerant Quantized Neural Network Accelerators selective channel replication; fault-aware scheduling of processing elements for folded implementations 3 2 3 2023 DAC Yonsei RQ-DNN: Reliable Quantization for Fault-tolerant Deep Neural Networks quantization to enhance fault tolerance caused by fault in memory; quantize to bimodal 3 3 3"},{"location":"software/algorithm/#quantization-aware-training","title":"Quantization-Aware Training","text":"<p>Solution: Quantization-aware training (QAT) is a technique that simulates the effects of quantization during the training process, allowing the model to learn to adapt to the quantization noise.</p> Year Venue Authors Title Tags P E N 2018 arXiv IBM PACT: Parameterized Clipping Activation for Quantized Neural Networks activation quantization scheme for finding the optimal quantization scale during training 3 4 3 2020 ICLR IBM Learned Step Size Quantization approximate the gradient to the quantizer step size; heuristic to bring the magnitude of step size updates into better balance with weight updates 3 4 3 2022 CVPR HKUST Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation Nonuniform-to-Uniform Quantizer (N2UQ) via learning input thresholds; Generalized Straight-Through Estimator (GSTE) to tackle intractable gradient computation in N2UQ 3 3 3 2025 arXiv HKU &amp; ByteDance Scaling Law for Quantization-Aware Training a mathematical model for QAT quantization error 4 4 4"},{"location":"software/algorithm/#dnn-compression","title":"DNN Compression","text":"<p>Solution: DNN compression aims to reduce the size and computational requirements of deep neural networks</p> Year Venue Authors Title Tags P E N 2016 ICLR Stanford Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding three stage pipeline: pruning, trained quantization and Huffman coding 4 4 4 2020 JSTSP Fraunhofer HHI DeepCABAC: A Universal Compression Algorithm for Deep Neural Networks identify set of priors in DNN; redefine CABAC's core scheme to capture priors 3 5 3"},{"location":"software/algorithm/#statistical-parameter-estimation","title":"Statistical Parameter Estimation","text":"<p>Solution: infer the distribution of variables using statistical methods from observed data</p> Year Venue Authors Title Tags P E N 1977 JRSSB Harvard Maximum Likelihood from Incomplete Data via the EM Algorithm incomplete data; maximum likelihood expectation algorithm 2 1 3 2016 Big Data LPNU Machine Learning, Linear and Bayesian Models for Logistic Regression in Failure Detection Problems extreme gradient boosting classifier; generalized linear model 2 1 2 2023 J Process Contr UA Modeling and Bayesian inference for processes characterized by abrupt variations dynamic latent variable model; variational Bayesian inference framework 3 2 2"},{"location":"software/algorithm/#time-synchronization","title":"Time Synchronization","text":"<p>Solution: designing appropriate synchronization strategies, and improving the performance and adaptability of discrete event simulation</p> Year Venue Authors Title Tags P E N 1993 JACM W&amp;M The Cost of Conservative Synchronization in Parallel Discrete Event Simulations windowing mechanism based conservative synchronization; lower-bound performance analysis based on stochastic modeling 2 3 2 2002 TPDS Dartmouth\u200c Composite Synchronization in Parallel Discrete-Event Simulation composite synchronization mechanism; mathematical model based on synchronization overhead optimization 3 3 2 2013 PDES MSOE Synchronization methods in parallel and distributed discrete-event simulation conservative/optimistic synchronization methods; chandy-misra-bryant algorithm; time warp mechanism 3 1 1"},{"location":"software/algorithm/#communication-optimization","title":"Communication Optimization","text":"<p>Solution: modeling and searching the parameter space of collective communication libraries, dynamically selecting optimal configurations under real training/analyzing workloads</p> Year Venue Authors Title Tags P E N 2024 SIGCOMM UPenn&amp;Microsoft Rethinking Machine Learning Collective Communication as a Multi-Commodity Flow Problem traffic engineering based collective communication optimization; mixed-integer linear program; A* technique for scaling 4 3 2 2025 NSDI USTC AutoCCL: Automated Collective Communication Tuning for Accelerating Distributed and Parallel DNN Training low-level performance parameters tuning; subspace division and intra-subspace coordinate descent search algorithms 4 4 2 2025 SC THU TraceFlow: Efficient Trace Analysis for Large-Scale Parallel Applications via Interaction Pattern-Aware Trace Distribution communication skeleton tree; interaction-aware trace distribution; communication-minimized trace shuffling 4 4 2"},{"location":"software/algorithm/#data-structures","title":"Data structures","text":"<p>Solution: organizing and storing data efficiently to enable fast access, modification, and processing</p>"},{"location":"software/algorithm/#dynamic-graph-processing","title":"Dynamic Graph Processing","text":"<p>Solution: data structures for processing dynamic graphs, which are graphs that change over time.</p>"},{"location":"software/algorithm/#architecture-specific-data-structures","title":"Architecture-specific Data Structures","text":"<p>Solution: Data structures targeting specific hardware architectures</p> Year Venue Authors Title Tags P E N 2023 TKDE PKU An Efficient Data Structure for Dynamic Graph on GPUs leveled packed memory array; redundancy-free top-down re-balancing method; con-concurrent strategy Opera 4 4 3 2024 VLDB PKU Towards Sufficient GPU-Accelerated Dynamic Graph Management: Survey and Experiment topology structure; attribute storage; auxiliary structures 4 4 2"},{"location":"software/algorithm/#computational-complexity","title":"Computational complexity","text":"<p>Solution: analyzing and classifying how the time and space requirements of an algorithm grow as the input size increases.</p>"},{"location":"software/algorithm/#computability-theory","title":"Computability theory","text":"<p>Solution: helping to identify the fundamental limits of what can be computed, regardless of time or space constraints.</p>"},{"location":"software/ds/","title":"Distributed Systems","text":""},{"location":"software/ds/#distributed-algorithms","title":"Distributed algorithms","text":"<p>Focusing on the distributed algorithms, such as consensus and replication, like RAFT.</p> <p>Challenge: concurrency, synchronous and communication complexities across independent nodes</p> <p>Solution: problems that require coordination, computation, and data management across multiple independent computer systems.</p>"},{"location":"software/ds/#computing-framework","title":"Computing Framework","text":"<p>Solution: Developing distributed algorithms requires a clear understanding of the computing framework, which scales small computing units to achieve a more clear data processing. The common computing frameworks are MapReduce, Spark, etc.</p> Year Venue Authors Title Tags P E N 2004 OSDI Google MapReduce: simplified data processing on large clusters divife the data processing into map and reduce stages; use master-worker architecture 4 5 5"},{"location":"software/ds/#domain-specific-computing-framework","title":"Domain Specific Computing Framework","text":"<p>Challenge: specific bounds of different situations</p> Year Venue Authors Title Tags P E N 2018 PLDI UT Austin &amp; UIUC Gluon: A Communication-Optimizing Substrate for Distributed Heterogeneous Graph Analytics communication-optimizing substrate; heterogeneous processor support; structural/temporal invariant-based communication optimization; memoization of address translation 4 4 4 2024 PPoPP NUDT GraphCube: Interconnection Hierarchy-aware Graph Processing interconnection hierarchy-aware; topology-aware graph partitioning; extreme-scale graph processing 4 5 5"},{"location":"software/ds/#graph-mining-frameworks","title":"Graph Mining Frameworks","text":"<p>Challenge: inefficient algorithms and communication patterns</p> Year Venue Authors Title Tags P E N 2015 SOSP QCRI Arabesque: A System for Distributed Graph Mining distributed graph mining; think like an embedding paradigm; filter-process model; ODAG embedding storage 4 4 4 2023 ASPLOS Purdue Khuzdul: Efficient and Scalable Distributed Graph Pattern Mining Engine distributed graph mining engine; extendable embedding abstraction; fine-grained task scheduling; BFS-DFS hybrid exploration 4 4 4"},{"location":"software/ds/#parallel-strategies","title":"Parallel Strategies","text":"<p>Soultion: using the computation and memory resources of multiple processors to solve a problem.</p> <p>Challenge: communication overhead and load balancing</p>"},{"location":"software/ds/#data-parallelism","title":"Data Parallelism","text":"<p>Solution: Data parallelism addresses scenarios where a single GPU can accommodate the model, but the dataset's size necessitates distribution across multiple GPUs for efficient processing and accelerated training.</p> <p>Modern DNN acceleration systems commonly use the combination of data parallelism and model parallelism.</p> Year Venue Authors Title Tags P E N 2012 Nips Google Large Scale Distributed Deep Networks data parallel; use many model to optimize the same data; distributed model training 3 4 3 2014 OSDI CMU Scaling Distributed Machine Learning with the Parameter Server the foundation of tensor parallel; parameter server; pull-based data transfer 4 5 3 2020 SC Miscrosoft ZeRO: Memory Optimizations Toward Training Trillion Parameter Models fix the problem that dp cannot reduce the memory usage on single GPU 3 4 3"},{"location":"software/ds/#model-parallelism","title":"Model Parallelism","text":"<p>Solution: Model parallelism addresses scenarios where the model's size exceeds the processing and memory capacity of a single GPU. There are two types of model parallelism:</p> <ol> <li> <p>Pipeline parallelism: divide the model as pipeline stages, each gpu processes one or more stages.</p> </li> <li> <p>Tensor parallelism: divide the tensor into different GPUs.</p> </li> </ol> <p>Usually, pipeline parallelism and tensor parallelism are used together.</p> Year Venue Authors Title Tags P E N 2019 arXiv NVIDIA Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism transformer based model parallel; pipeline parallel; divide model into different GPUs 3 4 3 2021 SC NVIDIA Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM Megatron2; dive deep into tensor parallelism; how to train a LLM on 1000 GPUs 4 4 3 2022 arXiv NVIDIA Reducing Activation Recomputation in Large Transformer Models Megatron3; sequence parallel; selective activation recomputation; reduce the amount of recomputed activation 3 4 3"},{"location":"software/ds/#llm-specific-parallel-strategies","title":"LLM-specific Parallel Strategies","text":"<p>Focusing on the parallel strategies for LLM-specific deep learning systems.</p> Year Venue Authors Title Tags P E N 2022 ACL NUS Sequence Parallelism: Long Sequence Training from System Perspective splits input sequences into chunks; Ring Self-Attention; sparse attention 3 4 3"},{"location":"software/ds/#cloud-computing-platforms-and-architectures","title":"Cloud computing platforms and architectures","text":"<p>Challenge: when providing services to users, facing scalability, resource management, fault tolerance, and cost-effectiveness for building and deploying large-scale distributed applications and services.</p>"},{"location":"software/ds/#cloud-platform-llm-scheduling","title":"Cloud Platform LLM Scheduling","text":"<p>Challenge: meet the SLO when providing LLM service on cloud platform.</p> Year Venue Authors Title Tags P E N 2025 arXiv Azure TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud Platforms thermal/power property characterization; dynamically adjust in response to power or cooling failures; thermal- and poweraware manner"},{"location":"software/ds/#microservices","title":"Microservices","text":"<p>Focusing on the microservices.</p>"},{"location":"software/ds/#root-cause-analysis","title":"Root Cause analysis","text":"<p>Challenge: accurately and efficiently identify true root causes from massive, noisy, and interdependent monitoring data in complex online service systems.</p> Year Venue Authors Title Tags P E N 2020 ISSRE THU&amp;NKU Unsupervised Detection of Microservice Trace Anomalies through Service-Level Deep Bayesian Networks service trace vector(STV); deep bayesian networks with posterior flows; STV based RCA algorithm 3 3 2 2022 SIGKDD THU&amp;BizSeer Causal Inference-Based Root Cause Analysis for Online Service Systems with Intervention Recognition intervention recognition criterion; causal bayesian network; regression-based hypothesis testing; descendant adjustment 3 3 2 2023 DATE IIE-CAS ImpactTracer: Root Cause Localization in Microservices Based on Fault Propagation Modeling isolation forest for outlier detection; backward tracing algorithm; fault propagation modeling 4 3 2 2023 SIGKDD Microsoft Root Cause Analysis for Microservice Systems via Hierarchical Reinforcement Learning from Human Feedback site reliability engineer experimence for RCA's first stage; human-in-the-loop training; temporal dimension extension 3 4 2"},{"location":"software/ds/#memory-management","title":"Memory Management","text":"<p>Challenge: coordinating memory access and maintaining data consistency across multiple independent nodes with their own local memories, especially when dealing with shared data.</p>"},{"location":"software/ds/#remote-memory","title":"Remote Memory","text":"<p>Challenge: efficiently providing access to memory on a remote node while minimizing latency and overhead, and ensuring consistency and reliability despite network communication complexities and potential failures.</p> Year Venue Authors Title Tags P E N 2020 TC Georgia Tech Hierarchical Orchestration of Disaggregated Memory XMemPod architecture for hierarchical memory orchestration; compressed swap page table (CSPT) for metadata management; hybrid swap-out algorithm for memory utilization; proactive swap-in optimization for performance; RDMA-based remote memory sharing for low-latency access 2025 ATC HUST Fast Distributed Transactions for RDMA-based Disaggregated Memory fast commit protocol by coalescing validation and commit phases; RDMA-enabled offloading for data synchronization; priority-based locking for mission-critical transactions 2 3 4"},{"location":"software/ds/#scratchpad-memory","title":"Scratchpad Memory","text":"<p>Challenge: efficiently allocating and coordinating limited fast memory across distributed nodes to minimize access latency and contention, while ensuring data consistency and scalability.</p> Year Venue Authors Title Tags P E N 2023 ASPLOS Cornell Beyond Static Parallel Loops: Supporting Dynamic Task Parallelism on Manycore Architectures with Software-Managed Scratchpad Memories work-stealing based dynamic task parallelism; stack/task queue in SPM; read-only data duplication 3 3 3"},{"location":"software/ds/#memory-optimization-for-graph-processing","title":"Memory Optimization for Graph Processing","text":"<p>Challenge: efficiently optimize huge memory requirement from graph processing.</p> Year Venue Authors Title Tags P E N 2024 PPoPP KAIST INFINEL: An efficient GPU-based processing method for unpredictable large output graph queries unpredictable large output queries; one-phase GPU graph processing; kernel stop/restart 4 4 3"},{"location":"software/ds/#llm-memory-management","title":"LLM Memory Management","text":"<p>Solution: efficient memory management can reduce memory usage, thus enable larger batch size and higher throughput.</p>"},{"location":"software/ds/#memory-management-algorithms","title":"Memory Management Algorithms","text":"<p>Solution: efficient memory management algorithms, like virtual memory, page table, etc.</p> Year Venue Authors Title Tags P E N 2023 SOSP UCB Efficient Memory Management for Large Language Model Serving with PagedAttention Paged KV-Cache management; Better memory management for larger batch size; Preemptive memory scheduling 4 5 3 2025 ASPLOS Miscrosoft vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention use cuda hardware page table instead of vllm's; hack cuda's driver to support page table modify 2 3 3 2025 arXiv SJTU eLLM: Elastic Memory Management Framework for Efficient LLM Serving activation weight paged; all scene virtual memory; cpu memory swap 3 2 2 2025 arXiv SJTU Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training offline planning and online patch; MoE-aware reuse pockets; time-aware reuse 3 3 2"},{"location":"software/ds/#tradeoff-between-compute-and-memory","title":"Tradeoff between compute and memory","text":"<p>Solution: Transformer is a compute-bound model. To improve the performance, sometimes can use recomputation to reduce the memory usage.</p> Year Venue Authors Title Tags P E N 2022 NIPS Stanford FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Generalized Acceleration of Attention Mechanisms; Change attention to utilize the SRAM on GPU; use recompute to reduce IO burden 4 5 4 2023 ICLR Stanford FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning optimize the thread block parallelization of attention; parallel memory access; reduce no-malmul operation 4 4 3 2024 Nips Stanford FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision Hopper architecture based optimization; fp8 quantization; backward support 3 3 3"},{"location":"software/ds/#general-llm-memory-management","title":"General LLM Memory Management","text":"<p>Challenge: LLM memory management faces challenges like limited HBM memory, efficient KV Cache management, memory sharing between multiple GPUs, multi-level memory management.</p> Year Venue Authors Title Tags P E N 2022 SC Miscrosoft DeepSpeed-Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale kernel fusion; GPU-CPU-NVMe heterogeneous memory; PCIe-based memory prefetch 4 4 3 2025 SOSP THU Jenga: Effective Memory Management for Serving LLM with Heterogeneity two-level LCM allocator; customizable prefix caching; request-aware allocation 4 4 2 2025 FAST THU Mooncake: Trading More Storage for Less Computation \u2014 A KVCache-centric Architecture for Serving LLM Chatbot PD-disaggregate system; kv-cache centered; global kv-cache pool; dynamic SLO scheduler; paged KV-Cache storage 3 4 2 2025 SOSP SJTU KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider real-world KVCache trace anatomy; workload-aware eviction; analysis of KVCache hit in the real-world 4 4 2"},{"location":"software/ds/#application-specific-memory-management","title":"Application specific memory management","text":"<p>Solution: Memory management is actually the core for request scheduleing. Application specific memory management use the application information to manage the memory.</p> Year Venue Authors Title Tags P E N 2025 arXiv UCSD KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows agent grapg; prefetch KV Cache from CPU for next agent; agent-aware prefix cache management 2 2 2"},{"location":"software/ds/#kv-cache-reuse-systems","title":"KV Cache Reuse Systems","text":"<p>Solution: reduce redundant computation and high memory consumption during inference by allowing the reuse of previously computed key-value pairs for shared or repeated parts of input sequences.</p>"},{"location":"software/ds/#prefix-sharing","title":"Prefix Sharing","text":"<p>Solution: reuse KV Cache when the input sequence has shared or repeated parts, use prefix tree to manage KV Cache.</p> Year Venue Authors Title Tags P E N 2023 Nips Stanford SGLang: Efficient Execution of Structured Language Model Programs KV-Cache share; python-like DSL; compute graph; LRU cache management stragety 4 4 3 2024 ACL Microsoft ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition prefix aware attention compute; manage kv-cache chunks as prefix tree; reduce kv-cache redundancy 3 4 2 2024 arXiv Microsoft BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix Sharing and Throughput-oriented Token Batching global prefix tree ahead-of-time; request reorder; horizontal fusioned prefix-shared attention kernel 2024 arXiv Berkeley BlendServe: Optimizing Offline Inference for Auto-regressive Large Models with Resource-aware Batching offline batch inference; resource-aware prefix tree; compute-intensive / memory-intensive requests 2024 arXiv UChicago DroidSpeak: Enhancing Cross-LLM Communication selectively layer reuse; communication protocol for inter-agent exchanges; LLMs that share a common foundational model 2025 arXiv PKU TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained Elastic Long-Context LLM Serving bipartite matching dispatching; segment level cache pooling 3 3 2"},{"location":"software/ds/#kv-cache-store","title":"KV cache store","text":"<p>Solution: store the KV cache in the memory or other storage device, supporting multi-level storage.</p> Year Venue Authors Title Tags P E N 2024 ATC Huawei Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention store KV cache in the memory; multi level KV cache management; position mask modified 3 3 3 2024 SIGCOMM UChicago CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving efficient KV Cache streaming; KV Cache compression; knowledge delivery network; The transfer part of LMCache 3 4 3 2024 EuroSys UChicago CacheBlend: Fast Large Language Model Serving for RAG with  Cached Knowledge Fusion multiple precomputed text chunks; selective KV recompute; sparsity of attention matrices; The system intro of LMCache 3 4 3"},{"location":"software/ds/#other-techniques","title":"Other Techniques","text":"<p>Solution: KV cache reuse techniques beyond prefix sharing. Prefix is a high requirement and is not always possible.</p> Year Venue Authors Title Tags P E N 2024 arXiv Berkeley Optimizing LLM Queries in Relational Workloads prefix sharing maximization; KV cache hit rate; deduplication and cost estimation techniques"},{"location":"software/ds/#kv-cache-storage-systems","title":"KV Cache Storage Systems","text":"<p>Solution: efficiently storing and retrieving the key-value cache, thus reuse when needed.</p> <p>Challenge: the prefetch and eviction of the KV cache, the balance between saving GPU memory and refetching time from the storage device.</p> Year Venue Authors Title Tags P E N 2025 arXiv NVIDIA FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving block-sparse format; customizable attention template; dynamic load-balanced scheduling framework 2025 arXiv PKU FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference imbalanced KV cache compression mitigation; fair-copying for load balancing; best-effort assignment"},{"location":"software/ds/#kv-cache-evict-systems","title":"KV Cache Evict Systems","text":"<p>Challenge: selectively discard the least important key-value pairs to free up memory for longer contexts or larger batch sizes without significantly degrading the model's generation quality or increasing computational overhead for the eviction process itself.</p> Year Venue Authors Title Tags P E N 2023 NIPS UT-Austin H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models sparsity for small cache size; heavy-hitters; greedy algorithm for low-cost policy 2024 arXiv Fujitsu CO2: Precise Attention Score Observation for improving KV Cache Replacement in Large Language Models long measurement step; decay of the accumulated attention score; adjusting FIFO cache size"},{"location":"software/ds/#systems-with-other-caches","title":"Systems with Other Caches","text":"<p>Solution: use other caches (not just KV cache) to improve the performance of LLM inference.</p> Year Venue Authors Title Tags P E N 2025 arXiv KAIST Efficient LLM Inference with Activation Checkpointing and Hybrid Caching activation checkpointing; KV-activation hybrid caching; balanced approach to determine the best ratio"},{"location":"software/ds/#llm-prefetching","title":"LLM Prefetching","text":"<p>Solution: prefetch to avoid memory transfer between different devices, reduce the latency of memory access.</p> Year Venue Authors Title Tags P E N 2025 arXiv Huawei Zurich PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM Serving computational graph-based prefetching; prefetch KV cache to L2 cache"},{"location":"software/ds/#communication-centric-optimization","title":"Communication-Centric Optimization","text":"<p>Challenge: communication is a bottleneck of some distributed systems, trying to reduce the communication.</p>"},{"location":"software/ds/#io-characterization-and-optimization","title":"I/O Characterization and Optimization","text":"<p>Challenge: minimize data movement and maximize resource utilization across heterogeneous distributed environments.</p> Year Venue Authors Title Tags P E N 2020 ASPLOS CMU Livia: Data-Centric Computing Throughout the Memory Hierarchy Memory service programming model; task graphs linked to data location; dynamic task/data scheduling for minimal movement 2 4 3 2025 arXiv UOregon Parallel I/O Characterization and Optimization on Large-Scale HPC Systems: A 360-Degree Survey different HPC I/O stack layers; profiling and tracing tools; tuning echniques 2025 arXiv ETH TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents IO optimization for large scale agent simulation; distributed agent simulation engine; communication optimization for agent scenarios 3 2 2"},{"location":"software/ds/#gpu-gpu-communication","title":"GPU-GPU Communication","text":"<p>Challenge: limited interconnect bandwidth between GPUs using nvLink, PCIe, synchronization delays in parallel workloads, load imbalance across GPUs. The GPU-GPU can also across different nodes.</p> Year Venue Authors Title Tags P E N 2025 arXiv Apple SPD: Sync-Point Drop for efficient tensor parallelism of Large Language Models sync-point drop; block-wise sensitivity analysis; attention output synchronization reduction 2025 arXiv Miscrosoft Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters heterogeneous pipeline stages with flexible GPU counts and types; CPU offloading of both parameters and activations 4 4 2 2025 SIGCOMM PKU InfiniteHBD: Building Datacenter-Scale High-Bandwidth Domain for LLM with Optical Circuit Switching Transceivers optical circuit switching; node-level fault isolation; optimize TP in DCN network 4 3 3"},{"location":"software/ds/#many-core-systems","title":"Many-Core Systems","text":"<p>Challenge: the heterogeneity of cores, the load imbalance, and the communication overhead.</p>"},{"location":"software/ds/#workload-characterization","title":"Workload Characterization","text":"<p>Challenge: dynamic workloads across numerous cores, resource contention for shared hardware.</p> Year Venue Authors Title Tags P E N 2015 VLDB Intel GraphMat: High performance graph analytics made productive vertex program to sparse matrix mapping; generalized SPMV for graph analytics; single-node multicore framework 4 4 4 2018 SC Intel Many-Core Graph Workload Analysis multicore simulator sniper; selective caching and prefetching; heterogeneous high-performance low-power cores 2018 DATE UGA Parallel Code Generation of Synchronous Programs for a Many-core Architecture banked memory mapping; worst-case response time analysis 2025 IPDPS UChicago Optimizing Fine-Grained Parallelism Through Dynamic Load Balancing on Multi-Socket Many-Core Systems lock-less and concurrent task queue xqueue; distributed tree barrier; NUMA-aware redirect push/work stealing"},{"location":"software/ds/#fault-propagation","title":"Fault Propagation","text":"<p>Challenge: one core or component can easily spread to others due to shared resources, leading to system-wide reliability issues. Core counts grow make it hard to predict, detect, and contain errors effectively.</p> Year Venue Authors Title Tags P E N 2008 ASPLOS UIUC Understanding the Propagation of Hard Errors to Software and Implications for Resilient System Design stuck-at fault; bridging fault; software failure detection 2010 PRDC UBC Modeling the Propagation of Intermittent Hardware Faults in Programs instruction based intermittent fault; dynamic dependency graph(DDG) based propagation modeling 2015 SC IBM Understanding the Propagation of Transient Errors in HPC Applications fault propagation in MPI application; fault classification:V,ONA,WO,PEX,C; fault propagation speed factors 2023 ISCA UChicago Understanding and Mitigating Hardware Failures in Deep Learning Training Accelerator Systems NVDLA based fault injection framework; re-execution based light-weight recovery technique; failure effects:SlowDegrade,SharpSlowDegrade,SharpDegrade,LowTestAccuracy 2025 PPoPP University of Oregon ATTNChecker: Highly-Optimized Fault Tolerant Attention for Large Language Model Training Algorithm-Based Fault Tolerance (ABFT); soft error correction; training reliability; INF/NaN error handling 4 4 3"},{"location":"software/ds/#fault-injection-technique","title":"Fault Injection Technique","text":"<p>Challenge: It is difficult to target specific components, reproduce realistic fault scenarios, and observe system behavior without disturbing normal operation, especially as system scale and complexity increase.</p> Year Venue Authors Title Tags P E N 2008 VLSI DISCA Enhancement of Fault Injection Techniques Based on the Modification of VHDL Code saboteurs and mutants technique based fault injection; VHDL level fault-tolerance mechanism 2014 DSN UBC Quantifying the Accuracy of High-Level Fault Injection Techniques for Hardware Faults fault injection quantification; assembly level fault injection; LLVM compiler based fault injector"},{"location":"software/ds/#communication","title":"Communication","text":"<p>Challenge: efficiently managing data exchange between a large number of cores, due to limited bandwidth, high latency, and contention in shared resources like interconnects and memory.</p> Year Venue Authors Title Tags P E N 2025 arXiv UCLM Understanding intra-node communication in HPC systems and Datacenters intra- and inter-node simulation model; intra-node network interface bottleneck; impacts of communication pattern"},{"location":"software/ds/#heterogeneous-systems","title":"Heterogeneous Systems","text":"<p>Heterogeneous systems are systems that have different types of processors, such as CPUs and GPUs.</p> <p>Solution: ultilize the heterogeneous resources to improve the performance.</p>"},{"location":"software/ds/#general-applications","title":"General Applications","text":"Year Venue Authors Title Tags P E N 2013 SOSP MSR Silicon Valley Dandelion: a Compiler and Runtime for Heterogeneous  Systems unified programming model; \u201csingle machine\u201d abstraction; a rich object-oriented programming language for data-parallel computing 2025 EuroSys SJTU Improving GPU Sharing Performance through Adaptive Bubbleless Spatial-Temporal Sharing Bubble-less spatial-temporal sharing; kernel squad scheduling; fine-grained concurrent kernel management 4 3 2 2025 ISPASS CMU Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Coupled Architectures Effective regions for balanced utilization of PUs; Proximity-based kernel fusion recommendation; operator-kernel dependency graphs from PyTorch Profiler traces 3 4 2 2025 OSDI SJTU XSched: Preemptive Scheduling for Diverse XPUs a unified XPU usage interface layer; queue abstraction turns every XPU into a preemptible command queue; enable microsecond preemption on closed-source CUDA kernels 4 4 3"},{"location":"software/ds/#decentralized-serving","title":"Decentralized Serving","text":"<p>Challenge: managing diverse hardware and software environments, balancing workloads across uneven resources, minimizing communication overhead, ensuring consistency without centralized control.</p> Year Venue Authors Title Tags P E N 2019 ASPLOS USC Hop: Heterogeneity-aware Decentralized Training iteration gap; queue-based synchronization; backup workers and bounded staleness 2020 ASPLOS USC Prague: High-Performance Heterogeneity-Aware Asynchronous Decentralized Training Partial All-Reduce to reduce synchronization cost; group scheduling to avoid conflicts 2025 arXiv Berkeley DeServe: Towards Affordable Offline LLM Inference via Decentralization decentralized LLM inference; high-latency optimization; idle GPU utilization; modular on-chain integration 2025 arXiv HKUST DreamDDP: Accelerating Data Parallel Distributed LLM Training with Layer-wise Scheduled Partial Synchronization partial synchronization based local SGD; DFS algorithm with pruned search space; enables the opportunity of overlapping communication and computation"},{"location":"software/ds/#ml-training-systems","title":"ML Training Systems","text":"<p>Solution: balance between faster training and high precision.</p> Year Venue Authors Title Tags P E N 2023 SOSP CMU Sia: Heterogeneity-aware, goodput-optimized ML-cluster scheduling heterogeneity-aware and adaptivity-aware; ILP formulation for scheduling; bootstrapped from observing just a few mini-batches 2023 SOSP Rice&amp;AWS Gemini: Fast Failure Recovery in Distributed Training with In-Memory Checkpoints holistic trace analysis; cross-layer failure detection; checkpoint partition algorithm 4 4 2"},{"location":"software/ds/#llm-inference-heterogeneous-systems","title":"LLM Inference Heterogeneous Systems","text":"<p>Solution: managing diverse hardware and software environments, balancing workloads across uneven resources, meeting the SLO.</p>"},{"location":"software/ds/#mobile-edge-network-serving","title":"Mobile &amp; Edge-Network Serving","text":"<p>Challenge: limited computation, memory, power coupled with intermittent and unreliable network connectivity, making it difficult to perform computationally intensive training tasks, manage large datasets, and ensure efficient communication and synchronization across distributed edge nodes.</p> Year Venue Authors Title Tags P E N 2024 arXiv UIC Priority-Aware Model-Distributed Inference at Edge Networks priority-aware model distributed inference algorithm; prioritization of ML inference tasks; model-distributed inferencing mechanism 2024 arXiv Yonsei Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large Language Models hybrid language model; selectively skip uplink transmissions; uncertainty-aware 2024 arXiv UMD Distributed Mixture-of-Agents for Edge Inference with Large Language Models Mixture-of-Agents; semantics of the data being gossiped and its timeliness; queuing stability 2025 arXiv PKU SplitLLM: Hierarchical Split Learning for Large Language Model over Wireless Network hierarchical split learning; edge-cloud collaboration; LoRA adapter update 2025 arXiv SJTU HeteroLLM: Accelerating Large Language Model Inference on Mobile SoCs platform with Heterogeneous AI Accelerators both layer-level and tensor-level GPU-NPU parallelism; different tensor partition strategies; fast synchronization mechanism based on predictable kernel waiting times; tensor partition solver"},{"location":"software/ds/#gpu-gpu-heterogeneous-system","title":"GPU-GPU Heterogeneous System","text":"<p>Solution: the system is composed of heterogeneous GPUs and not inferencing on CPU/ The system need to manage the heterogeneous GPUs' communication and memory.</p> Year Venue Authors Title Tags P E N 2024 arXiv CMU Helix: Distributed Serving of Large Language Models via Max-Flow on Heterogeneous GPUs LLM model placement as a max-flow problem; per-request pipeline; mixed integer linear programming 2025 ICLR HKUST HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous Environment a combination of graph partitioning and max-flow algorithm; TP and PP with disaggregation; bottleneck and underutilized edges; swap edges 2025 arXiv UM Hetis: Serving LLMs in Heterogeneous GPU Clusters with Fine-grained and Dynamic Parallelism dynamic parallelism design; LLM module level optimization; selective parallelism for dense modules 3 3 3"},{"location":"software/ds/#xpu-gpu-heterogeneous-system","title":"XPU-GPU Heterogeneous System","text":"<p>Challenge: effectively managing and coordinating diverse hardware (CPUs, TPUs, etc.), interconnects, and memory hierarchies</p> Year Venue Authors Title Tags P E N 2023 ICML Stanford FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU dynamic offload tensor; quantize the weights to 4-bits; linear aggregation of the store and load operations 4 4 3 2025 arXiv CMU Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Coupled Architectures SKIP profiling tool; TKLQT metric for CPU/GPU boundedness; proximity score kernel fusion 2 3 2 2025 SPAA Huawei WindVE: Collaborative CPU-NPU Vector Embedding seamless CPU-NPU collaboration for vector embedding; linear regression based estimator; high-throughput offloading vector embedding 2 4 3 2025 arXiv Huawei High-Throughput LLM inference on Heterogeneous Clusters lightweight profiling while avoiding resource-intensive throughput benchmarks; a scheduler that accounts for both instance computational capacity and memory usage; exhaustive search method 2 4 2 2025 ISCA KAIST EOD: Enabling Low Latency GNN Inference via Near-Memory Concatenate Aggregation concatenated ZVC compression; precomputation for neighborhood explosion problem 2 3 2 2025 ISCA UIUC LIA: A Single-GPU LLM Inference Acceleration with Cooperative AMX-Enabled CPU-GPU Computation and CXL Offloading heterogeneous LLM offloading via AMX; optimal compute-offloading policy for LLM inference; CXL-DDR hybrid memory offloading for throughput 3 4 2"},{"location":"software/ds/#heterogeneous-device-task-scheduling","title":"Heterogeneous Device Task Scheduling","text":"<p>Solution: assigning different parts of the LLM serving workload to the most suitable heterogeneous devices to maximize throughput and minimize latency.</p> Year Venue Authors Title Tags P E N 2023 PACT Yonsei Virtual PIM: Resource-aware Dynamic DPU Allocation and Workload Scheduling Framework for Multi-DPU PIM Architecture dynamic DPU allocation for multitasking; fine-grained scheduling 3 2 2 2025 arXiv NUS Data-aware Dynamic Execution of Irregular Workloads on Heterogeneous Systems lightweight and input-aware framework; multiobjective and multi-constraint design space; dynamically creating optimal schedules 2025 HPCA Samsung PAISE: PIM-Accelerated Inference Scheduling Engine for Transformer-based LLM task scheduling algorithm across host and PIM; interleave-batched GEMM; data layout adjustment 2 3 3"},{"location":"software/ds/#task-scheduling-for-specific-tasks","title":"Task Scheduling for specific tasks","text":"<p>Solution: In specific scene, the schedule goal is different. Assigning tasks to differnet devices can fix the gap between the characteristics of devices' and tasks'.</p> Year Venue Authors Title Tags P E N 2023 HPCA Princeton Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications distributed data-local tiled architecture; task-based programming for pointer indirection; traffic-aware task scheduling with headerless NoC 3 3 3 2025 arXiv Georgia Tech HARP: A Taxonomy for Heterogeneous and Hierarchical Processors for Mixed-reuse Workloads a taxonomy to classify the heterogeneous and hierarchical accelerators; characterize hardware organization of different accelerators; classify based on relative location of sub-accelerators 2025 arXiv PKU Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC agent application-specific scheduling on heterogeneous SoC; heterogeneous execution graph with eastic kernels; bandwidth-aware dispatch for NPU-iGPU contention mitigation 3 2 3 2025 arXiv Stanford Efficient and Scalable Agentic AI with Heterogeneous Systems mapping agent worloads to heterogeneous accelerators; MLIR based excution graph; cost model based device choose 2 1 1"},{"location":"software/ds/#llm-training-heterogeneous-systems","title":"LLM Training Heterogeneous Systems","text":"<p>Solution: compared to LLM Inference Heterogeneous Systems, need to solve the backward compatibility and heterogeneity issues.</p> Year Venue Authors Title Tags P E N 2024 arXiv PKU Demystifying Workload Imbalances in Large Transformer Model Training over Variable-length Sequences data sampling imbalance; data packing imbalance; subgraph abstraction 2024 arXiv Ant Group EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large Language Models Local Stochastic Gradient Descent (Local SGD); consistent stragglers within heterogeneous devices; hierarchical distribution strategy on a two-dimensional device mesh; layer by layer forward syncing; pseudo-gradient penalty method 2024 arXiv ZJU Frenzy: A Memory-Aware Serverless LLM Training System for Heterogeneous GPU Clusters efficient and low-overhead task-to-cluster scheduling; bin-packing algorithms; seamless and user-friendly 2025 arXiv OSU Scaling Large Language Model Training on Frontier with Low-Bandwidth Partitioning low-bandwidth interconnects; three-level hierarchical partitioning strategy; improved hierarchical partitioning on top of ZeRO++ 2025 arXiv PKU Split Fine-Tuning for Large Language Models in Wireless Networks split fine-tuning; device and server partition; novel compression scheme and resource management algorithm 2025 arXiv Neuchatel SkipPipe: Partial and Reordered Pipelining Framework for Training LLMs in Heterogeneous Networks partial pipeline parallelism; stage skipping; path scheduling algorithm"},{"location":"software/ds/#schedule-optimization","title":"Schedule Optimization","text":"<p>Solution: develop task schedule algorithms, to achieve efficient overall system performance despite incomplete and evolving system state information.performance.</p>"},{"location":"software/ds/#general-task-scheduling","title":"General Task Scheduling","text":"<p>Solution: optimizing the allocation and execution of diverse and dynamic workloads.</p> Year Venue Authors Title Tags P E N 2019 NSDI MIT Shinjuku: Preemptive Scheduling for \u00b5second-scale Tail Latency preemptive scheduling; single-address space OS; hardware-supported virtualization 2021 SOSP UPenn When Idling is Ideal: Optimizing Tail-Latency for Heavy-Tailed Datacenter Workloads with Pers\u00e9phone reserve cores; non-conserving; request dispatching algorithm 2017 HPCA UGent Reliability-Aware Scheduling on Heterogeneous Multicore Processors core reliability characteristics difference; system soft error rate; sampling-based reliability-aware scheduling algorithm 2020 TCAD ASU Runtime Task Scheduling Using Imitation Learning for Heterogeneous Many-Core Systems offline Oracle optimizaion strategy; hierarchical imitation learning based scheduling; two-level scheduling"},{"location":"software/ds/#speculative-execution-non-llm","title":"Speculative Execution (Non-LLM)","text":"<p>Solution: balancing the potential performance gains from speculative executions, including accurately predicting outcomes, handling incorrect speculations and their side effects across multiple nodes.</p> <p>Refer to Speculative Execution for the speculative execution algorithms for LLM.</p> Year Venue Authors Title Tags P E N 2024 arXiv MSR Forerunner: Constraint-based Speculative Transaction Execution for Ethereum constraint-based speculative transaction execution; many-future nature; specialized fast-path program 2024 arXiv Politecnico di Milano Minimizing speculation overhead in a parallel recognizer for regular texts speculation overhead; chunk automaton; reduced-interface DFA"},{"location":"software/ds/#llm-related-scheduling","title":"LLM-Related Scheduling","text":"<p>Challenge: efficiently managing the immense computational and memory demands of training and inference across numerous interconnected devices, requiring sophisticated strategies to partition massive models.</p>"},{"location":"software/ds/#llm-request-scheduling","title":"LLM Request Scheduling","text":"<p>Solution: develop intelligent strategies to route requests, prioritize urgent or critical tasks, handle varying input lengths and complexities, manage resource contention to meet the SLO requirements.</p> Year Venue Authors Title Tags P E N 2024 arXiv Yale TimelyLLM: Segmented LLM Serving System for Time-sensitive Robotic Applications segmented generation; time-sensitive scheduling; latency-guided batch size selection 2025 arXiv MSRI Niyama : Breaking the Silos of LLM Inference Serving QoS-driven LLM inference serving system; co-scheduling requests with diverse QoS targets on a shared rather than siloed infrastructure; allows graceful service degradation during overload conditions; deadline slack; a hybrid prioritization and an eager relegation policy 4 4 3 2025 arXiv MIT Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints fluid dynamics approximation; Waiting for Accumulated Inference Threshold; a hierarchical framework comprising multiple segments 3 4 2 2025 arXiv PKU SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large Language Model Inference service-aware and latency-optimized scheduling algorithm; doubling budget (DB) scheduling algorithm; search-based placement algorithm 3 4 2 2025 SOSP UChicago PrefillOnly: An Inference Engine for Prefill-only Workloads in Large Language Model Applications prefill-only workload; hybrid prefilling; continuous JCT calibration; JCT-aware scheduling 4 4 4"},{"location":"software/ds/#batching-for-better-throughput","title":"Batching for Better Throughput","text":"<p>Challenge: the batching is a trade-off between the throughput and the latency. The requests in the same batch also call for different resources. Split different compute pattern request for different batches is a good idea.</p> Year Venue Authors Title Tags P E N 2025 OSDI UA NanoFlow: Towards Optimal Large Language Model Serving Throughput LLM inference is indeed compute-bound; model-based batch size analysis; micro batch split for overlapping between all resources 4 4 3"},{"location":"software/ds/#info-predict-scheduling","title":"Info Predict Scheduling","text":"<p>Challenge: The general schedule if for better batching and meeting the SLO requirements. By predicting the information of the requests, we can make the schedule more efficient.</p> Year Venue Authors Title Tags P E N 2023 Nips Harvard S3: Increasing GPU Utilization during Generative Inference for Higher Throughput predict the length of LLM request to fixed types; Orca based dynamic batching 3 2 3 2024 ASPLOS UIUC Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction length prediction; left time prediction; bert-based proxy model 4 3 2"},{"location":"software/ds/#llm-application-level-scheduling","title":"LLM Application-Level Scheduling","text":"<p>Solution: to optimize the end-to-end latency of the application, including the scheduling of the LLM instances.</p> Year Venue Authors Title Tags P E N 2024 OSDI SJTU Parrot: Efficient Serving of LLM-based Applications with Semantic Variable Semantic Variable; application-level information; LLM applications as first-class citizens 2024 OSDI CUHK Teola: Towards End-to-End Optimization of LLM-based Applications mismatch between request-level scheduling and end-to-end  application performance; primitive-level dataflow graph; two-tier scheduling mechanism 2024 arXiv Yext SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to Question Answering constantly changing and sometimes adverse conditions; Dynamically Reconfigurable Horizontal Scaling Framework; dynamically adjust resource allocation based on query requirements 2025 arXiv Berkeley Autellix: An Efficient Serving Engine for LLM Agents as General Programs formalize agentic programs as dynamic, non-deterministic DAGs; non-clairvoyant scheduler; simple load-balancing policy to balance data locality and KV-cache recomputation 2025 ICDCS SJTU LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM Applications a DAG with regular stage, LLM stage, dynamic stage; bayesian network-based profiler; identify uncertainty-reducing stages 4 4 3 2025 arXiv SJTU Efficient Serving of LLM Applications with Probabilistic Demand Modeling DAG-based scheduling; dynamic excution; cpu excutor warmup 3 1 1"},{"location":"software/ds/#llm-speculative-inference","title":"LLM Speculative Inference","text":"<p>Refer to non-LLM speculative execution.</p> Year Venue Authors Title Tags P E N 2024 arXiv F&amp;M College AMUSD: Asynchronous Multi-Device Speculative Decoding for LLM Acceleration simultaneous and independent predictions; asynchronous speculative decoding; rollback mechanism 2024 arXiv Purdue Constrained Decoding with Speculative Lookaheads computational expense of generating lookaheads; speculated lookaheads; task specific reward function 2024 arXiv Rutgers Interactive Speculative Planning: Enhance Agent Efficiency through Co-design of System and User Interface active user intervention; speculative planning algorithm; UI-level rescheduling algorithm 2024 arXiv USTC Parallel Speculative Decoding with Adaptive Draft Length adaptive draft length; pre-verify and post-verify; draft-then-verify framework; mutual waiting problem 2024 arXiv SEU SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative Decoding reasoning tree construction; parallel drafting with speculative decoding; FCFS queue verification"},{"location":"software/ds/#spec-others","title":"Spec + Others","text":"Year Venue Authors Title Tags P E N 2025 arXiv Huawei Speculative MoE: Communication Efficient Parallel MoE Inference with Speculative Token and Expert Pre-scheduling speculative MoE; speculative token shuffling; speculative expert pre-grouping 2025 INFOCOM UoA SPIN: Accelerating Large Language Model Inference with Heterogeneous Speculative Models internal neurons sparsification; model-agnostic acceleration framework; dynamic early-exit thresholds; multi-layered feature fusion 2025 arXic SUST FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference SPEC on memory limited dedvices; Efficient draft management with tree pruning and early stop reduces redundancy and maintains causal relationships 3 3 3"},{"location":"software/ds/#llm-serving-outages-and-incidents","title":"LLM Serving Outages and Incidents","text":"Year Venue Authors Title Tags P E N 2025 arXiv Vrije Universiteit Amsterdam An Empirical Characterization of Outages and Incidents in Public Services for Large Language Models empirical characterization of outages; failure recovery optimization; public LLM service reliability"},{"location":"software/ds/#energy-optimized-llm-scheduling","title":"Energy-Optimized LLM Scheduling","text":"Year Venue Authors Title Tags P E N 2025 arXiv UvA GREEN-CODE: Optimizing Energy Efficiency in Large Language Models for Code Generation dynamic early exit; energy-aware code generation; reinforcement learning for llms"},{"location":"software/ds/#multi-llm-scheduling","title":"Multi-LLM Scheduling","text":"Year Venue Authors Title Tags P E N 2025 arXiv UCLA Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving Long-tail model popularity; Frequent idle periods; Rapid workload fluctuations 3 4 2"},{"location":"software/ds/#dnn-scheduling","title":"DNN Scheduling","text":"<p>Solution: optimizing data parallelism and model parallelism while minimizing communication overhead between nodes, effectively managing limited GPU memory and other resources to achieve scalability and high throughput.</p> <p>Refer to LLM-Related Scheduling for the LLM-related scheduling algorithms.</p>"},{"location":"software/ds/#task-offloading","title":"Task Offloading","text":"Year Venue Authors Title Tags P E N 2024 arXiv USTC Collaborative Inference for Large Models with Task Offloading and Early Exiting early exit mechanism; jointly optimize its offloading strategy and the confidence threshold; distributed task offloading algorithm 2025 ISCA ETHZ OptiPIM: Optimizing Processing-in-Memory Acceleration Using Integer Linear Programming integer linear programming for offload optimization; PIM-friendly mapping representation; accurate cost modeling for data layout 4 2 3"},{"location":"software/ds/#thermal-aware-scheduling","title":"Thermal-Aware Scheduling","text":"<p>Challenge: balancing real-time performance with thermal safety under tight power budgets</p> Year Venue Authors Title Tags P E N 2012 TVLSI SU A Multi-Agent Framework for Thermal Aware Task Migration in Many-Core Systems steady-state temperature-based migration; temperature prediction-based migration; master-slave communication 3 3 2 2015 TVLSI UCR Task Migrations for Distributed Thermal Management Considering Transient Effects effective initial temperature; frequency-domain moment matching 3 3 2 2019 TVLSI Soton Predictive Thermal Management for Energy-Efficient Execution of Concurrent Applications on Heterogeneous Multicores temperature predictor with error correction algorithm; dynamic temperature management 3 4 2 2019 TECS UMich Thermal-Aware Scheduling for Integrated CPUs\u2013GPU Platforms thermally-balanced task-to-core assignment; CPUs\u2013GPU thermal coupling model 3 3 2 2022 TPDS SUT TherMa-MiCs: Thermal-Aware Scheduling for Fault-Tolerant Mixed-Criticality Systems thermal safe power abstraction; maximum safe simultaneous active cores (MSSAC) factor; offline&amp;online co-scheduling 4 4 2"},{"location":"software/ds/#dark-silicon-optimization","title":"Dark Silicon Optimization","text":"<p>Challenge: maximizing performance and reliability within dark silicon constraints</p> Year Venue Authors Title Tags P E N 2015 DATE NYU Variability-Aware Dark Silicon Management in On-Chip Many-Core Systems variability-aware dark core patterning; joint thread-to-core mapping and power-state control; temperature-dependent leakage feedback modeling 3 3 2 2018 DATE NTU HiMap: A Hierarchical Mapping Approach for Enhancing Lifetime Reliability of Dark Silicon Manycore Systems two-level hierarchical mapping for lifetime reliability; block-based selection; cluster-based adjustment 4 3 2 2019 DAC NTU LifeGuard: A Reinforcement Learning-Based Task Mapping Strategy for Performance-Centric Aging Management performance- and aging-aware objective formulation; reaction\u2013diffusion (R\u2013D)-based NBTI aging model 3 3 2"},{"location":"software/ds/#general-optimizations-for-deep-learning-systems","title":"General optimizations for Deep Learning Systems","text":"<p>Solution: general optimizations for deep learning systems.</p> <p>If the paper is focusing on an above-mentioned specific scene (e.g., memory, scheduling, IO, etc.), it will be put in the corresponding section.</p>"},{"location":"software/ds/#llm-training-systems","title":"LLM Training Systems","text":"<p>Solution: arrange model parameters and data across multiple devices, reduce the time spent communicating, scale up smoothly as models and data keep growing\u2014all while staying efficient and speeding up training.</p>"},{"location":"software/ds/#general-optimizations","title":"General Optimizations","text":"Year Venue Authors Title Tags P E N 2025 arXiv THU Enhancing Memory Efficiency in Large Language Model Training Through Chronos-aware Pipeline Parallelism chronos-aware pipeline parallelism; temporal locality optimization; activation balancing 2025 arXiv NUS PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization selective offload strategy; memory offload optimization; pipeline parallelism scalability; lifespan-based offloading 2025 arXiv UCSD WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training workload-aware variable-length document packing; per-document sharding strategy; adaptive sharding selection mechanism; delay execution of extremely long documents 4 5 2 2025 EuroSys UToronto Mist: Efficient Distributed Training of Large Language Models via Memory-Parallelism Co-Optimization fine-grained overlap-centric scheduling; symbolic-based performance analysis; imbalance-aware hierarchical tuning 4 4 2 2025 arXiv ByteDance veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD guaranteed single device consistency; zero overhead tensor alloc mode; fused communication 3 3 3 2025 PPoPP THU WeiPipe: Weight Pipeline Parallelism for Communication-Effective Long-Context Large Model Training weight-passing pipeline parallelism; long-context training; communication optimization 4 4 2"},{"location":"software/ds/#optimizations-on-special-scene","title":"Optimizations on Special Scene","text":"Year Venue Authors Title Tags P E N 2025 arXiv HKU Hecate: Unlocking Efficient Sparse Model Training via Fully Sharded Sparse Data Parallelism Fully Sharded Sparse Data Parallelism (FSSDP); sparsely materializes MoE parameters; two sparse collective communications 2025 arXiv SJTU PipeWeaver: Addressing Data Dynamicity in Large Multimodal Model Training with Dynamic Interleaved Pipeline dynamic interleaved pipeline; hierarchical schedule space for rapid pipeline schedule search; spatialtemporal subgraph reuse 3 4 2"},{"location":"software/ds/#experiments","title":"Experiments","text":"Year Venue Authors Title Tags P E N 2025 arXiv JSC Memory and Bandwidth are All You Need for Fully Sharded Data Parallel an extensive analysis of the FSDP training distribution strategy; a grid search methodology; both simulation and empirical results 2 4 1"},{"location":"software/ds/#multi-modal-optimizations","title":"Multi-Modal Optimizations","text":"<p>Challenge: multimodal data is more complex and requires more resources to train.</p> Year Venue Authors Title Tags P E N 2025 HPCA Meta Characterizing and Efficiently Accelerating Multimodal Generation Model Inference system performance characterization for multi-modal generative AI tasks; optimized baseline for inference optimization 4 3 1 2025 arXiv ByteDance OrchMLLM: Orchestrate Multimodal Data with Batch Post-Balancing to Accelerate Multimodal Large Language Model Training multimodal mini-batch imbalance; batch post-balancing algorithm; node-wise all-to-all communicator for practical rearrangement of mini-batches 4 4 3 2025 arXiv ICT ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism unified prefix cache fusing vision and text tokens; modality-aware load balancer for bursty vision traffic 2 3 2 2024 NSDI OSU&amp;&amp;Amazon DISTMM\uff1aAccelerating Distributed Multimodal Model Training Heterogeneous Submodules;Modality-aware Partitioner;Data Load Balancer;Heterogeneity-aware Placement Manager;Pipeline Executor 5 5 4 2025 ATC Harvard&amp;&amp;ByteDance&amp;&amp;USC Optimus: Accelerating Large-Scale Multimodal LLM Training by Bubble Exploitation Separate Parallel Plans;Dual-Stage Dependency Management;Kernel-Level Bubble Scheduling;Schedule encoder computations within LLM bubbles 4 5 4"},{"location":"software/ds/#kernel-level-optimizations","title":"Kernel-Level Optimizations","text":"Year Venue Authors Title Tags P E N 2025 arXiv HUST CFP: Low-overhead Profiling-based Intra-operator Parallelism Generation by Preserving Communication-Free Structures model segment profile-based cost model; communication-free tensor partition propagation property; extracting a set of unique model segments; Communication-Free Preserve 4 5 3"},{"location":"software/ds/#sft-and-fine-tuning-optimizations","title":"SFT and Fine-tuning Optimizations","text":"<p>Challenge: SFT plays an important role to enhance model's ability on specific tasks. Thus SFT has a strong taks's pattern, like the distribution of input-output length.</p> Year Venue Authors Title Tags P E N 2025 arXiv PKU LobRA: Multi-tenant Fine-tuning over Heterogeneous Data Deploys different model replicas to handle varied workloads; balancing different LoRAs' workload caused by dataset 3 3 2"},{"location":"software/ds/#llm-inference-systems","title":"LLM Inference Systems","text":"<p>Focusing on the optimizations for LLM inference systems.</p> Year Venue Authors Title Tags P E N 2025 ISCA DeepSeek Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures software-hardware co-design for deepseek-v3; insight into hardware for ai architectures 5 5 4 2024 Mlsys SJTU FlashDecoding++: Faster Large Language Model Inference on GPUs asynchronized softmax with unified max value; flat GEMM optimization with double buffering; heuristic dataflow with hardware resource adaptation 4 4 3 2024 MICRO UC Irvine SCAR: Scheduling Multi-Model AI Workloads on Heterogeneous Multi-Chiplet Module Accelerators two-level scheduling framework; greedy layer packing algorithm for time window assignment; inter-chiplet pipelining and dynamic chiplet regrouping 3 4 2"},{"location":"software/ds/#slo-aware-systems","title":"SLO-Aware Systems","text":"<p>Challenge: providing service for users to meet specific latency requirements with limited resources.</p> Year Venue Authors Title Tags P E N 2025 arXiv Berkeley AdaServe: SLO-Customized LLM Serving with Fine-Grained Speculative Decoding fine-grained speculative decoding; token tree verification; slo customization 2025 arXiv UIUC HyGen: Efficient LLM Serving via Elastic Online-Offline Request Co-location online-offline request co-location; interference-aware profiler; latency predictor; adaptive scheduler 2025 arXiv PKU Memory Offloading for Large Language Model Inference with Latency SLO Guarantees effectively captures the tension between meeting SLOs and maximizing host memory usage; dynamic offloading interval; per-bus coordinator 2025 arXiv Huawei Hybrid Offline-online Scheduling Method for Large Language Model Inference Optimization hybrid offline-online scheduling; preemptive scheduling for hardware utilization; lagrangian method for cost efficiency evaluation 2025 ASPLOS BUAA Past-Future Scheduler for LLM Serving under SLA Guarantees lightLLM; predict future system memory usage; reduce evict by better request scheduling 3 2 3"},{"location":"software/ds/#surveys","title":"Surveys","text":""},{"location":"software/ds/#system-optimization-surveys","title":"System Optimization Surveys","text":"Year Venue Authors Title Tags P E N 2024 arXiv NEU LLM Inference Serving: Survey of Recent Advances and Opportunities KV cache and memory management; LLM computation optimization; Cloud LLM deployment; focus on system-level enhancements 2024 arXiv CUHK A Survey on Inference Optimization Techniques for Mixture of Experts Models model compression; expert skip; expert merge; sparse to dense; expert parallel; expert offloading 2024 arXiv PolyU A Survey on Large Language Model Acceleration based on KV Cache Management cache selection; budget allocation; cache merging; cache quantization; cache low-rank decomposition; attention grouping and sharing; memory management; hardware-aware design 2025 arXiv THU Beyond A Single AI Cluster: A Survey of Decentralized LLM Training resource-driven paradigm; community-driven decentralization; organizational decentralization; decentralized LLM training taxonomy 2025 arXiv FIU Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions distributed solutions for LMs; workload imbalance in LLM training; M-ICL; model security enhancement"},{"location":"software/ds/#application-surveys","title":"Application Surveys","text":"Year Venue Authors Title Tags P E N 2024 arXiv PKU Retrieval-Augmented Generation for AI-Generated Content: A Survey Query Transformation; Data Augmentation; Recursive Retrieval; Chunk Optimization; Retriever Finetuning; Hybrid Retrieval; Re-ranking; Retrieval Transformation; Prompt Engineering; Decoding Tuning; Generator Finetuning; Output Rewrite; Adaptive Retrieval; Iterative RAG 2024 arXiv WHU A survey on LLM-based multi-agent systems: workflow, infrastructure, and challenges personalized characteristics; perceive environmental information; utilize memory mechanisms; mutual interaction; agent self-reflection 2024 arXiv PolyU Deploying Foundation Model Powered Agent Services: A Survey FM-powered agent services within the edge-cloud environment; low-level hardware perspective; high-level software perspective"},{"location":"software/ds/#multimodal-systems","title":"Multimodal Systems","text":"Year Venue Authors Title Tags P E N 2025 arXiv UW\u2013Madison LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models query-block distributed exchange; shared visual token recomputation; sequence-parallelism with minimal communication overhead 2025 arXiv Microsoft Towards Efficient Large Multimodal Model Serving fine-grained stage-aware resource management; multimodal workload-specific scheduling; model architecture-specific optimizations 2025 arXiv Huawei Efficiently Serving Large Multimedia Models Using EPD Disaggregation encode-prefill-decode disaggregation; multimodal cache; intra-request parallel 2025 arXiv TU/e Fine-tuning Multimodal Transformers on Edge: A Parallel Split Learning Approach Multimodal Parallel Split Learning; computation-efficient training; server-side loss aggregation mechanism 2025 arXiv HUST FastCache: Optimizing Multimodal LLM Serving through Lightweight KV-Cache Compression Framework resource-aware KV-cache memory pool; multimodal KV-cache compression; modality-specific compression"},{"location":"software/ds/#mixture-of-experts-llm-systems","title":"Mixture-of-Experts LLM Systems","text":"<p>Challenge: efficiently coordinating and scaling expert models across multiple nodes, leading to issues like uneven workload distribution, high communication overhead, and difficulty in fault tolerance.</p>"},{"location":"software/ds/#expert-offloading-and-placement","title":"Expert Offloading and Placement","text":"Year Venue Authors Title Tags P E N 2025 DATE Berkeley DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient MoE Inference data-aware offloading; predictive pre-calculation; sequence-specific expert allocation 2025 arXiv Stevens Tech fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts Serving expert map; iteration-level probability distributions; track fine-grained input semantic embeddings; semantic-based and trajectorybased 2025 arXiv Georgia Tech MoETuner: Optimized Mixture of Expert Serving with Balanced Expert Placement and Token Routing ILP for expert placement; cross-layer dependencies; minimizing total dispatched token number 2025 EuroMLSys EPFL Accelerating MoE Model Inference with Expert Sharding expert sharding for load balancing; tensor sharding for moe experts; fused expert computations for reduced kernel launches 2025 DAC PKU HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient MoE Inference dynamically balances workloads across GPUs and CPUs; impact-driven prefetching; MoE-specialized cache management 3 4 2"},{"location":"software/ds/#batching-and-scheduling","title":"Batching and Scheduling","text":"Year Venue Authors Title Tags P E N 2025 arXiv Alibaba Static Batching of Irregular Workloads on GPUs: Framework and Application to Efficient MoE Model Inference statically batching irregular workloads; batch-task-tile partition; decompress the mapping and dispatch the workload 2025 arXiv Edinburgh MoE-Gen: High-Throughput MoE Inference on a Single GPU with Module-Based Batching module-based batching; high-throughput MoE inference; full KV-cache offloading 2025 arXiv KTH Priority-Aware Preemptive Scheduling for Mixed-Priority Workloads in MoE Inference fine-grained preemption; priority-aware scheduling; per-expert queues; expert-level preemption 2025 arXiv UMich MoE-Lens: Towards the Hardware Limit of High-Throughput MoE LLM Serving Under Resource Constraints two-stage performance modeling; analyzes the theoretical performance upper bound; captures how system execution mechanisms 4 4 2 2025 Arxiv Nvidia MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core decouples parallelization strategies for attention and MoE layers; flexible and efficient token-level dispatcher; 5-D hybrid parallelism 4 5 2"},{"location":"software/ds/#memory-and-communication-efficiency","title":"Memory and Communication Efficiency","text":"Year Venue Authors Title Tags P E N 2025 arXiv ByteDance Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts fine-grained communication-computation overlapping for efficient MoE execution; dependency resolving method; adaptive workload assignment method; shared data buffers between communication and computation operations 2025 arXiv UVA eMoE: Task-aware Memory Efficient Mixture-of-Experts-Based (MoE) Model Inference expert prediction; task-aware expert loading; task-aware request scheduling 2025 mobiCom HKUST D$^{2}$MoE: Dual Routing and Dynamic Scheduling for Efficient On-Device MoE-based LLM Serving dually sparselygated Mixture-of-Experts; token-adaptive bit-width selection; matryoshka weight quantization; bit-width-aware I/O-compute pipeline 3 4 4 2025 ODSI SJTU Fast and Live Model Auto Scaling with O(1) Host Caching auto-scaling with minimal caching; optimize parameter loading; enabling fine-grained layer-level scale 3 3 2 2023 ASPLOS Google TelaMalloc: Efficient On-Chip Memory Allocation for Production Machine Learning Accelerators hybrid heuristic-solver memory allocator for ML accelerators; contention-aware phased allocation strategy 4 4 3"},{"location":"software/ds/#architectural-innovations","title":"Architectural Innovations","text":"Year Venue Authors Title Tags P E N 2025 arXiv Shanghai AI Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts linear sequence modeling with MoE; sparse activation via moe layers; hybrid models combining linear-moe and transformer-moe layers 2025 arXiv Berkeley HeterMoE: Efficient Training of Mixture-of-Experts Models on Heterogeneous GPUs zebra parallelism; attention-expert disaggregation; asymmetric expert assignment mechanism; gather and squeeze strategy 4 5 3"},{"location":"software/ds/#compute-kernel-level-optimizations","title":"Compute-Kernel-Level Optimizations","text":"Year Venue Authors Title Tags P E N 2025 arXiv SJTU Samoyeds: Accelerating MoE Models with Structured Sparsity Leveraging Sparse Tensor Cores dual-side structured sparsity; sparse-sparse matrix multiplication kernel; vector-wise + 2:4 hybrid sparsity; token-aware activation compression"},{"location":"software/ds/#long-sequence-llm-systems","title":"Long Sequence LLM Systems","text":"Year Venue Authors Title Tags P E N 2024 OSDI SJTU &amp; Alibaba Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache inefficient model parallelism intra-instance; inefficient resource management inter-instance; KV cache scheduling 2025 arXiv PKU ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length on More Than 12,000 GPUs hybrid data parallelism; data-aware sharding; a heuristic algorithm that reorganizes data assignment based on the characteristics of data and pipeline parallelism 2025 ICML ByteDance ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference offload value cache to CPU and keep outliers on GPU; landmark-guided sparse KV selection per chunk 3 3 3"},{"location":"software/ds/#sparse-attention","title":"Sparse Attention","text":"<p>Solution: handle the prompt token by token introduce high latency, trying to use sparse attention to reduce the computation and memory burden. This can be achieved by not using the full attention matrix, but only the upper triangular part.</p> Year Venue Authors Title Tags P E N 2025 arXiv CWRU Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques sparse attention with graph computing perspective; work-optimal graph algorithms; achieve true sparsity 2025 MLSys MIT LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention unified sparse attention; hybrid static and dynamic sparsity; hierarchical kv cache management with query-centric pruning"},{"location":"software/ds/#ring-computation","title":"Ring Computation","text":"<p>Solution: use the device layout to reduce the communication overhead. The key idea is to parallel the computation and communication.</p> Year Venue Authors Title Tags P E N 2023 Nips UCB Ring Attention with Blockwise Transformers for Near-Infinite Context divide the input into blocks and each block is processed by a single GPU; ring-type device layout 4 3 3 2024 arXiv SJTU TokenRing: An Efficient Parallelism Framework for Infinite-Context LLMs via Bidirectional Communication communication-oriented parallelism framework; inter-node P2P bidirectional communication bandwidth; optimization of attention block communication"},{"location":"software/ds/#p-d-disaggregated-systems","title":"P-D Disaggregated Systems","text":"<p>Solution: prefill and decode have different computation characteristics, so we can disaggregate them to different GPUs.</p> Year Venue Authors Title Tags P E N 2024 OSDI PKU DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving goodput-optimized; prefill-decoding interference;novel placement algorithm for p-d schema 2024 ISCA UW Splitwise: Efficient Generative LLM Inference Using Phase Splitting optimized cache context transfer; performance per dollar; performance per watt; exploration of homogeneous and heterogeneous cluster deployments 2024 arXiv CMU A System for Microserving of LLMs fine-grained sub-request level actions; dynamic reconfiguration according to workloads; unified KV cache abstraction 2025 arXiv PKU ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud Environments two-level hierarchical optimization; tabu search algorithm for GPU partition; a lightweight re-scheduling mechanism"},{"location":"software/ds/#p-d-disaggregated-system-optimizations","title":"P-D Disaggregated System Optimizations","text":"Year Venue Authors Title Tags P E N 2025 arXiv ByteDance KVDirect: Distributed Disaggregated LLM Inference tensor-centric communication mechanism; pull-based KV cache transfer; dynamic GPU resource scheduling via RDMA 2025 arXiv SYSU Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation attention disaggregation and offloading mechanism; low-latency decoding synchronization; resource-efficient prefill colocation; load-aware offloading scheduling 4 4 3 2025 arXiv Alibaba FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache Transfer and Load-Aware Scheduling analyze the communication patterns; KV cache structure adjustment method; load-aware scheduling 4 4 2 2025 arXiv NUS &amp; USTC DynaServe: Unified and Elastic Tandem-Style Execution for Dynamic Disaggregated LLM Serving a novel Tandem Serving execution model; two virtual subrequests; explicitly permit the two subrequests to execute on either GPU instance 3 4 2"},{"location":"software/ds/#a-f-disaggregated-systems","title":"A-F Disaggregated Systems","text":"<p>Challenge: In the decode stage, the attention need memory access while the FFN only need compute. The increasingly sparse FFN in MoE models make the GPU computation ultilization low. Disaggregate the attention and FFN module to different can make better batching and heterogeneous devices implement.</p> Year Venue Authors Title Tags P E N 2025 arXiv ByteDance MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism the first the disaggregated attention and FFN disaggregate; ping-pong pipeline parallel; high performance M2N communication 4 3 3 2025 arXiv StepFun Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding detailed AFD math analysis; co-design of model and system; low decode latenty implementation 3 4 2 2025 arXiv SJTU Expert-as-a-Service: Towards Efficient Scalable and Robust Large-scale MoE Serving treat expert on independent servers; expert as a service enable scalable MoE serving 3 3 2"},{"location":"software/ds/#throughput-optimized-systems","title":"Throughput-Optimized Systems","text":"Year Venue Authors Title Tags P E N 2025 arXiv HKUST Improving the End-to-End Efficiency of Offline Inference for Multi-LLM Applications Based on Sampling and Simulation sampling-then-simulation cost model; model-level pipeline parallelism; minimumtotal-latency application scheduling 4 4 3 2025 EuroSys SJTU TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via Preemptive Scheduling brust scene target; tradeoff between TTFT and throughout; slowdown some TOT to human's read speed 4 3 2"},{"location":"software/ds/#diffusion-llm-systems","title":"Diffusion LLM Systems","text":"<p>Challenge: traditional LLM use an auto-regressive model to generate the text, which is slow. Diffusion LLM have a parallel generation ability, which is fast.</p> Year Venue Authors Title Tags P E N 2025 arXiv SJTU dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching training-free adaptive caching mechanism; V-verify mechanism for caching 4 3 3 2025 arXiv SJTU Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing First dLLM faster than AR LLM; fuse KV-cached autoregression with parallel diffusion decoding; inter-block parallelism without full denoise 4 3 4 2025 arXiv NVIDIA Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding block-wise approximate KV Cache for diffusion LLMs; confidence-aware parallel decoding strategy 3 3 4 2025 arXiv Cornell Remasking Discrete Diffusion Models with Inference-Time Scaling ReMDM sampler for iterative refinement; inference-time compute scaling via token remasking; custom remasking backward process for pre-trained models 3 4 3 2025 arXiv Univ. of Virginia Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion speculative diffusion Decoding; discrete diffusion models as non-autoregressive drafters; parallelized draft sequence generation 3 3 3"},{"location":"software/ds/#visual-auto-regressive-models-serving-system","title":"Visual Auto-Regressive Models Serving System","text":"<p>Challenge: as the VAR model moves to large scales (e.g. 1024x1024), the number of tokens to be processed explodes, making final steps expensive compared to the initial coarse steps.</p> Year Venue Authors Title Tags P E N 2025 ICCV THU FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning token pruning cache for resolution scaling; frequency-based pivotal token selection; cached token restoration from previous scales 4 3 3 2025 arXiv UESTC SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping frequency-aware adaptive acceleration; step-skipping for step redundancy; unconditional branch replacement for branch redundancy 3 2 3"},{"location":"software/ds/#fair-serving-systems","title":"Fair Serving Systems","text":"Year Venue Authors Title Tags P E N 2024 arXiv Virginia Tech Ensuring Fair LLM Serving Amid Diverse Applications multi-tenant LLM platform; overload and interaction-driven throttling; weighted service counter 2025 arXiv UIUC Hierarchical Autoscaling for Large Language Model Serving with Chiron hierarchical backpressure; interactive requests and batch requests; mixed instances 2025 arXiv Berkeley Locality-aware Fair Scheduling in LLM Serving deficit-based longest prefix matching; distributed deficit-round coordination; prefix-aware fairness bound analysis"},{"location":"software/ds/#rl-system","title":"RL System","text":"<p>Challenge: RL contains both backward and frontward. This need to handle the interaction between multi step and task, posing challenge to parallelism and synchronization.</p> Year Venue Authors Title Tags P E N 2018 OSDI UCB Ray: A Distributed Framework for Emerging AI Applications unified actor and task model; stateless architecture via global control store; bottom-up distributed scheduling 4 4 3"},{"location":"software/ds/#rlhf-system","title":"RLHF System","text":"<p>Challenge: RL system includes both training and inference. On top of that, multi agents(LLMs) when running in parallel, which makes the data flow more complex.</p> Year Venue Authors Title Tags P E N 2025 EuroSys HKU HybridFlow: A Flexible and Efficient RLHF Framework auto-mapping model placement; 3D-HybridEngine to reduce the communication overhead; hybrid programming 4 4 3 2025 arXiv Alibaba Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library bind many LLMs in one device cluster; fix the batch problem of long tail requests; reuse many utils in HybridFlow 4 4 2 2025 arXiv FDU DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training distributed controller for better scalability; fully decoupled DAG-defined RL pipeline; linear scalability 3 3 2"},{"location":"software/ds/#communication-computation-overlap","title":"Communication-Computation Overlap","text":"<p>Challenge: effectively hiding communication latency by overlapping it with computation, which requires careful scheduling and resource management to avoid bottlenecks and ensure that both communication and computation proceed efficiently without stalling each other.</p> Year Venue Authors Title Tags P E N 2023 NSDI KAIST ARK: GPU-driven Code Execution for Distributed Deep Learning communication-motivated DL system; pipeline DMA engine; GPU-direct-controlled DMA 2024 ASPLOS PKU Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning communication partition abstraction; hybrid LLM training tasks; 3-level decompose 2024 ASPLOS UW\u2013Madison T3: Transparent Tracking &amp; Triggering for Fine-grained Overlap of Compute &amp; Collectives lightweight track and trigger; pre-programmed DMA commands; atomic memory update 2024 ASPLOS UIUC Two-Face: Combining Collective and One-Sided Communication for Efficient Distributed SpMM distributed SpMM; sparsity-aware partition; Synchronous Stripes and Asynchronous Stripes 2024 arXiv AMD Optimizing ML Concurrent Computation and Communication with GPU DMA Engines concurrent computation and communication; compute and memory interference among concurrent kernels; schedule prioritization and careful resource partitioning"},{"location":"software/ds/#configuration-optimization","title":"Configuration Optimization","text":"<p>Challenge: the configuration space is too large to be searched manually.</p> Year Venue Authors Title Tags P E N 2025 OSDI PKU Mirage: A Multi-Level Superoptimizer for Tensor Programs auto algebraically transfer tensor; using DAG to search configuration space; auto generate kernel function 4 4 3 2020 ASPLOS PKU FlexTensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous System tvm auto schedule; RL based stragety find; auto optimizing in large configuration space 4 4 3"},{"location":"software/ds/#diffusion-model-serving-systems","title":"Diffusion Model Serving Systems","text":"<p>Challenge: serving diffusion models need sequential iterative process, which creates prohibitive latency for generating high-resolution images in real-time and is inherently difficult to parallelize.</p> Year Venue Authors Title Tags P E N 2024 CVPR MIT DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models displaced patch parallelism; asynchronous GroupNorm for distributed normalization 3 4 2 2024 arXiv Tencent PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference patch-level pipeline parallelism; reusing one-step stale feature maps to exploit temporal redundancy; simultaneous image patch and model layer partitioning 4 4 2 2025 arXiv UMASS HADIS: Hybrid Adaptive Diffusion Model Serving for Efficient Text-to-Image Generation offline analysis based router choose; LIP based resource allocation 2 3 2 2024 arXiv Tencent xDiT: an Inference Engine for Diffusion Transformers (DiTs) with Massive Parallelism hybrid of sequence parallelism &amp; PipeFusion &amp; classifier-free guidance parallelism; patch parallelism for the VAE decoder to prevent out-of-memory errors 3 3 2"},{"location":"software/hpc/","title":"High-Performance Computing","text":""},{"location":"software/hpc/#parallel-programming-models","title":"Parallel programming models","text":"<p>Challenge: the complexity of great number of cores and the heterogeneity of the hardware</p>"},{"location":"software/hpc/#scientific-computing-applications-and-libraries","title":"Scientific computing applications and libraries","text":"<p>Solution: enable researchers to model physical phenomena, process large datasets, and accelerate discoveries across fields on HPC systems</p>"},{"location":"software/hpc/#parallel-algorithms","title":"Parallel algorithms","text":"<p>Solution: develop parallel algorithms for HPC systems, enabling scalable performance</p>"},{"location":"software/os/","title":"Operating Systems","text":""},{"location":"software/os/#kernel-design-and-implementation","title":"Kernel design and implementation","text":"<p>Solution: abstracting the underlying hardware and providing a secure, managed, and efficient environment for user-level applications to run concurrently and share system resources</p>"},{"location":"software/os/#process-and-thread-management","title":"Process and thread management","text":"<p>Solution: ensuring fair resource allocation, smooth multitasking, and proper coordination between processes and threads to maximize system performance and responsiveness.</p>"},{"location":"software/os/#gpu-checkpoint-and-restore","title":"GPU Checkpoint and Restore","text":"<p>Challenge: Saving and restoring the complete state for GPUs is difficult without causing long application stalls</p> Year Venue Authors Title Tags P E N 2025 SOSP SJTU PHOENIXOS: Concurrent OS-level GPU Checkpoint and Restore with Validated Speculation validated speculation; concurrent GPU checkpoint and restore; soft copy-on-write; GPU context pool 4 4 4"},{"location":"software/os/#memory-management","title":"Memory management","text":"<p>Memory management is the basic of system design.</p>"},{"location":"software/os/#memory-partitioning-mapping","title":"Memory Partitioning &amp; Mapping","text":"<p>Solution: efficiently and securely divide physical memory among multiple processes or systems</p> <p>Challenge: ensuring processes get enough memory without waste, while preventing interference and maintaining system performance and security.</p> Year Venue Authors Title Tags P E N 2012 HPCA Georgia Institute of Technology TAP: A TLP-Aware Cache Management Policy for a CPU-GPU Heterogeneous Architecture thread-level parallelism; core sampling for cache effort indentification; cache block lifetime normalization 2024 arXiv UMich Mercury: QoS-Aware Tiered Memory System contend for local memory; priority inversion; intra- and inter-tier interference; per-tier page reclaimation 2025 HPCA Seoul National FACIL: Flexible DRAM Address Mapping for SoC-PIM Cooperative On-device LLM Inference flexible PIM address mapping and remapping; OS paging mechanism extension for MapID; user-level mapping selector 3 2 3"},{"location":"software/os/#page-migration","title":"Page Migration","text":"<p>Challenge: the trade-off between migration overhead and potential performance gains, complicated by prediction accuracy and hardware complexities</p> Year Venue Authors Title Tags P E N 2024 SC THU Hydrogen: Contention-Aware Hybrid Memory for Heterogeneous CPU-GPU Architectures fast memory decoupled partitioning; token-based slow memory migration; epoch-based sampling method; consistent hashing based reconfiguration 2024 OSDI UT Arlington Nomad: Non-Exclusive Memory Tiering via Transactional Page Migration Memory allocator for hardware tiering to mitigate outliers 2025 ISCA XMU ArtMem: Adaptive Migration in Reinforcement Learning-Enabled Tiered Memory reinforcement learning for tiered memory management; dynamic adjustment of migration scope 2 4 3"},{"location":"software/os/#memory-pooling","title":"Memory Pooling","text":"<p>Solution: Memory Pooling reduces the overhead of frequent memory allocation and deallocation by maintaining a pool of pre-allocated memory blocks for reuse.</p> Year Venue Authors Title Tags P E N 2023 ASPLOS Virginia Tech Pond: CXL-Based Memory Pooling Systems for Cloud Platforms CXL-based memory pooling; small-pool design for low latency; machine learning model for memory allocation prediction; zero-core virtual NUMA (zNUMA) node for untouched memory"},{"location":"software/os/#cache-evict","title":"Cache Evict","text":"<p>Challenge: to balance the cache hit ratio and the cache miss penalty.</p> <p>The cache here not only includes the cache in the processors, but also the software concept in systems.</p> Year Venue Authors Title Tags P E N 2024 NSDI CMU Sieve is Simpler than LRU: an Efficient Turn-Key Eviction Algorithm for Web Caches web cache management; fifo-like schedule; recognizing the importance of objects 3 3 3"},{"location":"software/os/#file-systems-and-storage","title":"File systems and storage","text":"<p>Solution: organizing, storing, and retrieving data efficiently and reliably on storage devices, manage disk space, and ensure data integrity and security.</p>"},{"location":"software/os/#storage-systems","title":"Storage Systems","text":"<p>Solution: eficient, reliable, and secure management of data storage across diverse hardware while achieving balance between performance, durability, efficiency, and scalability .</p>"},{"location":"software/os/#out-of-core-processing","title":"Out-of-Core Processing","text":"<p>Challenge: performance penalty of slow disk I/O</p> Year Venue Authors Title Tags P E N 2018 OSDI UCLA RStream: Marrying Relational Algebra with Streaming for Efficient Graph Mining on A Single Machine single-machine out-of-core graph mining; tuple streaming; relational algebra on graphs; GRAS programming model 4 4 4"},{"location":"software/os/#ssd-management","title":"SSD Management","text":"<p>Challenge: limited write endurance, garbage collection, wear leveling, and the mismatch between erase and write operations, while optimizing performance and extending device lifespan</p> Year Venue Authors Title Tags P E N 2025 EuroSys Samsung Towards Efficient Flash Caches with Emerging NVMe Flexible Data Placement SSDs NVMe Flexible Data Placement (FDP) SSDs for data segregation; targeted data placement for reduced device write amplification; FDP-enabled CacheLib architecture; theoretical DLWA model for CacheLib 2025 arXiv SDU Managing Hybrid Solid-State Drives Using Large Language Models LLM-based auto-tuning framework for hybrid SSD management; hybrid SSD parameter categorization; performance-sensitive parameter selection; prompt engineering for LLM integration; dynamic configuration optimization in hybrid SSDs 2025 arXiv ETHZ Harmonia: A Multi-Agent Reinforcement Learning Approach to Data Placement and Migration in Hybrid Storage Systems dual RL agents for data placement &amp; migration; I/O latency-based reward shaping 3 4 4"},{"location":"software/os/#virtualization","title":"Virtualization","text":"<p>Solution: efficiently managing and isolating multiple virtual machines (VMs) or containers on a single physical machine, allowing them to share hardware resources securely and independently while maintaining performance and flexibility.</p>"},{"location":"software/os/#container-management","title":"Container Management","text":"<p>Challenge: container is a abstract of specific OS functions, managing containers need to solve the problem of blocking useless information and only keep the necessary information.</p> Year Venue Authors Title Tags P E N 2024 SOSP THU TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes share function excution between containers; byte-addressable CXL memory attach with CoW; Browser-multiplexing for LLM agents inside VMs 3 4 4"},{"location":"software/os/#synchronization","title":"Synchronization","text":"<p>Solution: coordinating access to shared resources among multiple processes or threads to prevent conflicts, ensure data consistency, and avoid issues like race conditions, deadlocks, and resource starvation.</p> Year Venue Authors Title Tags P E N 2019 ASPLOS THU&amp;USC pLock: A Fast Lock for Architectures with Explicit Inter-core Message Passing chaining lock for direct lock transfering; hierarchical lock for non-uniform network 4 4 2"},{"location":"software/perf/","title":"Performance Analysis","text":""},{"location":"software/perf/#profilers-sampling-instrumentation","title":"Profilers (sampling, instrumentation)","text":""},{"location":"software/perf/#detection","title":"Detection","text":""},{"location":"software/perf/#bottleneck-analysis","title":"Bottleneck Analysis","text":"<p>Challenge: Bottleneck Analysis faces challenges of high system complexity, unexpected real-world factors and the resource constraints when detecting.</p> Year Venue Authors Title Tags P E N 2018 PPoPP THU vSensor: Leveraging Fixed-Workload Snippets of Programs for Performance Variance Detection fixed-workload snippets; dependency propagation algorithm; lightweight on-line analysis algorithm 2020 SC THU ScalAna: automating scaling loss detection with graph analysis program structure graph; program performance graph; backtracking root cause detection algorithm 2022 PPoPP THU Vapro: Performance Variance Detection and Diagnosis for Production-Run Parallel Applications state transition graph; fixed workload snippets identification clustering algorithm; variance breakdown model; time of factors quantification method 2024 arXiv UGA Performance Debugging through Microarchitectural Sensitivity and Causality Analysis constraints propagation engine for causality analysis; differential analysis engine for sensitivity analysis 2024 SC BUAA GVARP: Detecting Performance Variance on Large-Scale Heterogeneous Systems asynchronous state transition graph; parameter-based workload estimation method; asynchronous event tracing technology"},{"location":"software/perf/#variance-attribution","title":"Variance Attribution","text":"Year Venue Authors Title Tags P E N 2014 ISPASS Intel A Top-Down Method for Performance Analysis and Counters Architecture top-down bottleneck analysis method; frontend bound; bad speculation; retiring; backend bound 2016 TPDS ICT Understanding Big Data Analytics Workloads on Modern Processors top-down analysis for big data workload; pipeline-characteristics basd performance implication analysis; BigDataBench benchmark 2019 SC NCSU Pinpointing Performance Inefficiencies via Lightweight Variance Profiling function-level variance detection; stack based deep call chains maintain; on-the-fly binary analysis technique for calling context"},{"location":"software/perf/#root-cause-analysis","title":"Root Cause Analysis","text":"<p>Challenge: difficulties in dependency graph modeling, scalability of detection algorithm for large-scale applications.</p>"},{"location":"software/perf/#heuristic-approaches","title":"Heuristic Approaches","text":"<p>Solution: trace system failures rely on dependency graphs, expert rules, or statistical correlations.</p> Year Venue Authors Title Tags P E N 2003 TISSEC IBM Clustering Intrusion Detection Alarms to Support Root Cause Analysis attribute-oriented induction based clustering algorithm; generalized alarm analysis 2 3 2 2017 Arxiv Intel; CA technologies Survey on Models and Techniques for Root-Cause Analysis deterministic/probabilistic model; RCA learning algorithms; RCA inference algorithms 4 1 1 2021 ASE eBay Groot: An Event-graph-based Approach for Root Cause Analysis in Industrial Settings event-graph based RCA; service dependency graph; event causality graph; pagerank based root cause ranking 4 5 2"},{"location":"software/perf/#machine-learning","title":"Machine Learning","text":"<p>Solution: employ machine learning models, like graph neural networks, to automatically learn complex causal patterns.</p> Year Venue Authors Title Tags P E N 2021 ASPLOS Cornell Sage: Practical &amp; Scalable ML-Driven Performance Debugging in Microservices RPC latency decomposition model; Markov based RPC latency propagation; causal bayesian network based dependency model 3 3 2 2023 ASPLOS Alibaba Sleuth: A Trace-Based Root Cause Analysis System for Large-Scale Microservices with Graph Neural Networks HDBSCAN trace clustering algorithm; GNN based dependency modeling 3 3 2 2023 SIGKDD UCF Interdependent Causal Networks for Root Cause Localization GNN based topological causal discovery; extreme value theory based individual causal discovery; causal integration 4 3 2 2023 SIGKDD UCF Incremental Causal Graph Learning for Online Root Cause Analysis trigger point detection; incremental desentangled causal graph learning; random walk with restart based root cause localization 3 3 2"},{"location":"software/perf/#burst-detection","title":"Burst Detection","text":"<p>Challenge: maintaining accuracy at high speed data streams, tradeoff between memory usage and detection accuracy.</p>"},{"location":"software/perf/#heavy-hitter-burst","title":"Heavy Hitter Burst","text":"Year Venue Authors Title Tags P E N 2019 CloudNet PKU Dynamic Sketch: Efficient and Adjustable Heavy Hitter Detection for Software Packet Processing door keeper mechanism for high memory efficiency; bucket sampling for accuracy monitoring 3 3 1 2021 SIGMOD PKU BurstSketch: Finding Bursts in Data Streams running track based burst item filtering; snapshotting based burst item detection 3 3 1 2023 SIGMOD PKU Double-Anonymous Sketch: Achieving Top-\ud835\udc3e-fairness for Finding Global Top-\ud835\udc3e Frequent Items double-anonymity technique; randomized admission policy for top-k stage; CMM sketch for count stage 3 4 2 2024 IFIP NPC PKU 2FA Sketch: Two-Factor Armor Sketch for Accurate and Efficient Heavy Hitter Detection in Data Streams improved arbitration strategy for in-bucket competition; cross-bucket conflict avoidance hashing scheme 2 3 1 2024 IEEE ICDE PKU Scalable Overspeed Item Detection in Streams bucket sharing based basic speedsketch algorithm; global-clock for reducing timestamp overhead; counter-flip technique for compression 3 4 2"},{"location":"software/perf/#straggler-analysis","title":"Straggler Analysis","text":"<p>Challenge: stragglers can arise from various complex factors, identifying their root causes and quantifying their impact on performance is difficult.</p> Year Venue Authors Title Tags P E N 2019 TSC BUAA Straggler Root-Cause and Impact Analysis for Massive-scale Virtualized Cloud Datacenters detailing straggler filtration based root cause analysis; DoS-indexf for straggler detection 3 3 1 2020 TJSC QMUL&amp;NUDT Tails in\u00a0the\u00a0cloud: a\u00a0survey and\u00a0taxonomy of\u00a0straggler management within\u00a0large\u2011scale cloud data centres taxonomy of\u00a0straggler causes; straggler management technique 3 1 1 2024 Arxiv HKUST&amp;Alibaba FALCON: Pinpointing and Mitigating Stragglers for Large-Scale Hybrid-Parallel Training Bayesian online change-point detection algorithm; adaptive multi-level mitigation mechanism 4 4 2 2025 Arxiv NYU&amp;ByteDance Understanding Stragglers in Large Model Training Using What-if Analysis what-if analysis; dependency model based simulation; SMon monitoring system 3 4 2"},{"location":"software/perf/#other-bursts","title":"Other Bursts","text":"Year Venue Authors Title Tags P E N 2023 CIKM Edinburgh Tight-Sketch: A High-Performance Sketch for Heavy Item-Oriented Data Stream Mining with Limited Memory Size probabilistic decay strategy; differentiated eviction for cold and hot items 4 4 2 2024 INFOCOM SCU BurstDetector: Real-Time and Accurate Across-Period Burst Detection in High-Speed Networks two-stage across-period burst detection; hierarchical cell for memory optimization 3 4 1"},{"location":"software/perf/#network-tomography","title":"Network Tomography","text":""},{"location":"software/perf/#survey","title":"Survey","text":"Year Venue Authors Title Tags P E N 2004 STAT SCI Berkeley Network Tomography: Recent Developments tomography linear model; multicast delay distribution inference; origin\u2013destination traffic matrix inference 3 1 1"},{"location":"software/perf/#passive-inference","title":"Passive Inference","text":"Year Venue Authors Title Tags P E N 2003 IMC AT&amp;T Laboratories Simple Network Performance Tomography smallest consistent failure set algorithm; seperable performance; false positive/coverage probability estimation of bad links 3 3 3 2014 ICDCS ZJU Domo: Passive Per-Packet Delay Tomography in Wireless Ad-hoc Networks FIFO/order/sum-of-delays constraints for delay reconstruction; semi-definite relaxation based optimization 4 3 2"},{"location":"software/perf/#active-inference","title":"Active Inference","text":"Year Venue Authors Title Tags P E N 2022 ICASSP UMich Unicast-based inference of network link delay distributions using mixed finite mixture models dirac delta based mixed finite mixture model; EM algorithm for parameter evaluation 3 2 2 2003 IEEE TSP Rice University Network Delay Tomography end-to-end packet pair link delay distribution estimation; FFT based expectation-maximization acceleration algorithm 3 3 2 2021 IEEE TNSM QMUL Optimal Estimation of Link Delays Based on End-to-End Active Measurements active network monitoring framework; ILP/heuristic/meta-heuristic algorithm for monitoring flows selection 3 3 2"},{"location":"software/perf/#profiling-techniques","title":"Profiling Techniques","text":""},{"location":"software/perf/#extended-berkeley-packet-filter","title":"Extended Berkeley Packet Filter","text":"<p>Solution: A technique used for dynamically programing the kernel for efficient networking, observability, tracing, and security.</p>"},{"location":"software/perf/#ebpf-component-analysis","title":"eBPF Component Analysis","text":"Year Venue Authors Title Tags P E N 2024 eBPF THU Understanding Performance of eBPF Maps eBPF map benchmark; impact of cache hotness on eBPF map; volume discount feature of eBPF program 4 4 2 2024 OSDI ETH Zurich Validating the eBPF Verifier via State Embedding state embedding mechanism for eBPF verifier bug detection; SEV pratical realization 4 4 3 2025 EuroSys UW\u2013Madison Revealing the Unstable Foundations of eBPF-Based Kernel Extensions potential mismatches dataset; dependency surface/set analysis 4 4 2 2025 OSDI UCSD KPerfIR: Towards an Open and Compiler-centric Ecosystem for GPU Kernel Performance Tooling on Modern AI Workloads compiler-centric profiling infrastructure; multi-level IR instrumentation; region-based timing tool; trace replay for overhead correction 4 4 3"},{"location":"software/perf/#ebpf-like-applications","title":"eBPF Like Applications","text":"Year Venue Authors Title Tags P E N 2025 HCDS UCSC eGPU: Extending eBPF Programmability and Observability to GPUs dynamic PTX injection; real-time synchronization to avoid race conditions 3 2 2"},{"location":"software/perf/#distributed-tracing","title":"Distributed Tracing","text":"<p>Solution: A technique used for monitoring and diagnosing errors in microserves systems by recording full request paths.</p> Year Venue Authors Title Tags P E N 2015 SOSP Brown Pivot Tracing: Dynamic Causal Monitoring for Distributed Systems happened-beforejoin for arbitrary event correlation; dynamic instrumentation; metadata propagation technique baggage 4 3 2 2017 SOSP Facebook&amp;Brown Canopy: An End-to-End Performance Tracing And Analysis System tracing decouple for separate modeling and analyzing; trace feature extraction pipeline 4 4 2"},{"location":"software/perf/#tracing-optimization","title":"Tracing Optimization","text":"<p>Challenge: Tradeoff between tracing storage overhead and the effectiveness of preserved data.</p> Year Venue Authors Title Tags P E N 2015 SOSP Brown Pivot Tracing: Dynamic Causal Monitoring for Distributed Systems happened-beforejoin for arbitrary event correlation; dynamic instrumentation; metadata propagation technique baggage 4 3 2 2021 ICWS SYSU Sieve: Attention-based Sampling of End-to-End Trace Data in Distributed Microservice Systems path vector encoding; attention score based biased sampler 3 3 2 2023 NSDI Emory\u200c&amp;Princeton The Benefit of Hindsight: Tracing Edge-Cases in Distributed Systems retroactive trace sampling; trace coherence mechanism (breadcrumb); lateral tracing across requests 4 4 2 2025 ASPLOS SYSU&amp;Alibaba Mint: Cost-Efficient Tracing with All Requests Collection via Commonality and Variability Analysis commonality and variability based tracing strategy; pattern exraction at the span level and trace level 4 4 2"},{"location":"software/perf/#diagnosis-and-analysis","title":"Diagnosis and Analysis","text":"<p>Challenge: How to use the trace data to accurately locate failures or bottlenecks, especially in large\u2011scale systems.</p> Year Venue Authors Title Tags P E N 2017 SOSP Facebook&amp;Brown Canopy: An End-to-End Performance Tracing And Analysis System tracing decouple for separate modeling and analyzing; trace feature extraction pipeline 4 4 2 2023 NSDI BUPT&amp;ByteDance Hostping: Diagnosing Intra-host Network Bottlenecks in RDMA Servers loopback tests between RNICs and endpoints; bus utilization monitoring; binary network tomography inspired path analysis 3 4 2 2025 Arxiv Alibaba PerfTracker: Online Performance Troubleshooting for Large-scale Model Training in Production runtime behavior pattern representation; critical path identification; distance from expectation&amp;differential distance based localization 4 4 2"},{"location":"software/perf/#profiling-with-llm","title":"Profiling with LLM","text":"<p>Solution: using LLMs to generate profile results.</p> Year Venue Authors Title Tags P E N 2025 SOSP UChicago METIS: Fast Quality-Aware RAG Systems with Configuration Adaptation LLM-based query profiler; rule-based configuration pruning; resource-aware joint scheduling; per-query configuration adaptation 3 4 2"},{"location":"software/perf/#simulators-and-emulators-for-softwaresystem-analysis","title":"Simulators and emulators (for software/system analysis)","text":"<p>Challenge: how to balance the accuracy, time cost and complexity of a simulator.</p>"},{"location":"software/perf/#general-performance-modeling","title":"General Performance Modeling","text":"<p>Focusing on the performance modeling for general systems. The LLM performance modeling is in the LLM Performance Modeling section.</p> Year Venue Authors Title Tags P E N 2009 CACM Berkeley Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures operational intensity; memory bound; compute bound 2014 IISWC ETH Zurich Extending the Roofline Model: Bottleneck Analysis with Microarchitectural Constraints dag-based performance model; Tomasulo's greedy algorithm; scheduled dag based bottleneck modeling 3 4 3 2021 Intelligent Computing Berkeley Hierarchical Roofline Performance Analysis for Deep Learning Applications Nsight Compute based hierarchical roofline model; FP16\u3001FP32 extension for ERT 2025 arXiv Google Concorde: Fast and Accurate CPU Performance Modeling with Compositional Analytical-ML Fusion per-resource throughput analysis; fine-grained performance attribution 3 2 2 2025 ASPLOS Georgia Tech Forecasting GPU Performance for Deep Learning Training and Inference tile-level kernal decomposition; fundamental performance laws bounded prediction; ML based utilization prediction 3 4 2"},{"location":"software/perf/#memory-subsystem-simulators","title":"Memory Subsystem Simulators","text":"<p>Challenge: Memory subsystem simulators face accuracy issues with realistic workloads and lack efficient parallelization strategies.</p> Year Venue Authors Title Tags P E N 2024 MICRO Barcelona Supercomputing Center A Mess of Memory System Benchmarking, Simulation and Application Profiling inaccuracy of simulators like Ramulator and DRAMsim3; analytical memory simulator \"Mess\" based on bandwidth-latency curves 4 2 2 2025 arXiv ETHZ Cleaning up the Mess refuting Mess paper's claims via identifying simulator configuration errors; correcting Mess paper's evaluation via identifying erroneous statistics usage; Mess Request Generator for a corrected Ramulator 2 evaluation 3 2 3 2025 arXiv Cornell gem5 Co-Pilot: AI Assistant Agent for Architectural Design Space Exploration LLM-powered agent for computer architecture DSE; state machine-driven exploration loop (ANA-GEN-QA-EXIT); prompt engineering for results retrospection and baseline preservation 3 2 3"},{"location":"software/perf/#llm-performance-modeling","title":"LLM Performance Modeling","text":"<p>Solution: LLM inference is expensive, performance modeling can help decide on the best configuration for the given system without actually running the LLM.</p>"},{"location":"software/perf/#llm-serving-performance-modeling","title":"LLM Serving Performance Modeling","text":"Year Venue Authors Title Tags P E N 2024 arXiv KAIST LLMServingSim: A HW/SW Co-Simulation Infrastructure for LLM Inference Serving at Scale iteration-level simulation; computation reuse optimization; heterogeneous accelerator mapping 2024 Mlsys GIT Vidur: A Large-Scale Simulation Framework For LLM Inference Operation-level simulation; Using the simulator to search the best configuration for the given system 3 3 3"},{"location":"software/perf/#llm-training-performance-modeling","title":"LLM Training Performance Modeling","text":"Year Venue Authors Title Tags P E N 2024 MICRO KAIST vTrain: A Simulation Framework for Evaluating Cost-effective and Compute-optimal Large Language Model Training profiling-driven LLM training simulation; operator-to-task lookup table; cost-effective training plan exploration; compute-optimal chinchilla point estimation 3 4 2 2025 MLSys Cornell Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training trace-driven performance modeling and estimation toolkit; the first system to provide accurate performance models that effectively capture the execution behaviors of LLMs; modify and generate new execution graphs from existing traces 3 4 2"},{"location":"software/perf/#thermal-modeling","title":"Thermal Modeling","text":"<p>Challenge: precisely capturing the nonlinear, multi-scale and heterogeneous thermal behaviors on different architecture</p> Year Venue Authors Title Tags P E N 2003 ISCA Virginia Temperature-Aware Microarchitecture architecture level thermal modeling; temperature-tracking frequency scaling; local feedback-controlled fetch toggling 3 3 2 2024 ICCAD PKU FaStTherm: Fast and Stable Full-Chip Transient Thermal Predictor Considering Nonlinear Effects ResNet-18-based deep autoencoder; unrolled training and noise injection for stability; GA-based training data generation 4 3 2 2024 ISEDA PKU ATSim3D: Towards Accurate Thermal Simulator for Heterogeneous 3D-IC Systems Considering Nonlinear Leakage and Conductivity global-local approach for simulation with ultrahigh resolution; Kirchhoff transformation for nonlinear conductivity 3 3 2"},{"location":"software/perf/#benchmarking-methodologies-and-suites","title":"Benchmarking methodologies and suites","text":""},{"location":"software/perf/#systematic-optimization-methodologies","title":"Systematic Optimization Methodologies","text":"<p>Soluntion: general systematic optimization methods through benchmarking</p>"},{"location":"software/perf/#benchmark","title":"Benchmark","text":"<p>Solution: benchmark targeted at performance analysis and characterization.</p> Year Venue Authors Title Tags P E N 2018 ICPP WUSTL Varbench: an Experimental Framework to Measure and Characterize Performance Variability spatial/temperal variability; Resource Variability (RV) statistic 2021 IEEE Access D-ITET DAMOV: A New Methodology and Benchmark Suite for Evaluating Data Movement Bottlenecks NDP focused workload characterization methodology; memory-bound function identification; locality-based clustering; memory bottlenecks classification"},{"location":"software/perf/#llm-serving-benchmarks","title":"LLM Serving Benchmarks","text":"<p>Challenge: There is different optimize targets for different LLM serving systems. Develop a fair benchmark is crucial.</p> Year Venue Authors Title Tags P E N 2025 arXiv Intel On Evaluating Performance of LLM Inference Serving Systems introduces a practical checklist to avoid misleading benchmarks 3 3 2"},{"location":"software/pl/","title":"Programming Languages and Software Engineering","text":""},{"location":"software/pl/#language-design-and-semantics","title":"Language design and semantics","text":"<p>Solution: user-friendly, resource-efficient, and secure programming languages</p>"},{"location":"software/pl/#compiler-construction-and-optimization","title":"Compiler construction and optimization","text":"<p>Solution: improving performance, reducing resource usage, and ensuring correctness</p> Year Venue Authors Title Tags P E N 2024 MICRO Georgia Tech Unleashing CPU Potential for Executing GPU Programs through Compiler/Runtime Optimizations anti-coalescing transformation; block size invariant analysis; tail block adaptive synchronization; GPU-block dynamic tiling 2 4 3"},{"location":"software/pl/#deep-learning-compilers","title":"Deep Learning Compilers","text":"<p>Solution: Graph transformations, Kernel fusion, Tensor optimization for compute and memory</p> Year Venue Authors Title Tags P E N 2013 PLDI MIT Halide: A Language and Compiler for Optimizing Parallelism Locality and Recomputation in Image Processing Pipelines image process DSL; Compute and Schedule Separation IR Design Idea; Pipeline optimization 4 4 3 2018 OSDI UW TVM: An Automated End-to-End Optimizing Compiler for Deep Learning operator fusion; graph-level DL compiler; automatic code generation; tensor expression simplification 4 4 4 2025 PPoPP Thu FlashTensor: Optimizing Tensor Programs by Leveraging Fine-grained Tensor Property dataflow centered code recognition and optimization; two-stage heuristic algorithm to optimize tensor computation; kernel fusion 4 4 2 2025 arXiv PKU TileLang: A Composable Tiled Programming Model for AI Systems tile-based programming; tvm-based compiler; compared to trition is more flexible 4 4 3 2025 OSDI PKU PipeThreader: Software-Defined Pipelining for Efficient DNN Execution Pipeline programming abstraction and orchestration mechanism for heterogeneous computing units; tile size and pipeline stage number tradeoff 4 4 4"},{"location":"software/pl/#compiler-for-accelerators","title":"Compiler for Accelerators","text":"<p>Solution: High-level programming model, automatic code generation, performance optimization for specialized hardware</p> Year Venue Authors Title Tags P E N 2022 PLDI MIT Exocompilation for Productive Programming of Hardware Accelerators Exocompilation; externalized accelerator specification; user-defined instructions; rewrite-based scheduling; effect analysis for safety 4 4 3"},{"location":"software/pl/#sparse-tensor-compilers","title":"Sparse Tensor Compilers","text":"<p>Challenge: Compared to dense tensors, sparse tensors have more complex data structure and computation patterns.</p> Year Venue Authors Title Tags P E N 2023 ASPLOS UW SparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning use composable formats for the expression for sparse matrix; divide the compute to different stages and reuse current optimizations 4 3 3"},{"location":"software/pl/#graph-mining-compilers","title":"Graph Mining Compilers","text":"<p>Challenge: generic runtime algorithms, automatically compile high-level specifications into efficient code.</p> Year Venue Authors Title Tags P E N 2019 SOSP CSM AutoMine: Harmonizing High-Level Abstraction and High Performance for Graph Mining automatic algorithm generation; set-based embedding representation; schedule generation via graph tournament 4 4 4"},{"location":"software/pl/#domain-specific-languages","title":"Domain-Specific Languages","text":"<p>Solution: formal semantics definition, tool generation automation, cross-domain generalization</p>"},{"location":"software/pl/#sparse-tensor-algebra-compilers","title":"Sparse Tensor Algebra Compilers","text":"<p>Solution: multi-format iteration efficiency, format combination optimization, architecture-agnostic code generation</p> Year Venue Authors Title Tags P E N 2018 OOPSLA MIT Format Abstraction for Sparse Tensor Algebra Compilers coordinate hierarchies; level formats abstraction; property-based merge lattice optimizations; level iterator conversion 4 4 3 2020 PLDI MIT Automatic Generation of Efficient Sparse Tensor Format Conversion Routines coordinate remapping notation; attribute query language; tensor assembly abstract interface; three-phase conversion decomposition 4 4 4 2022 OOPSLA MIT Compilation of Dynamic Sparse Tensor Algebra node schema language; assembly abstract interface; map function generation; iterator optimization; dynamic tensor format composition 4 4 3"},{"location":"software/pl/#transpilers","title":"Transpilers","text":"<p>Solution: automatic, correct, and performant source-to-source code translation across different hardware ecosystems</p> Year Venue Authors Title Tags P E N 2025 OSDI CAS QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach neural-symbolic synthesis; LLM-assisted transcompilation; SMT-based code repair; hierarchical auto-tuning 3 4 2"},{"location":"software/pl/#hardware-description-languages","title":"Hardware Description Languages","text":"<p>Solution: expressive hardware specification, efficient simulation and synthesis, robust verification methodologies</p>"},{"location":"software/pl/#hdl-language-design","title":"HDL Language Design","text":"<p>Challenge: balancing expressiveness, usability, and synthesis efficiency in HDL design</p> Year Venue Authors Title Tags P E N 2024 FPGA PKU Cement: Streamlining FPGA Hardware Design with Cycle-Deterministic eHDL and Synthesis incorporates an event layer and the ctrl sub-language; event-based extension; cycle-level timing analysis and control synthesis techniques 4 4 4"},{"location":"software/pl/#streaming-computation-models","title":"Streaming Computation Models","text":"<p>Solution: high-throughput data processing, real-time analytics, efficient resource utilization for continuous data</p> Year Venue Authors Title Tags P E N 2020 ASPLOS Stanford Fleet: A Framework for Massively Parallel Streaming on FPGAs user write serial code for parallel; multi-stream parallelism; ready-valid signaling 3 4 3"},{"location":"software/pl/#hls-code-generation-and-automation","title":"HLS Code Generation and Automation","text":"<p>Solution: bridging high-level languages to hardware, design space exploration, QoR improvement automation</p>"},{"location":"software/pl/#general-hls-compiler","title":"General HLS Compiler","text":"<p>C/C++/SystemC to RTL, microarchitecture optimization, resource sharing and scheduling</p> Year Venue Authors Title Tags P E N 2022 ASPLOS UCLA HeteroGen: transpiling C to heterogeneous HLS code with automated test generation and program repair automated test generation; dependence-guided search space pruning; early candidate rejection using coding styles 3 4 3 2022 FPGA Cornell HeteroFlow: An Accelerator Programming Model with Decoupled Data Placement for Software-Defined FPGAs Decoupled data placement; Unified data placement primitive; Multi-level memory hierarchy optimization 4 4 4 2024 DATE UIUC Subgraph Extraction-Based Feedback-Guided Iterative Scheduling for HLS ISDC iterative SDC scheduling; subgraph extraction-based low-level feedback; fanout and window-based subgraph extraction mechanism 4 4 4 2024 MICRO HUST A Scalable Efficient and Robust Dynamic Memory Management Library for HLS-based FPGAs DMM as graph analytics; request-guided graph traversal; data-centric concurrent traversal; shortcut-assisted fast traversal 4 4 3 2025 FPGA University of Glasgow Dynamic Loop Fusion in High-Level Synthesis Dynamic loop fusion; HLS; Irregular memory access; Address monotonicity; Decoupled Access/Execute (DAE); Program-order schedule; Data Unit (DU) 4 4 4"},{"location":"software/pl/#mlir-based-hls-frameworks","title":"MLIR-based HLS Frameworks","text":"<p>Solution: Leveraging modern compiler infrastructures like MLIR to build modular, extensible, and reusable HLS frameworks.</p> Year Venue Authors Title Tags P E N 2022 ICCAD PKU HECTOR: A Multi-level Intermediate Representation for Hardware Synthesis Methodologies high-level topological representation; low-level hierarchical elastic componenent; time graph transformation 4 4 3 2022 HPCA UIUC ScaleHLS: A New Scalable High-Level Synthesis Framework on Multi-Level Intermediate Representation multi-level IR for HLS; HLS-dedicated analysis/transform library; MLIR-based HLS framework 4 4 3 2023 ASPLOS IISc HIR: An MLIR-based Intermediate Representation for Hardware Accelerator Description MLIR-based hardware IR; datapath + schedule model; explicit scheduling via time variables; automatic FSM synthesis 4 4 4 2024 HPCA SJTU An Optimizing Framework on MLIR for Efficient FPGA-based Accelerator Generation polyhedral model for HLS; MLIR-based HLS framework; automatic design space exploration 4 4 4"},{"location":"software/pl/#hls-verification-and-testing","title":"HLS Verification and Testing","text":"<p>Solution: automated bug detection, verification dataset generation, LLM-aided debugging for HLS designs</p> Year Venue Authors Title Tags P E N 2024 MICRO PKU Hestia: An Efficient Cross-level Debugger for High-level Synthesis allowing inspection at multiple granularities; establishes the correspondence at different levels; a multi-level interpreter for three levels 4 4 4 2024 LAD UIUC An Iteratively-refined Dataset for High-Level Synthesis Functional Verification through LLM-Aided Bug Injection Chrysalis dataset with bug injection; ICL+RAG+CoT bug injection methodology; iteratively-refined HLS verification dataset 4 4 4"},{"location":"software/pl/#dataflow-hls-acceleration","title":"Dataflow HLS acceleration","text":"<p>Solution: exploiting task-level parallelism, optimizing inter-kernel communication, maximizing pipeline throughput</p> Year Venue Authors Title Tags P E N 2024 ASPLOS UIUC HIDA: A Hierarchical Dataflow Compiler for High-Level Synthesis hierarchical dataflow IR (HIDA-IR); multi-level dataflow optimizer (HIDA-OPT); pattern-driven task fusion 3 5 4 2025 FPGA UCLA Stream-HLS: Towards Automatic Dataflow Acceleration automatic dataflow HLS; global scheduling for streaming; MINLP for HLS optimization 4 4 4"},{"location":"software/pl/#hls-for-specialized-hardware","title":"HLS for specialized hardware","text":"<p>Solution: target-specific code generation, custom memory interface synthesis, co-optimization with physical constraints</p> Year Venue Authors Title Tags P E N 2023 FPGA UoP DONGLE: Direct FPGA-Orchestrated NVMe Storage for HLS HLS direct NVMe access; FPGA-orchestrated storage; Unified HLS storage interface; Single-source HLS for storage; DONGLE architecture 4 4 4 2023 FPGA HKUST FADO: Floorplan-Aware Directive Optimization for High-Level Synthesis Designs on Multi-Die FPGAs Floorplan-aware HLS; Multi-die FPGA optimization; Directive-floorplan co-optimization; Incremental floorplanning for HLS; MMBP for HLS DSE 3 4 4 2025 FPGA Brown University ARIES: An Agile MLIR-Based Compilation Flow for Reconfigurable Devices with AI Engines MLIR-based AIE compilation; Unified AIE+PL IR; Tile-based parallelism; ADF dialect; Automated AIE placement 4 5 4"},{"location":"software/pl/#program-analysis","title":"Program analysis","text":"<p>Solution: statically or dynamically analyzing programs to understand their behavior, detect errors, and optimize performance</p>"},{"location":"software/pl/#domain-specific-program-analysis","title":"Domain-specific program analysis","text":"<p>Solution: leveraging domain knowledge for precise analysis, specialized bug detection, targeted optimization insights</p> Year Venue Authors Title Tags P E N 2024 PPoPP Information Engineering University A Holistic Approach to Automatic Mixed-Precision Code Generation and Tuning for Affine Programs holistic code generation and tuning; polyhedral model for mixed-precision; model-driven autotuning 4 5 4 2024 PPoPP University of Delaware Recurrence Analysis for Automatic Parallelization of Subscripted Subscripts recurrence analysis for parallelization; subscripted subscript analysis; intermittent monotonicity detection 3 4 3"},{"location":"software/pl/#hls-program-analysis","title":"HLS program analysis","text":"<p>Solution: verifying functional correctness of HLS, analyzing performance bottlenecks, ensuring interface compatibility</p> Year Venue Authors Title Tags P E N 2025 FPGA UoE Latency Insensitivity Testing for Dataflow HLS Designs Automated Latency Insensitivity Testing; Parallel Hardware-Accelerated Testing Platform; Test space reduction; Stalling Units (SU) 4 4 4"}]}